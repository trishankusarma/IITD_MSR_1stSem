{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Non-Stationary Environment\n"
     ]
    }
   ],
   "source": [
    "from env import FootballSkillsEnv\n",
    "from model import model\n",
    "from policyIterationAlgo import policyIterationAlgo\n",
    "from valueIterationAlgo import valueIterationAlgo\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Key Environment Methods to Use:\n",
    "    - env.state_to_index(state_tuple): Converts (x, y, has_shot) tuple to integer index\n",
    "    - env.index_to_state(index): Converts in   teger index back to (x, y, has_shot) tuple\n",
    "    - env.get_transitions_at_time(state, action, time_step=None): Default method for accessing transitions.\n",
    "    - env._is_terminal(state): Check if state is terminal (has_shot=True)\n",
    "    - env._get_reward(ball_pos, action, player_pos): Get reward for transition\n",
    "    - env.reset(seed=None): Reset environment to initial state, returns (observation, info)\n",
    "    - env.step(action): Execute action, returns (obs, reward, done, truncated, info)\n",
    "    - env.get_gif(policy, seed=20, filename=\"output.gif\"): Generate GIF visualization \n",
    "      of policy execution from given seed\n",
    "    \n",
    "    Key Env Variables Notes:\n",
    "    - env.observation_space.n: Total number of states (use env.grid_size^2 * 2)\n",
    "    - env.action_space.n: Total number of actions (7 actions: 4 movement + 3 shooting)\n",
    "    - env.grid_size: Total number of rows in the grid\n",
    "'''\n",
    "print(\"Starting Non-Stationary Environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (40, 800) and shape of valueFn (41, 800)\n",
      "Errors in policy_state : 27976\n",
      "Errors in policy_state : 19954\n",
      "Errors in policy_state : 10058\n",
      "Errors in policy_state : 9558\n",
      "Errors in policy_state : 4290\n",
      "Errors in policy_state : 2194\n",
      "Errors in policy_state : 770\n",
      "Errors in policy_state : 164\n",
      "Errors in policy_state : 80\n",
      "Errors in policy_state : 94\n",
      "Errors in policy_state : 72\n",
      "Errors in policy_state : 80\n",
      "Errors in policy_state : 64\n",
      "Errors in policy_state : 58\n",
      "Errors in policy_state : 22\n",
      "Errors in policy_state : 6\n",
      "Errors in policy_state : 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutputNonStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  4896000\n",
      "The policy iteration converged after 17 outer_iterations, 34 inner_iterations and 4896000 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_pi, optimal_valueFn_pi,outer_num_iterations_pi, inner_num_iterations_pi, calls_to_getTransisions_fn_pi = policyIterationAlgo(logEnabled = False, degrade_pitch = True)\n",
    "\n",
    "print(f\"The policy iteration converged after {outer_num_iterations_pi} outer_iterations, {inner_num_iterations_pi} inner_iterations and {calls_to_getTransisions_fn_pi} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (40, 800) and shape of valueFn (41, 800)\n",
      "maxAbsDiff after iteration : 1 is 58.159054415340094\n",
      "maxAbsDiff after iteration : 2 is 48.24391990791517\n",
      "maxAbsDiff after iteration : 3 is 40.54074611531525\n",
      "maxAbsDiff after iteration : 4 is 34.34997255728222\n",
      "maxAbsDiff after iteration : 5 is 30.683464366389167\n",
      "maxAbsDiff after iteration : 6 is 27.20732636057118\n",
      "maxAbsDiff after iteration : 7 is 24.49930090017895\n",
      "maxAbsDiff after iteration : 8 is 21.980702029483925\n",
      "maxAbsDiff after iteration : 9 is 19.63736419354079\n",
      "maxAbsDiff after iteration : 10 is 17.461641814066684\n",
      "maxAbsDiff after iteration : 11 is 15.44768847508903\n",
      "maxAbsDiff after iteration : 12 is 13.607094683275507\n",
      "maxAbsDiff after iteration : 13 is 12.091565672317323\n",
      "maxAbsDiff after iteration : 14 is 10.704881897902904\n",
      "maxAbsDiff after iteration : 15 is 9.438049069973257\n",
      "maxAbsDiff after iteration : 16 is 8.283154986323453\n",
      "maxAbsDiff after iteration : 17 is 7.233178911605924\n",
      "maxAbsDiff after iteration : 18 is 6.560799973158739\n",
      "maxAbsDiff after iteration : 19 is 6.042742670617229\n",
      "maxAbsDiff after iteration : 20 is 5.414880741625428\n",
      "maxAbsDiff after iteration : 21 is 4.856766578551024\n",
      "maxAbsDiff after iteration : 22 is 4.352826500640392\n",
      "maxAbsDiff after iteration : 23 is 3.978986853778908\n",
      "maxAbsDiff after iteration : 24 is 3.6039400049052635\n",
      "maxAbsDiff after iteration : 25 is 3.235965467170315\n",
      "maxAbsDiff after iteration : 26 is 2.500120994196344\n",
      "maxAbsDiff after iteration : 27 is 2.2597176507198\n",
      "maxAbsDiff after iteration : 28 is 1.6642966800422725\n",
      "maxAbsDiff after iteration : 29 is 1.4621016054783915\n",
      "maxAbsDiff after iteration : 30 is 1.0265183565702385\n",
      "maxAbsDiff after iteration : 31 is 0.878318670400569\n",
      "maxAbsDiff after iteration : 32 is 0.7096409422004175\n",
      "maxAbsDiff after iteration : 33 is 0.5344274519225394\n",
      "maxAbsDiff after iteration : 34 is 0.39030272342858563\n",
      "maxAbsDiff after iteration : 35 is 0.25912629724262715\n",
      "maxAbsDiff after iteration : 36 is 0.17200664472767446\n",
      "maxAbsDiff after iteration : 37 is 0.10466048919566706\n",
      "maxAbsDiff after iteration : 38 is 0.06402267757824376\n",
      "maxAbsDiff after iteration : 39 is 0.03746578791497157\n",
      "maxAbsDiff after iteration : 40 is 0.021033901832482105\n",
      "maxAbsDiff after iteration : 41 is 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutputNonStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  9408000\n",
      "The value iteration converged after 1 outer_iterations, 41 inner_iterations and 9408000 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "# 1. Value Iteration\n",
    "optimal_policy_vi, optimal_valueFn_vi, outer_num_iterations_vi, inner_num_iterations_vi, calls_to_getTransisions_fn_vi = valueIterationAlgo(degrade_pitch = True)\n",
    "\n",
    "print(f\"The value iteration converged after {outer_num_iterations_vi} outer_iterations, {inner_num_iterations_vi} inner_iterations and {calls_to_getTransisions_fn_vi} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 \n",
    "\n",
    "# The policy iteration converged after 17 outer_iterations, 34 inner_iterations and 4896000 calls to getTransisions_fn\n",
    "# The value iteration converged after 1 outer_iterations, 41 inner_iterations and 9408000 calls to getTransisions_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (40, 800) and shape of valueFn (41, 800)\n",
      "maxAbsDiff after iteration : 1 is 50.7\n",
      "maxAbsDiff after iteration : 2 is 27.739999999999995\n",
      "maxAbsDiff after iteration : 3 is 17.665249999999997\n",
      "maxAbsDiff after iteration : 4 is 13.400933699999996\n",
      "maxAbsDiff after iteration : 5 is 10.119844048999997\n",
      "maxAbsDiff after iteration : 6 is 7.835763851439996\n",
      "maxAbsDiff after iteration : 7 is 6.385366082079196\n",
      "maxAbsDiff after iteration : 8 is 5.648465156826261\n",
      "maxAbsDiff after iteration : 9 is 5.207060304803921\n",
      "maxAbsDiff after iteration : 10 is 4.651942631606094\n",
      "maxAbsDiff after iteration : 11 is 4.13356312585775\n",
      "maxAbsDiff after iteration : 12 is 3.6367333065307963\n",
      "maxAbsDiff after iteration : 13 is 3.2098650709601073\n",
      "maxAbsDiff after iteration : 14 is 2.9810147762157144\n",
      "maxAbsDiff after iteration : 15 is 2.7686709623298933\n",
      "maxAbsDiff after iteration : 16 is 2.5537037041816673\n",
      "maxAbsDiff after iteration : 17 is 2.314498848976048\n",
      "maxAbsDiff after iteration : 18 is 2.1081875655197058\n",
      "maxAbsDiff after iteration : 19 is 1.8907138780134383\n",
      "maxAbsDiff after iteration : 20 is 1.790661354447601\n",
      "maxAbsDiff after iteration : 21 is 1.6642341136976633\n",
      "maxAbsDiff after iteration : 22 is 1.5573880118205725\n",
      "maxAbsDiff after iteration : 23 is 1.4327896558091027\n",
      "maxAbsDiff after iteration : 24 is 1.3175499950295873\n",
      "maxAbsDiff after iteration : 25 is 1.2042912863981243\n",
      "maxAbsDiff after iteration : 26 is 1.1360398161391512\n",
      "maxAbsDiff after iteration : 27 is 1.0659259058221444\n",
      "maxAbsDiff after iteration : 28 is 0.9960115060622687\n",
      "maxAbsDiff after iteration : 29 is 0.9282972958436009\n",
      "maxAbsDiff after iteration : 30 is 0.8473399007734326\n",
      "maxAbsDiff after iteration : 31 is 0.7737842247766018\n",
      "maxAbsDiff after iteration : 32 is 0.7083523143683621\n",
      "maxAbsDiff after iteration : 33 is 0.587356922943636\n",
      "maxAbsDiff after iteration : 34 is 0.5405622736493552\n",
      "maxAbsDiff after iteration : 35 is 0.4690082515551648\n",
      "maxAbsDiff after iteration : 36 is 0.43150881293895793\n",
      "maxAbsDiff after iteration : 37 is 0.3908710554443626\n",
      "maxAbsDiff after iteration : 38 is 0.3627825484977052\n",
      "maxAbsDiff after iteration : 39 is 0.3314874737386848\n",
      "maxAbsDiff after iteration : 40 is 0.3024893755219704\n",
      "maxAbsDiff after iteration : 41 is 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutputNonStationaryWithoutPassingTimeStamp.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  9408000\n",
      "The value iteration converged after 1 outer_iterations, 41 inner_iterations and 9408000 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "# Q3 :: For comparison implement your usual Value Iteration(without time as a state), this can be done by making\n",
    "# calls to get transition at time function without specifying the time step and maintaing a Value function\n",
    "# that depends only on the state. What do you observe?\n",
    "\n",
    "optimal_policy_vi_ts_independent, optimal_valueFn_vi_ts_independent, outer_num_iterations_vi_ts_independent, inner_num_iterations_vi_ts_independent, calls_to_getTransisions_fn_vi_ts_independent = valueIterationAlgo(\n",
    "    degrade_pitch = True, passTimeStamp = False\n",
    ")\n",
    "\n",
    "print(f\"The value iteration converged after {outer_num_iterations_vi_ts_independent} outer_iterations, {inner_num_iterations_vi_ts_independent} inner_iterations and {calls_to_getTransisions_fn_vi_ts_independent} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of dissimilarities : 5454\n"
     ]
    }
   ],
   "source": [
    "# are the 2 policies identical :: optimal_policy_vi and optimal_policy_pi\n",
    "def comparePolicies(policy1, policy2, model = model):\n",
    "    cnt_of_dissimarlities = 0\n",
    "    model = model()\n",
    "\n",
    "    for timeStamp in range(model.non_stationary_horizon):\n",
    "        for i in range(model.maxStateSize):\n",
    "            if policy1[timeStamp][i] != policy2[timeStamp][i]:\n",
    "                cnt_of_dissimarlities += 1\n",
    "\n",
    "    print(\"Count of dissimilarities :\", cnt_of_dissimarlities)\n",
    "\n",
    "# comparePolicies(optimal_policy_vi, optimal_policy_pi)\n",
    "comparePolicies(optimal_policy_vi, optimal_policy_vi_ts_independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "# 1. After implementing the value iteration, we observed that both were bound to converge after 40 iterations \n",
    "# :: should be bcz we obtained as many equations as the no of variables\n",
    "# total_horizon * no_of_states equations\n",
    "# Because information can only move one time-layer per outer iteration, it takes:\n",
    "\n",
    "# iteration 1 → makes V_{H-1} correct (uses V_H = 0),\n",
    "# iteration 2 → makes V_{H-2} correct (now sees the correct V_{H-1}),\n",
    "# …\n",
    "# iteration H → finally makes V_0 correct.\n",
    "\n",
    "# So with H=40, you naturally get ~40 iterations before delta hits the threshold.\n",
    "# 2. The optimal policy obtained in both cases had dissimmilarities \n",
    "# Stationary VI is cheaper in state-space size, but will generally produce a (possibly) suboptimal policy in a non-stationary environment. \n",
    "# Non-stationary VI produces better policies but at extra computational cost roughly proportional to H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "def getEpisodeRewards(env, obs, policy, model):\n",
    "    episode_reward = 0\n",
    "    timeStamp = 0\n",
    "    while True:\n",
    "        if timeStamp >= model.non_stationary_horizon:\n",
    "            break\n",
    "        \n",
    "        state_index = env.state_to_index(obs)\n",
    "        action = policy[timeStamp][state_index]\n",
    "        \n",
    "        if action == -1:       \n",
    "            print(\"No valid action found!\")\n",
    "            break\n",
    "        \n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        timeStamp += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return episode_reward\n",
    "\n",
    "def getEpisodesRewardMean(envClass, episodeCount, policy, model, seeds):\n",
    "    if seeds is None:\n",
    "        seeds = np.random.randint(0, 30, size=episodeCount)\n",
    "    total_rewards = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        env = envClass(render_mode=None, degrade_pitch=True)\n",
    "        obs, _ = env.reset(seed= int(seed))\n",
    "        \n",
    "        # try to evaluate the episode starting from obs\n",
    "        total_rewards.append(getEpisodeRewards(env, obs, policy, model))\n",
    "\n",
    "    mean_reward = np.mean(total_rewards)\n",
    "    variance_reward = np.var(total_rewards)\n",
    "    \n",
    "    return mean_reward, variance_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the performance of each policy by running 20 episodes with different seeds\n",
    "# Given the optimal policies for both VI and PI\n",
    "def compareMeanAndVarianceOfRewards(policy1, policy2, envClass=FootballSkillsEnv, model=model):\n",
    "    \n",
    "    model = model()\n",
    "    seeds = np.random.randint(0, 30, size=model.episodeCount)\n",
    "    \n",
    "    meanPI, variancePI = getEpisodesRewardMean(envClass, model.episodeCount, policy1, model, seeds)\n",
    "    meanVI, varianceVI = getEpisodesRewardMean(envClass, model.episodeCount, policy2, model, seeds)\n",
    "\n",
    "    print(\"Value Iteration passing time stamp :: Mean : \", meanPI, \" Variance : \", variancePI)\n",
    "    print(\"Value Iteration without passing time stamp :: Mean : \", meanVI, \" Variance : \", varianceVI)\n",
    "    # Deterministic env → variance = 0 after convergence.\n",
    "    \n",
    "    if(meanPI == meanVI and variancePI == varianceVI):\n",
    "        print(\"mean and variance of policies are same\")\n",
    "    else:\n",
    "        print(\"mean and variance of policies are not same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration passing time stamp :: Mean :  53.8  Variance :  796.26\n",
      "Value Iteration without passing time stamp :: Mean :  25.8  Variance :  2212.46\n",
      "mean and variance of policies are not same\n"
     ]
    }
   ],
   "source": [
    "#compareMeanAndVarianceOfRewards(optimal_policy_vi, optimal_policy_pi)\n",
    "compareMeanAndVarianceOfRewards(optimal_policy_vi, optimal_policy_vi_ts_independent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
