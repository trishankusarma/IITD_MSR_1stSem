{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stationary Environment\n"
     ]
    }
   ],
   "source": [
    "from env import FootballSkillsEnv\n",
    "from model import model\n",
    "from policyIterationAlgo import policyIterationAlgo\n",
    "from valueIterationAlgo import valueIterationAlgo\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Key Environment Methods to Use:\n",
    "    - env.state_to_index(state_tuple): Converts (x, y, has_shot) tuple to integer index\n",
    "    - env.index_to_state(index): Converts in   teger index back to (x, y, has_shot) tuple\n",
    "    - env.get_transitions_at_time(state, action, time_step=None): Default method for accessing transitions.\n",
    "    - env._is_terminal(state): Check if state is terminal (has_shot=True)\n",
    "    - env._get_reward(ball_pos, action, player_pos): Get reward for transition\n",
    "    - env.reset(seed=None): Reset environment to initial state, returns (observation, info)\n",
    "    - env.step(action): Execute action, returns (obs, reward, done, truncated, info)\n",
    "    - env.get_gif(policy, seed=20, filename=\"output.gif\"): Generate GIF visualization \n",
    "      of policy execution from given seed\n",
    "    \n",
    "    Key Env Variables Notes:\n",
    "    - env.observation_space.n: Total number of states (use env.grid_size^2 * 2)\n",
    "    - env.action_space.n: Total number of actions (7 actions: 4 movement + 3 shooting)\n",
    "    - env.grid_size: Total number of rows in the grid\n",
    "'''\n",
    "print(\"Starting Stationary Environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Errors in policy_state : 798\n",
      "Errors in policy_state : 87\n",
      "Errors in policy_state : 57\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 34\n",
      "Errors in policy_state : 37\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 35\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 29\n",
      "Errors in policy_state : 28\n",
      "Errors in policy_state : 27\n",
      "Errors in policy_state : 24\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 17\n",
      "Errors in policy_state : 14\n",
      "Errors in policy_state : 12\n",
      "Errors in policy_state : 10\n",
      "Errors in policy_state : 8\n",
      "Errors in policy_state : 6\n",
      "Errors in policy_state : 4\n",
      "Errors in policy_state : 2\n",
      "Errors in policy_state : 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutputStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  406400\n",
      "The policy iteration converged after 25 outer_iterations, 333 inner_iterations and 406400 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_pi, optimal_valueFn_pi,outer_num_iterations_pi, inner_num_iterations_pi, calls_to_getTransisions_fn_pi = policyIterationAlgo(logEnabled = False)\n",
    "\n",
    "print(f\"The policy iteration converged after {outer_num_iterations_pi} outer_iterations, {inner_num_iterations_pi} inner_iterations and {calls_to_getTransisions_fn_pi} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "maxAbsDiff after iteration : 1 is 60.0\n",
      "maxAbsDiff after iteration : 2 is 39.9\n",
      "maxAbsDiff after iteration : 3 is 37.905\n",
      "maxAbsDiff after iteration : 4 is 36.00975\n",
      "maxAbsDiff after iteration : 5 is 34.209262499999994\n",
      "maxAbsDiff after iteration : 6 is 32.498799375\n",
      "maxAbsDiff after iteration : 7 is 30.87385940624999\n",
      "maxAbsDiff after iteration : 8 is 29.330166435937493\n",
      "maxAbsDiff after iteration : 9 is 27.863658114140613\n",
      "maxAbsDiff after iteration : 10 is 26.470475208433584\n",
      "maxAbsDiff after iteration : 11 is 25.146951448011905\n",
      "maxAbsDiff after iteration : 12 is 23.889603875611307\n",
      "maxAbsDiff after iteration : 13 is 22.69512368183074\n",
      "maxAbsDiff after iteration : 14 is 21.5603674977392\n",
      "maxAbsDiff after iteration : 15 is 20.48234912285224\n",
      "maxAbsDiff after iteration : 16 is 19.45823166670963\n",
      "maxAbsDiff after iteration : 17 is 16.956459023846957\n",
      "maxAbsDiff after iteration : 18 is 16.108636072654612\n",
      "maxAbsDiff after iteration : 19 is 15.30320426902188\n",
      "maxAbsDiff after iteration : 20 is 14.538044055570783\n",
      "maxAbsDiff after iteration : 21 is 13.811141852792243\n",
      "maxAbsDiff after iteration : 22 is 13.120584760152632\n",
      "maxAbsDiff after iteration : 23 is 12.464555522144998\n",
      "maxAbsDiff after iteration : 24 is 11.841327746037747\n",
      "maxAbsDiff after iteration : 25 is 11.249261358735858\n",
      "maxAbsDiff after iteration : 26 is 2.2593485638317734\n",
      "maxAbsDiff after iteration : 27 is 2.1463811356401847\n",
      "maxAbsDiff after iteration : 28 is 2.0390620788581755\n",
      "maxAbsDiff after iteration : 29 is 1.9371089749152668\n",
      "maxAbsDiff after iteration : 30 is 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutputStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  173600\n",
      "The value iteration converged after 1 outer_iterations, 30 inner_iterations and 173600 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_vi, optimal_valueFn_vi, outer_num_iterations_vi, inner_num_iterations_vi, calls_to_getTransisions_fn_vi = valueIterationAlgo()\n",
    "\n",
    "print(f\"The value iteration converged after {outer_num_iterations_vi} outer_iterations, {inner_num_iterations_vi} inner_iterations and {calls_to_getTransisions_fn_vi} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "# The policy iteration converged after 25 outer_iterations, 333 inner_iterations and 406400 calls to getTransisions_fn\n",
    "\n",
    "# The value iteration converged after 1 outer_iterations, 30 inner_iterations and 173600 calls to getTransisions_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. are the 2 policies identical :: optimal_policy_vi and optimal_policy_pi\n",
    "def comparePolicies(policy1, policy2, model = model):\n",
    "    cnt_of_dissimarlities = 0\n",
    "    model = model()\n",
    "\n",
    "    for i in range(model.maxStateSize):\n",
    "        if policy1[i] != policy2[i]:\n",
    "            cnt_of_dissimarlities += 1\n",
    "\n",
    "    print(\"Count of dissimilarities :\", cnt_of_dissimarlities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of dissimilarities : 0\n"
     ]
    }
   ],
   "source": [
    "comparePolicies(optimal_policy_pi, optimal_policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpisodeRewards(env, obs, policy):\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        state_index = env.state_to_index(obs)\n",
    "        action = policy[state_index]\n",
    "        \n",
    "        if action == -1:       \n",
    "            print(\"No valid action found!\")\n",
    "            break\n",
    "        \n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return episode_reward\n",
    "\n",
    "def getEpisodesRewardMean(envClass, episodeCount, policy, seeds):\n",
    "    total_rewards = []\n",
    "    \n",
    "    if seeds is None:\n",
    "        seeds = np.random.randint(0, 30, size=episodeCount)\n",
    "\n",
    "    for seed in seeds:\n",
    "        env = envClass(render_mode='gif')\n",
    "        obs, _ = env.reset(seed= int(seed))\n",
    "        \n",
    "        # try to evaluate the episode starting from obs\n",
    "        total_rewards.append(getEpisodeRewards(env, obs, policy))\n",
    "\n",
    "    mean_reward = np.mean(total_rewards)\n",
    "    variance_reward = np.var(total_rewards)\n",
    "    \n",
    "    return mean_reward, variance_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the performance of each policy by running 20 episodes with different seeds\n",
    "# Given the optimal policies for both VI and PI\n",
    "def compareMeanAndVarianceOfRewards(policy1, policy2, envClass=FootballSkillsEnv, model=model):\n",
    "    \n",
    "    model = model()\n",
    "    seeds = np.random.randint(0, 30, size=model.episodeCount)\n",
    "    \n",
    "    meanPI, variancePI = getEpisodesRewardMean(envClass, model.episodeCount, policy1, seeds)\n",
    "    meanVI, varianceVI = getEpisodesRewardMean(envClass, model.episodeCount, policy2, seeds)\n",
    "\n",
    "    print(\"Policy Iteration :: Mean : \", meanPI, \" Variance : \", variancePI)\n",
    "    print(\"Value Iteration :: Mean : \", meanVI, \" Variance : \", varianceVI)\n",
    "    # Deterministic env â†’ variance = 0 after convergence.\n",
    "    \n",
    "    if(meanPI == meanVI and variancePI == varianceVI):\n",
    "        print(\"mean and variance of policies are same\")\n",
    "    else:\n",
    "        print(\"mean and variance of policies are not same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareMeanAndVarianceOfRewards(optimal_policy_pi, optimal_policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor : 0.3\n",
    "\n",
    "# Policy Iteration :: Mean :  38.0  Variance :  3024.0\n",
    "# Value Iteration :: Mean :  38.0  Variance :  3024.0\n",
    "# mean and variance of policies are same\n",
    "\n",
    "# discount factor : 0.5\n",
    "\n",
    "# Policy Iteration :: Mean :  38.0  Variance :  3024.0\n",
    "# Value Iteration :: Mean :  38.0  Variance :  3024.0\n",
    "# mean and variance of policies are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Errors in policy_state : 798\n",
      "Errors in policy_state : 53\n",
      "Errors in policy_state : 13\n",
      "Errors in policy_state : 17\n",
      "Errors in policy_state : 21\n",
      "Errors in policy_state : 22\n",
      "Errors in policy_state : 25\n",
      "Errors in policy_state : 26\n",
      "Errors in policy_state : 27\n",
      "Errors in policy_state : 23\n",
      "Errors in policy_state : 21\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 18\n",
      "Errors in policy_state : 15\n",
      "Errors in policy_state : 13\n",
      "Errors in policy_state : 11\n",
      "Errors in policy_state : 9\n",
      "Errors in policy_state : 7\n",
      "Errors in policy_state : 5\n",
      "Errors in policy_state : 3\n",
      "Errors in policy_state : 1\n",
      "Errors in policy_state : 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutputStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  192000\n",
      "The policy iteration converged after 26 outer_iterations, 58 inner_iterations and 192000 calls to getTransisions_fn\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "maxAbsDiff after iteration : 1 is 60.0\n",
      "maxAbsDiff after iteration : 2 is 12.6\n",
      "maxAbsDiff after iteration : 3 is 3.7800000000000002\n",
      "maxAbsDiff after iteration : 4 is 1.1340000000000001\n",
      "maxAbsDiff after iteration : 5 is 0.34020000000000006\n",
      "maxAbsDiff after iteration : 6 is 0.10206000000000004\n",
      "maxAbsDiff after iteration : 7 is 0.030618000000000034\n",
      "maxAbsDiff after iteration : 8 is 0.00918540000000001\n",
      "maxAbsDiff after iteration : 9 is 0.0027556200000000697\n",
      "maxAbsDiff after iteration : 10 is 0.0008266860000001319\n",
      "maxAbsDiff after iteration : 11 is 0.0002480058000000618\n",
      "maxAbsDiff after iteration : 12 is 7.440174000006294e-05\n",
      "maxAbsDiff after iteration : 13 is 2.2320522000018883e-05\n",
      "maxAbsDiff after iteration : 14 is 6.696156600094483e-06\n",
      "maxAbsDiff after iteration : 15 is 2.0088469798729136e-06\n",
      "maxAbsDiff after iteration : 16 is 6.026540939618741e-07\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutputStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  95200\n",
      "The value iteration converged after 1 outer_iterations, 16 inner_iterations and 95200 calls to getTransisions_fn\n",
      "Count of dissimilarities : 46\n",
      "Policy Iteration :: Mean :  38.0  Variance :  3024.0\n",
      "Value Iteration :: Mean :  38.0  Variance :  3024.0\n",
      "mean and variance of policies are same\n",
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Errors in policy_state : 798\n",
      "Errors in policy_state : 72\n",
      "Errors in policy_state : 42\n",
      "Errors in policy_state : 33\n",
      "Errors in policy_state : 23\n",
      "Errors in policy_state : 25\n",
      "Errors in policy_state : 27\n",
      "Errors in policy_state : 29\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 24\n",
      "Errors in policy_state : 23\n",
      "Errors in policy_state : 22\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 17\n",
      "Errors in policy_state : 14\n",
      "Errors in policy_state : 12\n",
      "Errors in policy_state : 10\n",
      "Errors in policy_state : 8\n",
      "Errors in policy_state : 6\n",
      "Errors in policy_state : 4\n",
      "Errors in policy_state : 2\n",
      "Errors in policy_state : 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutputStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  204000\n",
      "The policy iteration converged after 25 outer_iterations, 80 inner_iterations and 204000 calls to getTransisions_fn\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "maxAbsDiff after iteration : 1 is 60.0\n",
      "maxAbsDiff after iteration : 2 is 21.0\n",
      "maxAbsDiff after iteration : 3 is 10.5\n",
      "maxAbsDiff after iteration : 4 is 5.25\n",
      "maxAbsDiff after iteration : 5 is 2.625\n",
      "maxAbsDiff after iteration : 6 is 1.3125\n",
      "maxAbsDiff after iteration : 7 is 0.65625\n",
      "maxAbsDiff after iteration : 8 is 0.328125\n",
      "maxAbsDiff after iteration : 9 is 0.1640625\n",
      "maxAbsDiff after iteration : 10 is 0.08203125\n",
      "maxAbsDiff after iteration : 11 is 0.041015625\n",
      "maxAbsDiff after iteration : 12 is 0.0205078125\n",
      "maxAbsDiff after iteration : 13 is 0.01025390625\n",
      "maxAbsDiff after iteration : 14 is 0.005126953125\n",
      "maxAbsDiff after iteration : 15 is 0.0025634765625\n",
      "maxAbsDiff after iteration : 16 is 0.00128173828125\n",
      "maxAbsDiff after iteration : 17 is 0.000457763671875\n",
      "maxAbsDiff after iteration : 18 is 0.0002288818359375\n",
      "maxAbsDiff after iteration : 19 is 0.00011444091796875\n",
      "maxAbsDiff after iteration : 20 is 5.7220458984375e-05\n",
      "maxAbsDiff after iteration : 21 is 2.86102294921875e-05\n",
      "maxAbsDiff after iteration : 22 is 1.430511474609375e-05\n",
      "maxAbsDiff after iteration : 23 is 7.152557373046875e-06\n",
      "maxAbsDiff after iteration : 24 is 3.5762786865234375e-06\n",
      "maxAbsDiff after iteration : 25 is 1.7881393432617188e-06\n",
      "maxAbsDiff after iteration : 26 is 0\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutputStationary.gif\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  151200\n",
      "The value iteration converged after 1 outer_iterations, 26 inner_iterations and 151200 calls to getTransisions_fn\n",
      "Count of dissimilarities : 0\n",
      "Policy Iteration :: Mean :  38.0  Variance :  3024.0\n",
      "Value Iteration :: Mean :  38.0  Variance :  3024.0\n",
      "mean and variance of policies are same\n"
     ]
    }
   ],
   "source": [
    "discountFactors = [0.3, 0.5]\n",
    "model = model()\n",
    "\n",
    "for discount_factor in discountFactors:    \n",
    "    optimal_policy_pi, optimal_valueFn_pi,outer_num_iterations_pi, inner_num_iterations_pi, calls_to_getTransisions_fn_pi = policyIterationAlgo(logEnabled = False, discount_factor = discount_factor)\n",
    "    print(f\"The policy iteration converged after {outer_num_iterations_pi} outer_iterations, {inner_num_iterations_pi} inner_iterations and {calls_to_getTransisions_fn_pi} calls to getTransisions_fn\")\n",
    "    \n",
    "    optimal_policy_vi, optimal_valueFn_vi, outer_num_iterations_vi, inner_num_iterations_vi, calls_to_getTransisions_fn_vi = valueIterationAlgo(logEnabled = False, discount_factor = discount_factor)\n",
    "    print(f\"The value iteration converged after {outer_num_iterations_vi} outer_iterations, {inner_num_iterations_vi} inner_iterations and {calls_to_getTransisions_fn_vi} calls to getTransisions_fn\")\n",
    "    \n",
    "    comparePolicies(optimal_policy_pi, optimal_policy_vi)\n",
    "    compareMeanAndVarianceOfRewards(optimal_policy_pi, optimal_policy_vi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
