{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Environment\n"
     ]
    }
   ],
   "source": [
    "from env import FootballSkillsEnv\n",
    "from model import model\n",
    "from policyIterationAlgo import policyIterationAlgo\n",
    "from valueIterationAlgo import valueIterationAlgo\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    Key Environment Methods to Use:\n",
    "    - env.state_to_index(state_tuple): Converts (x, y, has_shot) tuple to integer index\n",
    "    - env.index_to_state(index): Converts in   teger index back to (x, y, has_shot) tuple\n",
    "    - env.get_transitions_at_time(state, action, time_step=None): Default method for accessing transitions.\n",
    "    - env._is_terminal(state): Check if state is terminal (has_shot=True)\n",
    "    - env._get_reward(ball_pos, action, player_pos): Get reward for transition\n",
    "    - env.reset(seed=None): Reset environment to initial state, returns (observation, info)\n",
    "    - env.step(action): Execute action, returns (obs, reward, done, truncated, info)\n",
    "    - env.get_gif(policy, seed=20, filename=\"output.gif\"): Generate GIF visualization \n",
    "      of policy execution from given seed\n",
    "    \n",
    "    Key Env Variables Notes:\n",
    "    - env.observation_space.n: Total number of states (use env.grid_size^2 * 2)\n",
    "    - env.action_space.n: Total number of actions (7 actions: 4 movement + 3 shooting)\n",
    "    - env.grid_size: Total number of rows in the grid\n",
    "'''\n",
    "print(\"Starting Environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Errors in policy_state : 798\n",
      "Errors in policy_state : 87\n",
      "Errors in policy_state : 57\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 34\n",
      "Errors in policy_state : 37\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 35\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 29\n",
      "Errors in policy_state : 28\n",
      "Errors in policy_state : 27\n",
      "Errors in policy_state : 24\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 17\n",
      "Errors in policy_state : 14\n",
      "Errors in policy_state : 12\n",
      "Errors in policy_state : 10\n",
      "Errors in policy_state : 8\n",
      "Errors in policy_state : 6\n",
      "Errors in policy_state : 4\n",
      "Errors in policy_state : 2\n",
      "Errors in policy_state : 0\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  406400\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutput.gif\n",
      "The policy iteration converged after 25 outer_iterations, 333 inner_iterations and 406400 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_pi, optimal_valueFn_pi,outer_num_iterations_pi, inner_num_iterations_pi, calls_to_getTransisions_fn_pi = policyIterationAlgo(logEnabled = False)\n",
    "\n",
    "print(f\"The policy iteration converged after {outer_num_iterations_pi} outer_iterations, {inner_num_iterations_pi} inner_iterations and {calls_to_getTransisions_fn_pi} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  173600\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutput.gif\n",
      "The value iteration converged after 1 outer_iterations, 30 inner_iterations and 173600 calls to getTransisions_fn\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_vi, optimal_valueFn_vi, outer_num_iterations_vi, inner_num_iterations_vi, calls_to_getTransisions_fn_vi = valueIterationAlgo()\n",
    "\n",
    "print(f\"The value iteration converged after {outer_num_iterations_vi} outer_iterations, {inner_num_iterations_vi} inner_iterations and {calls_to_getTransisions_fn_vi} calls to getTransisions_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "# The policy iteration converged after 25 outer_iterations, 333 inner_iterations and 406400 calls to getTransisions_fn\n",
    "\n",
    "# The value iteration converged after 1 outer_iterations, 30 inner_iterations and 173600 calls to getTransisions_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of dissimilarities : 0\n"
     ]
    }
   ],
   "source": [
    "# 3. are the 2 policies identical :: optimal_policy_vi and optimal_policy_pi\n",
    "def comparePolicies(policy1, policy2, model = model):\n",
    "    cnt_of_dissimarlities = 0\n",
    "    model = model()\n",
    "\n",
    "    for i in range(model.maxStateSize):\n",
    "        if policy1[i] != policy2[i]:\n",
    "            cnt_of_dissimarlities += 1\n",
    "\n",
    "    print(\"Count of dissimilarities :\", cnt_of_dissimarlities)\n",
    "\n",
    "comparePolicies(optimal_policy_pi, optimal_policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpisodeRewards(env, obs, policy):\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        state_index = env.state_to_index(obs)\n",
    "        action = policy[state_index]\n",
    "        \n",
    "        if action == -1:       \n",
    "            print(\"No valid action found!\")\n",
    "            break\n",
    "        \n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return episode_reward\n",
    "\n",
    "def getEpisodesRewardMean(env, episodeCount, policy):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodeCount):\n",
    "        my_seed = np.random.randint(0, 30)\n",
    "        obs, _ = env.reset(seed= my_seed)\n",
    "        \n",
    "        # try to evaluate the episode starting from obs\n",
    "        total_rewards.append(getEpisodeRewards(env, obs, policy))\n",
    "\n",
    "    mean_reward = np.mean(total_rewards)\n",
    "    variance_reward = np.var(total_rewards)\n",
    "    \n",
    "    return mean_reward, variance_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration :: Mean :  52.0  Variance :  0.0\n",
      "Value Iteration :: Mean :  52.0  Variance :  0.0\n",
      "mean and variance of policies are same\n"
     ]
    }
   ],
   "source": [
    "# 4. Evaluate the performance of each policy by running 20 episodes with different seeds\n",
    "# Given the optimal policies for both VI and PI\n",
    "def compareMeanAndVarianceOfRewards(policy1, policy2, env=FootballSkillsEnv, model=model):\n",
    "    \n",
    "    env = FootballSkillsEnv(render_mode='gif')\n",
    "    model = model()\n",
    "    \n",
    "    meanPI, variancePI = getEpisodesRewardMean(env, model.episodeCount, policy1)\n",
    "    meanVI, varianceVI = getEpisodesRewardMean(env, model.episodeCount, policy2)\n",
    "\n",
    "    print(\"Policy Iteration :: Mean : \", meanPI, \" Variance : \", variancePI)\n",
    "    print(\"Value Iteration :: Mean : \", meanVI, \" Variance : \", varianceVI)\n",
    "    # Deterministic env â†’ variance = 0 after convergence.\n",
    "    \n",
    "    if(meanPI == meanVI and variancePI == varianceVI):\n",
    "        print(\"mean and variance of policies are same\")\n",
    "    else:\n",
    "        print(\"mean and variance of policies are not same\")\n",
    "\n",
    "compareMeanAndVarianceOfRewards(optimal_policy_pi, optimal_policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Errors in policy_state : 798\n",
      "Errors in policy_state : 87\n",
      "Errors in policy_state : 57\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 34\n",
      "Errors in policy_state : 37\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 35\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 29\n",
      "Errors in policy_state : 28\n",
      "Errors in policy_state : 27\n",
      "Errors in policy_state : 24\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 17\n",
      "Errors in policy_state : 14\n",
      "Errors in policy_state : 12\n",
      "Errors in policy_state : 10\n",
      "Errors in policy_state : 8\n",
      "Errors in policy_state : 6\n",
      "Errors in policy_state : 4\n",
      "Errors in policy_state : 2\n",
      "Errors in policy_state : 0\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  406400\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutput.gif\n",
      "The policy iteration converged after 25 outer_iterations, 333 inner_iterations and 406400 calls to getTransisions_fn\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  173600\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutput.gif\n",
      "The value iteration converged after 1 outer_iterations, 30 inner_iterations and 173600 calls to getTransisions_fn\n",
      "Count of dissimilarities : 0\n",
      "Policy Iteration :: Mean :  52.0  Variance :  0.0\n",
      "Value Iteration :: Mean :  52.0  Variance :  0.0\n",
      "mean and variance of policies are same\n",
      "Starting Policy Iteration Algorithm\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Errors in policy_state : 798\n",
      "Errors in policy_state : 87\n",
      "Errors in policy_state : 57\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 34\n",
      "Errors in policy_state : 37\n",
      "Errors in policy_state : 39\n",
      "Errors in policy_state : 35\n",
      "Errors in policy_state : 30\n",
      "Errors in policy_state : 29\n",
      "Errors in policy_state : 28\n",
      "Errors in policy_state : 27\n",
      "Errors in policy_state : 24\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 20\n",
      "Errors in policy_state : 17\n",
      "Errors in policy_state : 14\n",
      "Errors in policy_state : 12\n",
      "Errors in policy_state : 10\n",
      "Errors in policy_state : 8\n",
      "Errors in policy_state : 6\n",
      "Errors in policy_state : 4\n",
      "Errors in policy_state : 2\n",
      "Errors in policy_state : 0\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  406400\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/PIOutput.gif\n",
      "The policy iteration converged after 25 outer_iterations, 333 inner_iterations and 406400 calls to getTransisions_fn\n",
      "Total number of possible states : 800\n",
      "Total actions :  7\n",
      "Shape of policy matrix (800,) and shape of valueFn (800,)\n",
      "Count of total number of calls made to the  env.get_transitions_at_time is :  173600\n",
      "Starting rollout from position: (0, 10, 0)\n",
      "\n",
      "Reached terminal state.\n",
      "Episode GIF saved to ./output_seeds/VIOutput.gif\n",
      "The value iteration converged after 1 outer_iterations, 30 inner_iterations and 173600 calls to getTransisions_fn\n",
      "Count of dissimilarities : 0\n",
      "Policy Iteration :: Mean :  52.0  Variance :  0.0\n",
      "Value Iteration :: Mean :  52.0  Variance :  0.0\n",
      "mean and variance of policies are same\n"
     ]
    }
   ],
   "source": [
    "discountFactors = [0.3, 0.5]\n",
    "model = model()\n",
    "\n",
    "for discount_factor in discountFactors:\n",
    "    model.discount_factor = discount_factor\n",
    "    \n",
    "    optimal_policy_pi, optimal_valueFn_pi,outer_num_iterations_pi, inner_num_iterations_pi, calls_to_getTransisions_fn_pi = policyIterationAlgo(logEnabled = False)\n",
    "    print(f\"The policy iteration converged after {outer_num_iterations_pi} outer_iterations, {inner_num_iterations_pi} inner_iterations and {calls_to_getTransisions_fn_pi} calls to getTransisions_fn\")\n",
    "    \n",
    "    optimal_policy_vi, optimal_valueFn_vi, outer_num_iterations_vi, inner_num_iterations_vi, calls_to_getTransisions_fn_vi = valueIterationAlgo(logEnabled = False)\n",
    "    print(f\"The value iteration converged after {outer_num_iterations_vi} outer_iterations, {inner_num_iterations_vi} inner_iterations and {calls_to_getTransisions_fn_vi} calls to getTransisions_fn\")\n",
    "    \n",
    "    comparePolicies(optimal_policy_pi, optimal_policy_vi)\n",
    "    compareMeanAndVarianceOfRewards(optimal_policy_pi, optimal_policy_vi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
