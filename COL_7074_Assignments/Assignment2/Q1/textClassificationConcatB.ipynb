{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from naive_bayes_custom import NaiveBayesCustom\n",
    "from utils import tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary, getTrainingAndTestingData2, plot_wordclouds_per_class\n",
    "from modelUtils import run_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape : (140000, 3)\n",
      "   label                              title  \\\n",
      "0      3                          Ernie Cox   \n",
      "1     10                          Holosteum   \n",
      "2      9                Pestarella tyrrhena   \n",
      "3      1          MidSun Junior High School   \n",
      "4      6  St James' Church Wrightington Bar   \n",
      "\n",
      "                                             content  \n",
      "0   Ernest Ernie Cox (February 17 1894 – February...  \n",
      "1   Holosteum is a genus of plants in the Pink fa...  \n",
      "2   Pestarella tyrrhena (formerly Callianassa tyr...  \n",
      "3   MidSun Junior High School is a Canadian middl...  \n",
      "4   St James' Church Wrightington Bar is in Churc...  \n",
      "df_test.shape : (35000, 3)\n",
      "   label                          title  \\\n",
      "0      4                   Lajos Drahos   \n",
      "1      5          USS Huntsville (1857)   \n",
      "2      0                         SCAFCO   \n",
      "3      6               McLean's Mansion   \n",
      "4      5  Avioane Craiova IAR-93 Vultur   \n",
      "\n",
      "                                             content  \n",
      "0   Lajos Drahos (7 March 1895 - 2 June 1983) was...  \n",
      "1   USS Huntsville was a steamer acquired by the ...  \n",
      "2   Founded in 1954 by Ben G. Stone SCAFCO Corpor...  \n",
      "3   McLean's Mansion (originally Holly Lea) is a ...  \n",
      "4   The Avioane Craiova IAR-93 Vultur (Eagle) is ...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "print(f\"df_train.shape : {df_train.shape}\")\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(f\"df_test.shape : {df_test.shape}\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data Sample:\n",
      "                           Tokenized Title  \\\n",
      "0                             [ernie, cox]   \n",
      "1                              [holosteum]   \n",
      "2                   [pestarella, tyrrhena]   \n",
      "3           [midsun, junior, high, school]   \n",
      "4  [st, james', church, wrightington, bar]   \n",
      "\n",
      "                               Tokenized Description  Class Index  \n",
      "0  [ernest, ernie, cox, (february, 17, 1894, –, f...            3  \n",
      "1  [holosteum, is, a, genus, of, plants, in, the,...           10  \n",
      "2  [pestarella, tyrrhena, (formerly, callianassa,...            9  \n",
      "3  [midsun, junior, high, school, is, a, canadian...            1  \n",
      "4  [st, james', church, wrightington, bar, is, in...            6  \n",
      "\n",
      "Testing Data Sample:\n",
      "                      Tokenized Title  \\\n",
      "0                     [lajos, drahos]   \n",
      "1           [uss, huntsville, (1857)]   \n",
      "2                            [scafco]   \n",
      "3                 [mclean's, mansion]   \n",
      "4  [avioane, craiova, iar-93, vultur]   \n",
      "\n",
      "                               Tokenized Description  Class Index  \n",
      "0  [lajos, drahos, (7, march, 1895, -, 2, june, 1...            4  \n",
      "1  [uss, huntsville, was, a, steamer, acquired, b...            5  \n",
      "2  [founded, in, 1954, by, ben, g., stone, scafco...            0  \n",
      "3  [mclean's, mansion, (originally, holly, lea), ...            6  \n",
      "4  [the, avioane, craiova, iar-93, vultur, (eagle...            5  \n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Training with 14 classes, 140000 examples.\n",
      "Title vocab size: 120268, Content vocab size: 424380\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 120268)\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 424380)\n",
      "Predictions added to column 'Predicted'.\n",
      "Evaluating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 98.20%\n",
      "\n",
      "Class 0 -> Precision: 0.9754, Recall: 0.9361, F1-Score: 0.9554\n",
      "Class 1 -> Precision: 0.9739, Recall: 0.9924, F1-Score: 0.9831\n",
      "Class 2 -> Precision: 0.9853, Recall: 0.9631, F1-Score: 0.9741\n",
      "Class 3 -> Precision: 0.9868, Recall: 0.9941, F1-Score: 0.9904\n",
      "Class 4 -> Precision: 0.9830, Recall: 0.9895, F1-Score: 0.9862\n",
      "Class 5 -> Precision: 0.9857, Recall: 0.9946, F1-Score: 0.9901\n",
      "Class 6 -> Precision: 0.9626, Recall: 0.9794, F1-Score: 0.9710\n",
      "Class 7 -> Precision: 0.9808, Recall: 0.9931, F1-Score: 0.9869\n",
      "Class 8 -> Precision: 0.9997, Recall: 0.9622, F1-Score: 0.9806\n",
      "Class 9 -> Precision: 0.9990, Recall: 0.9780, F1-Score: 0.9884\n",
      "Class 10 -> Precision: 0.9916, Recall: 0.9963, F1-Score: 0.9940\n",
      "Class 11 -> Precision: 0.9727, Recall: 0.9972, F1-Score: 0.9848\n",
      "Class 12 -> Precision: 0.9884, Recall: 0.9888, F1-Score: 0.9886\n",
      "Class 13 -> Precision: 0.9646, Recall: 0.9830, F1-Score: 0.9737\n",
      "\n",
      "Macro-Average F1 Score: 0.9819\n",
      "Predictions added to column 'Predicted'.\n",
      "Evaluating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.45%\n",
      "\n",
      "Class 0 -> Precision: 0.9541, Recall: 0.8904, F1-Score: 0.9212\n",
      "Class 1 -> Precision: 0.9577, Recall: 0.9876, F1-Score: 0.9724\n",
      "Class 2 -> Precision: 0.9615, Recall: 0.9100, F1-Score: 0.9351\n",
      "Class 3 -> Precision: 0.9809, Recall: 0.9868, F1-Score: 0.9838\n",
      "Class 4 -> Precision: 0.9628, Recall: 0.9828, F1-Score: 0.9727\n",
      "Class 5 -> Precision: 0.9748, Recall: 0.9888, F1-Score: 0.9817\n",
      "Class 6 -> Precision: 0.9466, Recall: 0.9644, F1-Score: 0.9554\n",
      "Class 7 -> Precision: 0.9671, Recall: 0.9864, F1-Score: 0.9766\n",
      "Class 8 -> Precision: 0.9992, Recall: 0.9452, F1-Score: 0.9714\n",
      "Class 9 -> Precision: 0.9954, Recall: 0.9476, F1-Score: 0.9709\n",
      "Class 10 -> Precision: 0.9671, Recall: 0.9876, F1-Score: 0.9772\n",
      "Class 11 -> Precision: 0.9544, Recall: 0.9952, F1-Score: 0.9743\n",
      "Class 12 -> Precision: 0.9693, Recall: 0.9728, F1-Score: 0.9711\n",
      "Class 13 -> Precision: 0.9180, Recall: 0.9580, F1-Score: 0.9376\n",
      "\n",
      "Macro-Average F1 Score: 0.9644\n"
     ]
    }
   ],
   "source": [
    "# Question Part 1 :: Unigram without stemming and removing stop words\n",
    "# Train the model with context and title concatenated only corresponding to the labels\n",
    "df_train, vocabularyTitle = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1])\n",
    "\n",
    "df_train, vocabularyContent = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = [1])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData2(df_train, df_test, target_field1 = \"Tokenized Title\", target_field2 = \"Tokenized Description\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayesCustom() \n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model2(model, vocabularyTitle, vocabularyContent, trainingData, testingData, smoothening = 1.0, text_col1 = \"Tokenized Title\", text_col2 = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data Sample:\n",
      "                           Tokenized Title  \\\n",
      "0                              [erni, cox]   \n",
      "1                              [holosteum]   \n",
      "2                   [pestarella, tyrrhena]   \n",
      "3           [midsun, junior, high, school]   \n",
      "4  [st, james', church, wrightington, bar]   \n",
      "\n",
      "                               Tokenized Description  Class Index  \n",
      "0  [ernest, erni, cox, (februari, 17, 1894, –, fe...            3  \n",
      "1  [holosteum, genu, plant, pink, famili, (caryop...           10  \n",
      "2  [pestarella, tyrrhena, (formerli, callianassa,...            9  \n",
      "3  [midsun, junior, high, school, canadian, middl...            1  \n",
      "4  [st, james', church, wrightington, bar, church...            6  \n",
      "\n",
      "Testing Data Sample:\n",
      "                     Tokenized Title  \\\n",
      "0                      [lajo, draho]   \n",
      "1            [uss, huntsvil, (1857)]   \n",
      "2                           [scafco]   \n",
      "3                 [mclean', mansion]   \n",
      "4  [avioan, craiova, iar-93, vultur]   \n",
      "\n",
      "                               Tokenized Description  Class Index  \n",
      "0  [lajo, draho, (7, march, 1895, -, 2, june, 198...            4  \n",
      "1  [uss, huntsvil, steamer, acquir, union, navi, ...            5  \n",
      "2  [found, 1954, ben, g., stone, scafco, corpor, ...            0  \n",
      "3  [mclean', mansion, (origin, holli, lea), homes...            6  \n",
      "4  [avioan, craiova, iar-93, vultur, (eagle), twi...            5  \n",
      "--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Training with 14 classes, 140000 examples.\n",
      "Title vocab size: 113808, Content vocab size: 398983\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 113808)\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 398983)\n",
      "Predictions added to column 'Predicted'.\n",
      "Evaluating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.97%\n",
      "\n",
      "Class 0 -> Precision: 0.9611, Recall: 0.9175, F1-Score: 0.9388\n",
      "Class 1 -> Precision: 0.9721, Recall: 0.9912, F1-Score: 0.9816\n",
      "Class 2 -> Precision: 0.9827, Recall: 0.9503, F1-Score: 0.9662\n",
      "Class 3 -> Precision: 0.9872, Recall: 0.9955, F1-Score: 0.9913\n",
      "Class 4 -> Precision: 0.9844, Recall: 0.9876, F1-Score: 0.9860\n",
      "Class 5 -> Precision: 0.9834, Recall: 0.9930, F1-Score: 0.9882\n",
      "Class 6 -> Precision: 0.9659, Recall: 0.9736, F1-Score: 0.9697\n",
      "Class 7 -> Precision: 0.9860, Recall: 0.9940, F1-Score: 0.9900\n",
      "Class 8 -> Precision: 0.9996, Recall: 0.9723, F1-Score: 0.9858\n",
      "Class 9 -> Precision: 0.9992, Recall: 0.9851, F1-Score: 0.9921\n",
      "Class 10 -> Precision: 0.9946, Recall: 0.9962, F1-Score: 0.9954\n",
      "Class 11 -> Precision: 0.9595, Recall: 0.9971, F1-Score: 0.9779\n",
      "Class 12 -> Precision: 0.9829, Recall: 0.9887, F1-Score: 0.9858\n",
      "Class 13 -> Precision: 0.9589, Recall: 0.9742, F1-Score: 0.9665\n",
      "\n",
      "Macro-Average F1 Score: 0.9797\n",
      "Predictions added to column 'Predicted'.\n",
      "Evaluating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.91%\n",
      "\n",
      "Class 0 -> Precision: 0.9304, Recall: 0.8712, F1-Score: 0.8998\n",
      "Class 1 -> Precision: 0.9488, Recall: 0.9864, F1-Score: 0.9672\n",
      "Class 2 -> Precision: 0.9489, Recall: 0.8760, F1-Score: 0.9110\n",
      "Class 3 -> Precision: 0.9778, Recall: 0.9868, F1-Score: 0.9823\n",
      "Class 4 -> Precision: 0.9629, Recall: 0.9748, F1-Score: 0.9688\n",
      "Class 5 -> Precision: 0.9716, Recall: 0.9840, F1-Score: 0.9777\n",
      "Class 6 -> Precision: 0.9455, Recall: 0.9568, F1-Score: 0.9511\n",
      "Class 7 -> Precision: 0.9724, Recall: 0.9876, F1-Score: 0.9800\n",
      "Class 8 -> Precision: 0.9987, Recall: 0.9524, F1-Score: 0.9750\n",
      "Class 9 -> Precision: 0.9946, Recall: 0.9600, F1-Score: 0.9770\n",
      "Class 10 -> Precision: 0.9766, Recall: 0.9868, F1-Score: 0.9817\n",
      "Class 11 -> Precision: 0.9338, Recall: 0.9936, F1-Score: 0.9628\n",
      "Class 12 -> Precision: 0.9572, Recall: 0.9756, F1-Score: 0.9663\n",
      "Class 13 -> Precision: 0.9115, Recall: 0.9356, F1-Score: 0.9234\n",
      "\n",
      "Macro-Average F1 Score: 0.9589\n"
     ]
    }
   ],
   "source": [
    "# Question 1, part 2 :: Unigram with stemming and removing stop words\n",
    "df_train, vocabularyTitle = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1])\n",
    "\n",
    "df_train, vocabularyContent = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = [1])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData2(df_train, df_test, target_field1 = \"Tokenized Title\", target_field2 = \"Tokenized Description\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayesCustom() \n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model2(model, vocabularyTitle, vocabularyContent, trainingData, testingData, smoothening = 1.0, text_col1 = \"Tokenized Title\", text_col2 = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data Sample:\n",
      "                                     Tokenized Title  \\\n",
      "0                                        [ernie cox]   \n",
      "1                                                 []   \n",
      "2                              [pestarella tyrrhena]   \n",
      "3          [midsun junior, junior high, high school]   \n",
      "4  [st james', james' church, church wrightington...   \n",
      "\n",
      "                               Tokenized Description  Class Index  \n",
      "0  [ernest ernie, ernie cox, cox (february, (febr...            3  \n",
      "1  [holosteum is, is a, a genus, genus of, of pla...           10  \n",
      "2  [pestarella tyrrhena, tyrrhena (formerly, (for...            9  \n",
      "3  [midsun junior, junior high, high school, scho...            1  \n",
      "4  [st james', james' church, church wrightington...            6  \n",
      "\n",
      "Testing Data Sample:\n",
      "                                    Tokenized Title  \\\n",
      "0                                    [lajos drahos]   \n",
      "1               [uss huntsville, huntsville (1857)]   \n",
      "2                                                []   \n",
      "3                                [mclean's mansion]   \n",
      "4  [avioane craiova, craiova iar-93, iar-93 vultur]   \n",
      "\n",
      "                               Tokenized Description  Class Index  \n",
      "0  [lajos drahos, drahos (7, (7 march, march 1895...            4  \n",
      "1  [uss huntsville, huntsville was, was a, a stea...            5  \n",
      "2  [founded in, in 1954, 1954 by, by ben, ben g.,...            0  \n",
      "3  [mclean's mansion, mansion (originally, (origi...            6  \n",
      "4  [the avioane, avioane craiova, craiova iar-93,...            5  \n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Training with 14 classes, 140000 examples.\n",
      "Title vocab size: 189055, Content vocab size: 1980745\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 189055)\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 1980745)\n"
     ]
    }
   ],
   "source": [
    "# Question 1, part 4 :: Bigram without stemming or removing stop words\n",
    "df_train, vocabularyTitle = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [2])\n",
    "\n",
    "df_train, vocabularyContent = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = [2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData2(df_train, df_test, target_field1 = \"Tokenized Title\", target_field2 = \"Tokenized Description\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayesCustom() \n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model2(model, vocabularyTitle, vocabularyContent, trainingData, testingData, smoothening = 1.0, text_col1 = \"Tokenized Title\", text_col2 = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1, part 3 :: Bigram with stemming and removing stop words\n",
    "df_train, vocabularyTitle = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [2])\n",
    "\n",
    "df_train, vocabularyContent = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = [2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData2(df_train, df_test, target_field1 = \"Tokenized Title\", target_field2 = \"Tokenized Description\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayesCustom() \n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model2(model, vocabularyTitle, vocabularyContent, trainingData, testingData, smoothening = 1.0, text_col1 = \"Tokenized Title\", text_col2 = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1, part 5 :: uni + Bigram without stemming or removing stop words\n",
    "df_train, vocabularyTitle = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1,2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1,2])\n",
    "\n",
    "df_train, vocabularyContent = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = [1,2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = [1,2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData2(df_train, df_test, target_field1 = \"Tokenized Title\", target_field2 = \"Tokenized Description\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayesCustom() \n",
    "print(\"--------UNI+BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNI+BIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model2(model, vocabularyTitle, vocabularyContent, trainingData, testingData, smoothening = 1.0, text_col1 = \"Tokenized Title\", text_col2 = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1, part 6 :: uni + Bigram with stemming or removing stop words\n",
    "df_train, vocabularyTitle = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1,2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1,2])\n",
    "\n",
    "df_train, vocabularyContent = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = [1,2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = [1,2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData2(df_train, df_test, target_field1 = \"Tokenized Title\", target_field2 = \"Tokenized Description\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayesCustom() \n",
    "print(\"--------UNI+BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNI+BIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model2(model, vocabularyTitle, vocabularyContent, trainingData, testingData, smoothening = 1.0, text_col1 = \"Tokenized Title\", text_col2 = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
