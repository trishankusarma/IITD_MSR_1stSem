{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from naive_bayes import NaiveBayes\n",
    "from utils import tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary, getTrainingAndTestingData, plot_wordclouds_per_class\n",
    "from modelUtils import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape : (140000, 3)\n",
      "   label                              title  \\\n",
      "0      3                          Ernie Cox   \n",
      "1     10                          Holosteum   \n",
      "2      9                Pestarella tyrrhena   \n",
      "3      1          MidSun Junior High School   \n",
      "4      6  St James' Church Wrightington Bar   \n",
      "\n",
      "                                             content  \n",
      "0   Ernest Ernie Cox (February 17 1894 â€“ February...  \n",
      "1   Holosteum is a genus of plants in the Pink fa...  \n",
      "2   Pestarella tyrrhena (formerly Callianassa tyr...  \n",
      "3   MidSun Junior High School is a Canadian middl...  \n",
      "4   St James' Church Wrightington Bar is in Churc...  \n",
      "df_test.shape : (35000, 3)\n",
      "   label                          title  \\\n",
      "0      4                   Lajos Drahos   \n",
      "1      5          USS Huntsville (1857)   \n",
      "2      0                         SCAFCO   \n",
      "3      6               McLean's Mansion   \n",
      "4      5  Avioane Craiova IAR-93 Vultur   \n",
      "\n",
      "                                             content  \n",
      "0   Lajos Drahos (7 March 1895 - 2 June 1983) was...  \n",
      "1   USS Huntsville was a steamer acquired by the ...  \n",
      "2   Founded in 1954 by Ben G. Stone SCAFCO Corpor...  \n",
      "3   McLean's Mansion (originally Holly Lea) is a ...  \n",
      "4   The Avioane Craiova IAR-93 Vultur (Eagle) is ...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "print(f\"df_train.shape : {df_train.shape}\")\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(f\"df_test.shape : {df_test.shape}\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                           Tokenized Title  Class Index\n",
      "0                             [ernie, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "                      Tokenized Title  Class Index\n",
      "0                     [lajos, drahos]            4\n",
      "1           [uss, huntsville, (1857)]            5\n",
      "2                            [scafco]            0\n",
      "3                 [mclean's, mansion]            6\n",
      "4  [avioane, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 120268\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 120268)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 90.30%\n",
      "\n",
      "Class 0 -> Precision: 0.9459, Recall: 0.8595, F1-Score: 0.9006\n",
      "Class 1 -> Precision: 0.8975, Recall: 0.9819, F1-Score: 0.9378\n",
      "Class 2 -> Precision: 0.8693, Recall: 0.7712, F1-Score: 0.8173\n",
      "Class 3 -> Precision: 0.8609, Recall: 0.8456, F1-Score: 0.8532\n",
      "Class 4 -> Precision: 0.7991, Recall: 0.8754, F1-Score: 0.8355\n",
      "Class 5 -> Precision: 0.9572, Recall: 0.9459, F1-Score: 0.9515\n",
      "Class 6 -> Precision: 0.8891, Recall: 0.9264, F1-Score: 0.9074\n",
      "Class 7 -> Precision: 0.9665, Recall: 0.9638, F1-Score: 0.9652\n",
      "Class 8 -> Precision: 0.9835, Recall: 0.9788, F1-Score: 0.9812\n",
      "Class 9 -> Precision: 0.9938, Recall: 0.9662, F1-Score: 0.9798\n",
      "Class 10 -> Precision: 0.9896, Recall: 0.9914, F1-Score: 0.9905\n",
      "Class 11 -> Precision: 0.8246, Recall: 0.8855, F1-Score: 0.8539\n",
      "Class 12 -> Precision: 0.8622, Recall: 0.8177, F1-Score: 0.8394\n",
      "Class 13 -> Precision: 0.8220, Recall: 0.8332, F1-Score: 0.8276\n",
      "\n",
      "Macro-Average F1 Score: 0.9029\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 65.84%\n",
      "\n",
      "Class 0 -> Precision: 0.2905, Recall: 0.7676, F1-Score: 0.4215\n",
      "Class 1 -> Precision: 0.8344, Recall: 0.9696, F1-Score: 0.8969\n",
      "Class 2 -> Precision: 0.4697, Recall: 0.3412, F1-Score: 0.3953\n",
      "Class 3 -> Precision: 0.5466, Recall: 0.4808, F1-Score: 0.5116\n",
      "Class 4 -> Precision: 0.5332, Recall: 0.5392, F1-Score: 0.5362\n",
      "Class 5 -> Precision: 0.9067, Recall: 0.8316, F1-Score: 0.8675\n",
      "Class 6 -> Precision: 0.8173, Recall: 0.8676, F1-Score: 0.8417\n",
      "Class 7 -> Precision: 0.9024, Recall: 0.7728, F1-Score: 0.8326\n",
      "Class 8 -> Precision: 0.9335, Recall: 0.5500, F1-Score: 0.6922\n",
      "Class 9 -> Precision: 0.9393, Recall: 0.5012, F1-Score: 0.6536\n",
      "Class 10 -> Precision: 0.9439, Recall: 0.7472, F1-Score: 0.8341\n",
      "Class 11 -> Precision: 0.6619, Recall: 0.6916, F1-Score: 0.6764\n",
      "Class 12 -> Precision: 0.6625, Recall: 0.5464, F1-Score: 0.5989\n",
      "Class 13 -> Precision: 0.6372, Recall: 0.6112, F1-Score: 0.6239\n",
      "\n",
      "Macro-Average F1 Score: 0.6702\n"
     ]
    }
   ],
   "source": [
    "# Question Part 1 :: Unigram without stemming and removing stop words\n",
    "# Train the model with title only corresponding to the labels\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayes() \n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_wordclouds_per_class(trainingData, maxWords = 200, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word cloud on training data\n",
      "Word cloud on testing data\n",
      "trainingData.head()\n",
      "                           Tokenized Title  Class Index\n",
      "0                              [erni, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "                     Tokenized Title  Class Index\n",
      "0                      [lajo, draho]            4\n",
      "1            [uss, huntsvil, (1857)]            5\n",
      "2                           [scafco]            0\n",
      "3                 [mclean', mansion]            6\n",
      "4  [avioan, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 113808\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 113808)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 89.06%\n",
      "\n",
      "Class 0 -> Precision: 0.9064, Recall: 0.8444, F1-Score: 0.8743\n",
      "Class 1 -> Precision: 0.8808, Recall: 0.9802, F1-Score: 0.9278\n",
      "Class 2 -> Precision: 0.8515, Recall: 0.7605, F1-Score: 0.8034\n",
      "Class 3 -> Precision: 0.8477, Recall: 0.8319, F1-Score: 0.8397\n",
      "Class 4 -> Precision: 0.7848, Recall: 0.8648, F1-Score: 0.8229\n",
      "Class 5 -> Precision: 0.9468, Recall: 0.9396, F1-Score: 0.9432\n",
      "Class 6 -> Precision: 0.8606, Recall: 0.9277, F1-Score: 0.8929\n",
      "Class 7 -> Precision: 0.9577, Recall: 0.9653, F1-Score: 0.9615\n",
      "Class 8 -> Precision: 0.9810, Recall: 0.9777, F1-Score: 0.9794\n",
      "Class 9 -> Precision: 0.9911, Recall: 0.9616, F1-Score: 0.9761\n",
      "Class 10 -> Precision: 0.9885, Recall: 0.9905, F1-Score: 0.9895\n",
      "Class 11 -> Precision: 0.8111, Recall: 0.8583, F1-Score: 0.8340\n",
      "Class 12 -> Precision: 0.8339, Recall: 0.7989, F1-Score: 0.8160\n",
      "Class 13 -> Precision: 0.8375, Recall: 0.7670, F1-Score: 0.8007\n",
      "\n",
      "Macro-Average F1 Score: 0.8901\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 64.89%\n",
      "\n",
      "Class 0 -> Precision: 0.2802, Recall: 0.7468, F1-Score: 0.4075\n",
      "Class 1 -> Precision: 0.8182, Recall: 0.9684, F1-Score: 0.8870\n",
      "Class 2 -> Precision: 0.4593, Recall: 0.3452, F1-Score: 0.3942\n",
      "Class 3 -> Precision: 0.5369, Recall: 0.4772, F1-Score: 0.5053\n",
      "Class 4 -> Precision: 0.5291, Recall: 0.5416, F1-Score: 0.5353\n",
      "Class 5 -> Precision: 0.8932, Recall: 0.8264, F1-Score: 0.8585\n",
      "Class 6 -> Precision: 0.7840, Recall: 0.8680, F1-Score: 0.8238\n",
      "Class 7 -> Precision: 0.8941, Recall: 0.7732, F1-Score: 0.8293\n",
      "Class 8 -> Precision: 0.9232, Recall: 0.5484, F1-Score: 0.6881\n",
      "Class 9 -> Precision: 0.9297, Recall: 0.4976, F1-Score: 0.6483\n",
      "Class 10 -> Precision: 0.9425, Recall: 0.7472, F1-Score: 0.8336\n",
      "Class 11 -> Precision: 0.6538, Recall: 0.6708, F1-Score: 0.6622\n",
      "Class 12 -> Precision: 0.6504, Recall: 0.5484, F1-Score: 0.5951\n",
      "Class 13 -> Precision: 0.6573, Recall: 0.5248, F1-Score: 0.5836\n",
      "\n",
      "Macro-Average F1 Score: 0.6608\n"
     ]
    }
   ],
   "source": [
    "# Question 1, part 2 :: Unigram with stemming and removing stop words\n",
    "\n",
    "# # (a) Perform stemming and remove the stop-words in the training as well as the validation data.\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1])\n",
    "\n",
    "# (b) Construct word clouds for both classes on the transformed data\n",
    "print(\"Word cloud on training data\")\n",
    "# plot_wordclouds_per_class(trainingData, data_type = \"Training Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "print(\"Word cloud on testing data\")\n",
    "# plot_wordclouds_per_class(testingData, data_type = \"Testing Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "# (c) Learn a new model on the transformed data. Report the validation set accuracy\n",
    "# 2. unigram - with stem - with stop words       - done\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")\n",
    "\n",
    "# 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                                         [erni cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                                   Tokenized Title  Class Index\n",
      "0                                     [lajo draho]            4\n",
      "1                  [uss huntsvil, huntsvil (1857)]            5\n",
      "2                                               []            0\n",
      "3                                [mclean' mansion]            6\n",
      "4  [avioan craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 178478\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 178478)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 89.97%\n",
      "\n",
      "Class 0 -> Precision: 0.4233, Recall: 0.9941, F1-Score: 0.5938\n",
      "Class 1 -> Precision: 0.9865, Recall: 0.9895, F1-Score: 0.9880\n",
      "Class 2 -> Precision: 0.9994, Recall: 0.9656, F1-Score: 0.9822\n",
      "Class 3 -> Precision: 0.9984, Recall: 0.9946, F1-Score: 0.9965\n",
      "Class 4 -> Precision: 0.9974, Recall: 0.9936, F1-Score: 0.9955\n",
      "Class 5 -> Precision: 0.9978, Recall: 0.9819, F1-Score: 0.9898\n",
      "Class 6 -> Precision: 0.9901, Recall: 0.9715, F1-Score: 0.9807\n",
      "Class 7 -> Precision: 0.9974, Recall: 0.8597, F1-Score: 0.9235\n",
      "Class 8 -> Precision: 0.9965, Recall: 0.6258, F1-Score: 0.7688\n",
      "Class 9 -> Precision: 0.9996, Recall: 0.7371, F1-Score: 0.8485\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.7806, F1-Score: 0.8768\n",
      "Class 11 -> Precision: 0.9957, Recall: 0.9020, F1-Score: 0.9465\n",
      "Class 12 -> Precision: 0.9941, Recall: 0.8927, F1-Score: 0.9407\n",
      "Class 13 -> Precision: 0.9939, Recall: 0.9074, F1-Score: 0.9487\n",
      "\n",
      "Macro-Average F1 Score: 0.9129\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 25.55%\n",
      "\n",
      "Class 0 -> Precision: 0.0875, Recall: 0.9564, F1-Score: 0.1603\n",
      "Class 1 -> Precision: 0.9093, Recall: 0.7420, F1-Score: 0.8172\n",
      "Class 2 -> Precision: 0.4308, Recall: 0.0224, F1-Score: 0.0426\n",
      "Class 3 -> Precision: 0.8327, Recall: 0.0896, F1-Score: 0.1618\n",
      "Class 4 -> Precision: 0.7208, Recall: 0.1332, F1-Score: 0.2248\n",
      "Class 5 -> Precision: 0.9199, Recall: 0.2388, F1-Score: 0.3792\n",
      "Class 6 -> Precision: 0.8680, Recall: 0.4420, F1-Score: 0.5857\n",
      "Class 7 -> Precision: 0.9107, Recall: 0.1876, F1-Score: 0.3111\n",
      "Class 8 -> Precision: 0.9758, Recall: 0.1932, F1-Score: 0.3225\n",
      "Class 9 -> Precision: 0.8462, Recall: 0.0220, F1-Score: 0.0429\n",
      "Class 10 -> Precision: 0.9464, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.7957, Recall: 0.1792, F1-Score: 0.2925\n",
      "Class 12 -> Precision: 0.8011, Recall: 0.2336, F1-Score: 0.3617\n",
      "Class 13 -> Precision: 0.7029, Recall: 0.1164, F1-Score: 0.1997\n",
      "\n",
      "Macro-Average F1 Score: 0.2817\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram with stemming and removing stop words\n",
    "# 3. bigram - with stem - with stop words        -done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")\n",
    "\n",
    "# 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                                        [ernie cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                                    Tokenized Title  Class Index\n",
      "0                                    [lajos drahos]            4\n",
      "1               [uss huntsville, huntsville (1857)]            5\n",
      "2                                                []            0\n",
      "3                                [mclean's mansion]            6\n",
      "4  [avioane craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 189055\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 189055)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 90.35%\n",
      "\n",
      "Class 0 -> Precision: 0.4442, Recall: 0.9895, F1-Score: 0.6131\n",
      "Class 1 -> Precision: 0.9816, Recall: 0.9889, F1-Score: 0.9853\n",
      "Class 2 -> Precision: 0.9994, Recall: 0.9691, F1-Score: 0.9840\n",
      "Class 3 -> Precision: 0.9987, Recall: 0.9969, F1-Score: 0.9978\n",
      "Class 4 -> Precision: 0.9971, Recall: 0.9954, F1-Score: 0.9962\n",
      "Class 5 -> Precision: 0.9980, Recall: 0.9813, F1-Score: 0.9896\n",
      "Class 6 -> Precision: 0.9885, Recall: 0.9680, F1-Score: 0.9781\n",
      "Class 7 -> Precision: 0.9980, Recall: 0.8609, F1-Score: 0.9244\n",
      "Class 8 -> Precision: 0.9979, Recall: 0.6278, F1-Score: 0.7707\n",
      "Class 9 -> Precision: 0.9999, Recall: 0.7374, F1-Score: 0.8488\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.7805, F1-Score: 0.8767\n",
      "Class 11 -> Precision: 0.9809, Recall: 0.9233, F1-Score: 0.9512\n",
      "Class 12 -> Precision: 0.9700, Recall: 0.9104, F1-Score: 0.9392\n",
      "Class 13 -> Precision: 0.9710, Recall: 0.9190, F1-Score: 0.9443\n",
      "\n",
      "Macro-Average F1 Score: 0.9143\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 28.17%\n",
      "\n",
      "Class 0 -> Precision: 0.0928, Recall: 0.9444, F1-Score: 0.1690\n",
      "Class 1 -> Precision: 0.9041, Recall: 0.7696, F1-Score: 0.8315\n",
      "Class 2 -> Precision: 0.4375, Recall: 0.0224, F1-Score: 0.0426\n",
      "Class 3 -> Precision: 0.8358, Recall: 0.0896, F1-Score: 0.1618\n",
      "Class 4 -> Precision: 0.7352, Recall: 0.1344, F1-Score: 0.2273\n",
      "Class 5 -> Precision: 0.9198, Recall: 0.2384, F1-Score: 0.3787\n",
      "Class 6 -> Precision: 0.8626, Recall: 0.4572, F1-Score: 0.5976\n",
      "Class 7 -> Precision: 0.9205, Recall: 0.1852, F1-Score: 0.3084\n",
      "Class 8 -> Precision: 0.9779, Recall: 0.1944, F1-Score: 0.3243\n",
      "Class 9 -> Precision: 0.8438, Recall: 0.0216, F1-Score: 0.0421\n",
      "Class 10 -> Precision: 0.9815, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.6849, Recall: 0.2808, F1-Score: 0.3983\n",
      "Class 12 -> Precision: 0.6120, Recall: 0.3256, F1-Score: 0.4251\n",
      "Class 13 -> Precision: 0.5702, Recall: 0.2584, F1-Score: 0.3556\n",
      "\n",
      "Macro-Average F1 Score: 0.3074\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram without stemming or removing stop words\n",
    "# bigram - without stem - without stop words  - done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")\n",
    "\n",
    "# 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                           Tokenized Title  Class Index\n",
      "0                              [erni, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "                     Tokenized Title  Class Index\n",
      "0                      [lajo, draho]            4\n",
      "1            [uss, huntsvil, (1857)]            5\n",
      "2                           [scafco]            0\n",
      "3                 [mclean', mansion]            6\n",
      "4  [avioan, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 113916\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 113916)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 89.24%\n",
      "\n",
      "Class 0 -> Precision: 0.9297, Recall: 0.8346, F1-Score: 0.8796\n",
      "Class 1 -> Precision: 0.8897, Recall: 0.9805, F1-Score: 0.9329\n",
      "Class 2 -> Precision: 0.8585, Recall: 0.7574, F1-Score: 0.8048\n",
      "Class 3 -> Precision: 0.8533, Recall: 0.8322, F1-Score: 0.8426\n",
      "Class 4 -> Precision: 0.7903, Recall: 0.8641, F1-Score: 0.8255\n",
      "Class 5 -> Precision: 0.9550, Recall: 0.9339, F1-Score: 0.9443\n",
      "Class 6 -> Precision: 0.8793, Recall: 0.9215, F1-Score: 0.8999\n",
      "Class 7 -> Precision: 0.9640, Recall: 0.9610, F1-Score: 0.9625\n",
      "Class 8 -> Precision: 0.9823, Recall: 0.9773, F1-Score: 0.9798\n",
      "Class 9 -> Precision: 0.9932, Recall: 0.9615, F1-Score: 0.9771\n",
      "Class 10 -> Precision: 0.9891, Recall: 0.9901, F1-Score: 0.9896\n",
      "Class 11 -> Precision: 0.7939, Recall: 0.8732, F1-Score: 0.8317\n",
      "Class 12 -> Precision: 0.8453, Recall: 0.7924, F1-Score: 0.8180\n",
      "Class 13 -> Precision: 0.7936, Recall: 0.8136, F1-Score: 0.8035\n",
      "\n",
      "Macro-Average F1 Score: 0.8923\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 65.54%\n",
      "\n",
      "Class 0 -> Precision: 0.2880, Recall: 0.7400, F1-Score: 0.4146\n",
      "Class 1 -> Precision: 0.8271, Recall: 0.9700, F1-Score: 0.8929\n",
      "Class 2 -> Precision: 0.4666, Recall: 0.3440, F1-Score: 0.3960\n",
      "Class 3 -> Precision: 0.5427, Recall: 0.4776, F1-Score: 0.5081\n",
      "Class 4 -> Precision: 0.5350, Recall: 0.5416, F1-Score: 0.5383\n",
      "Class 5 -> Precision: 0.9033, Recall: 0.8216, F1-Score: 0.8605\n",
      "Class 6 -> Precision: 0.8102, Recall: 0.8640, F1-Score: 0.8362\n",
      "Class 7 -> Precision: 0.9015, Recall: 0.7728, F1-Score: 0.8322\n",
      "Class 8 -> Precision: 0.9285, Recall: 0.5456, F1-Score: 0.6873\n",
      "Class 9 -> Precision: 0.9375, Recall: 0.4980, F1-Score: 0.6505\n",
      "Class 10 -> Precision: 0.9419, Recall: 0.7460, F1-Score: 0.8326\n",
      "Class 11 -> Precision: 0.6475, Recall: 0.7008, F1-Score: 0.6731\n",
      "Class 12 -> Precision: 0.6591, Recall: 0.5452, F1-Score: 0.5968\n",
      "Class 13 -> Precision: 0.6244, Recall: 0.6084, F1-Score: 0.6163\n",
      "\n",
      "Macro-Average F1 Score: 0.6668\n"
     ]
    }
   ],
   "source": [
    "# unigram - with stem - without stop words\n",
    "# Question 4 :: uigram with stemming and removing stop words\n",
    "# 4. uigram - with stem - with stop words        -done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = [1])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                           Tokenized Title  Class Index\n",
      "0                             [ernie, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "                      Tokenized Title  Class Index\n",
      "0                     [lajos, drahos]            4\n",
      "1           [uss, huntsville, (1857)]            5\n",
      "2                            [scafco]            0\n",
      "3                 [mclean's, mansion]            6\n",
      "4  [avioane, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 120113\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 120113)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 90.19%\n",
      "\n",
      "Class 0 -> Precision: 0.9281, Recall: 0.8698, F1-Score: 0.8980\n",
      "Class 1 -> Precision: 0.8936, Recall: 0.9818, F1-Score: 0.9356\n",
      "Class 2 -> Precision: 0.8631, Recall: 0.7733, F1-Score: 0.8157\n",
      "Class 3 -> Precision: 0.8553, Recall: 0.8454, F1-Score: 0.8503\n",
      "Class 4 -> Precision: 0.7934, Recall: 0.8768, F1-Score: 0.8330\n",
      "Class 5 -> Precision: 0.9511, Recall: 0.9492, F1-Score: 0.9502\n",
      "Class 6 -> Precision: 0.8673, Recall: 0.9335, F1-Score: 0.8992\n",
      "Class 7 -> Precision: 0.9611, Recall: 0.9685, F1-Score: 0.9648\n",
      "Class 8 -> Precision: 0.9823, Recall: 0.9800, F1-Score: 0.9811\n",
      "Class 9 -> Precision: 0.9918, Recall: 0.9662, F1-Score: 0.9788\n",
      "Class 10 -> Precision: 0.9892, Recall: 0.9917, F1-Score: 0.9905\n",
      "Class 11 -> Precision: 0.8418, Recall: 0.8737, F1-Score: 0.8575\n",
      "Class 12 -> Precision: 0.8503, Recall: 0.8214, F1-Score: 0.8356\n",
      "Class 13 -> Precision: 0.8692, Recall: 0.7947, F1-Score: 0.8303\n",
      "\n",
      "Macro-Average F1 Score: 0.9015\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 65.23%\n",
      "\n",
      "Class 0 -> Precision: 0.2821, Recall: 0.7752, F1-Score: 0.4137\n",
      "Class 1 -> Precision: 0.8316, Recall: 0.9680, F1-Score: 0.8946\n",
      "Class 2 -> Precision: 0.4644, Recall: 0.3440, F1-Score: 0.3952\n",
      "Class 3 -> Precision: 0.5417, Recall: 0.4808, F1-Score: 0.5094\n",
      "Class 4 -> Precision: 0.5271, Recall: 0.5404, F1-Score: 0.5337\n",
      "Class 5 -> Precision: 0.8968, Recall: 0.8344, F1-Score: 0.8645\n",
      "Class 6 -> Precision: 0.7918, Recall: 0.8716, F1-Score: 0.8298\n",
      "Class 7 -> Precision: 0.8941, Recall: 0.7696, F1-Score: 0.8272\n",
      "Class 8 -> Precision: 0.9265, Recall: 0.5500, F1-Score: 0.6903\n",
      "Class 9 -> Precision: 0.9309, Recall: 0.5008, F1-Score: 0.6512\n",
      "Class 10 -> Precision: 0.9440, Recall: 0.7484, F1-Score: 0.8349\n",
      "Class 11 -> Precision: 0.6704, Recall: 0.6640, F1-Score: 0.6672\n",
      "Class 12 -> Precision: 0.6597, Recall: 0.5536, F1-Score: 0.6020\n",
      "Class 13 -> Precision: 0.6748, Recall: 0.5320, F1-Score: 0.5949\n",
      "\n",
      "Macro-Average F1 Score: 0.6649\n"
     ]
    }
   ],
   "source": [
    "# 4. unigram - without stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = [1])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = [1])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                                         [erni cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                                   Tokenized Title  Class Index\n",
      "0                                     [lajo draho]            4\n",
      "1                  [uss huntsvil, huntsvil (1857)]            5\n",
      "2                                               []            0\n",
      "3                                [mclean' mansion]            6\n",
      "4  [avioan craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 187030\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 187030)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 90.24%\n",
      "\n",
      "Class 0 -> Precision: 0.4437, Recall: 0.9880, F1-Score: 0.6124\n",
      "Class 1 -> Precision: 0.9799, Recall: 0.9886, F1-Score: 0.9842\n",
      "Class 2 -> Precision: 0.9993, Recall: 0.9690, F1-Score: 0.9839\n",
      "Class 3 -> Precision: 0.9984, Recall: 0.9969, F1-Score: 0.9976\n",
      "Class 4 -> Precision: 0.9971, Recall: 0.9950, F1-Score: 0.9960\n",
      "Class 5 -> Precision: 0.9978, Recall: 0.9812, F1-Score: 0.9894\n",
      "Class 6 -> Precision: 0.9882, Recall: 0.9671, F1-Score: 0.9776\n",
      "Class 7 -> Precision: 0.9979, Recall: 0.8605, F1-Score: 0.9241\n",
      "Class 8 -> Precision: 0.9979, Recall: 0.6277, F1-Score: 0.7707\n",
      "Class 9 -> Precision: 0.9996, Recall: 0.7374, F1-Score: 0.8487\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.7805, F1-Score: 0.8767\n",
      "Class 11 -> Precision: 0.9781, Recall: 0.9182, F1-Score: 0.9472\n",
      "Class 12 -> Precision: 0.9664, Recall: 0.9070, F1-Score: 0.9358\n",
      "Class 13 -> Precision: 0.9646, Recall: 0.9159, F1-Score: 0.9396\n",
      "\n",
      "Macro-Average F1 Score: 0.9131\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 28.50%\n",
      "\n",
      "Class 0 -> Precision: 0.0933, Recall: 0.9416, F1-Score: 0.1698\n",
      "Class 1 -> Precision: 0.8982, Recall: 0.7728, F1-Score: 0.8308\n",
      "Class 2 -> Precision: 0.4370, Recall: 0.0236, F1-Score: 0.0448\n",
      "Class 3 -> Precision: 0.8327, Recall: 0.0896, F1-Score: 0.1618\n",
      "Class 4 -> Precision: 0.7290, Recall: 0.1356, F1-Score: 0.2287\n",
      "Class 5 -> Precision: 0.9200, Recall: 0.2392, F1-Score: 0.3797\n",
      "Class 6 -> Precision: 0.8617, Recall: 0.4636, F1-Score: 0.6029\n",
      "Class 7 -> Precision: 0.9204, Recall: 0.1896, F1-Score: 0.3144\n",
      "Class 8 -> Precision: 0.9739, Recall: 0.1944, F1-Score: 0.3241\n",
      "Class 9 -> Precision: 0.8462, Recall: 0.0220, F1-Score: 0.0429\n",
      "Class 10 -> Precision: 0.9636, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.6849, Recall: 0.2948, F1-Score: 0.4122\n",
      "Class 12 -> Precision: 0.6047, Recall: 0.3316, F1-Score: 0.4283\n",
      "Class 13 -> Precision: 0.5728, Recall: 0.2708, F1-Score: 0.3677\n",
      "\n",
      "Macro-Average F1 Score: 0.3107\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - with stem - without stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = [2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                                        [ernie cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                                    Tokenized Title  Class Index\n",
      "0                                    [lajos drahos]            4\n",
      "1               [uss huntsville, huntsville (1857)]            5\n",
      "2                                                []            0\n",
      "3                                [mclean's mansion]            6\n",
      "4  [avioane craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 179585\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 179585)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 90.02%\n",
      "\n",
      "Class 0 -> Precision: 0.4235, Recall: 0.9947, F1-Score: 0.5941\n",
      "Class 1 -> Precision: 0.9874, Recall: 0.9899, F1-Score: 0.9887\n",
      "Class 2 -> Precision: 0.9994, Recall: 0.9657, F1-Score: 0.9823\n",
      "Class 3 -> Precision: 0.9987, Recall: 0.9946, F1-Score: 0.9966\n",
      "Class 4 -> Precision: 0.9975, Recall: 0.9940, F1-Score: 0.9957\n",
      "Class 5 -> Precision: 0.9978, Recall: 0.9819, F1-Score: 0.9898\n",
      "Class 6 -> Precision: 0.9907, Recall: 0.9717, F1-Score: 0.9811\n",
      "Class 7 -> Precision: 0.9977, Recall: 0.8599, F1-Score: 0.9237\n",
      "Class 8 -> Precision: 0.9967, Recall: 0.6259, F1-Score: 0.7689\n",
      "Class 9 -> Precision: 0.9999, Recall: 0.7371, F1-Score: 0.8486\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.7806, F1-Score: 0.8768\n",
      "Class 11 -> Precision: 0.9965, Recall: 0.9040, F1-Score: 0.9480\n",
      "Class 12 -> Precision: 0.9958, Recall: 0.8939, F1-Score: 0.9421\n",
      "Class 13 -> Precision: 0.9956, Recall: 0.9087, F1-Score: 0.9502\n",
      "\n",
      "Macro-Average F1 Score: 0.9133\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 25.21%\n",
      "\n",
      "Class 0 -> Precision: 0.0870, Recall: 0.9576, F1-Score: 0.1594\n",
      "Class 1 -> Precision: 0.9136, Recall: 0.7360, F1-Score: 0.8152\n",
      "Class 2 -> Precision: 0.4240, Recall: 0.0212, F1-Score: 0.0404\n",
      "Class 3 -> Precision: 0.8358, Recall: 0.0896, F1-Score: 0.1618\n",
      "Class 4 -> Precision: 0.7269, Recall: 0.1320, F1-Score: 0.2234\n",
      "Class 5 -> Precision: 0.9198, Recall: 0.2384, F1-Score: 0.3787\n",
      "Class 6 -> Precision: 0.8683, Recall: 0.4352, F1-Score: 0.5798\n",
      "Class 7 -> Precision: 0.9124, Recall: 0.1832, F1-Score: 0.3051\n",
      "Class 8 -> Precision: 0.9797, Recall: 0.1932, F1-Score: 0.3228\n",
      "Class 9 -> Precision: 0.8571, Recall: 0.0216, F1-Score: 0.0421\n",
      "Class 10 -> Precision: 0.9636, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.8132, Recall: 0.1672, F1-Score: 0.2774\n",
      "Class 12 -> Precision: 0.8023, Recall: 0.2256, F1-Score: 0.3522\n",
      "Class 13 -> Precision: 0.7173, Recall: 0.1076, F1-Score: 0.1871\n",
      "\n",
      "Macro-Average F1 Score: 0.2776\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - without stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = [2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = [2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                            [ernie, cox, ernie cox]            3\n",
      "1                                        [holosteum]           10\n",
      "2        [pestarella, tyrrhena, pestarella tyrrhena]            9\n",
      "3  [midsun, junior, high, school, midsun junior, ...            1\n",
      "4  [st, james', church, wrightington, bar, st jam...            6\n",
      "testingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                      [lajos, drahos, lajos drahos]            4\n",
      "1  [uss, huntsville, (1857), uss huntsville, hunt...            5\n",
      "2                                           [scafco]            0\n",
      "3              [mclean's, mansion, mclean's mansion]            6\n",
      "4  [avioane, craiova, iar-93, vultur, avioane cra...            5\n",
      "--------BI+UNI+GRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 309323\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 309323)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 96.05%\n",
      "\n",
      "Class 0 -> Precision: 0.9771, Recall: 0.9231, F1-Score: 0.9493\n",
      "Class 1 -> Precision: 0.9395, Recall: 0.9912, F1-Score: 0.9647\n",
      "Class 2 -> Precision: 0.9678, Recall: 0.9219, F1-Score: 0.9443\n",
      "Class 3 -> Precision: 0.9625, Recall: 0.9484, F1-Score: 0.9554\n",
      "Class 4 -> Precision: 0.9285, Recall: 0.9663, F1-Score: 0.9470\n",
      "Class 5 -> Precision: 0.9765, Recall: 0.9764, F1-Score: 0.9764\n",
      "Class 6 -> Precision: 0.9400, Recall: 0.9609, F1-Score: 0.9504\n",
      "Class 7 -> Precision: 0.9835, Recall: 0.9846, F1-Score: 0.9841\n",
      "Class 8 -> Precision: 0.9946, Recall: 0.9892, F1-Score: 0.9919\n",
      "Class 9 -> Precision: 0.9974, Recall: 0.9842, F1-Score: 0.9907\n",
      "Class 10 -> Precision: 0.9966, Recall: 0.9945, F1-Score: 0.9955\n",
      "Class 11 -> Precision: 0.9273, Recall: 0.9510, F1-Score: 0.9390\n",
      "Class 12 -> Precision: 0.9426, Recall: 0.9285, F1-Score: 0.9355\n",
      "Class 13 -> Precision: 0.9194, Recall: 0.9272, F1-Score: 0.9233\n",
      "\n",
      "Macro-Average F1 Score: 0.9605\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 65.92%\n",
      "\n",
      "Class 0 -> Precision: 0.2913, Recall: 0.7684, F1-Score: 0.4225\n",
      "Class 1 -> Precision: 0.8309, Recall: 0.9692, F1-Score: 0.8948\n",
      "Class 2 -> Precision: 0.4710, Recall: 0.3408, F1-Score: 0.3955\n",
      "Class 3 -> Precision: 0.5467, Recall: 0.4800, F1-Score: 0.5112\n",
      "Class 4 -> Precision: 0.5316, Recall: 0.5388, F1-Score: 0.5352\n",
      "Class 5 -> Precision: 0.9082, Recall: 0.8352, F1-Score: 0.8702\n",
      "Class 6 -> Precision: 0.8124, Recall: 0.8712, F1-Score: 0.8408\n",
      "Class 7 -> Precision: 0.9032, Recall: 0.7692, F1-Score: 0.8308\n",
      "Class 8 -> Precision: 0.9342, Recall: 0.5512, F1-Score: 0.6933\n",
      "Class 9 -> Precision: 0.9400, Recall: 0.5016, F1-Score: 0.6541\n",
      "Class 10 -> Precision: 0.9448, Recall: 0.7468, F1-Score: 0.8342\n",
      "Class 11 -> Precision: 0.6665, Recall: 0.6908, F1-Score: 0.6785\n",
      "Class 12 -> Precision: 0.6603, Recall: 0.5644, F1-Score: 0.6086\n",
      "Class 13 -> Precision: 0.6446, Recall: 0.6008, F1-Score: 0.6219\n",
      "\n",
      "Macro-Average F1 Score: 0.6708\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram+unigram - without stem - without stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1,2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = [1,2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BI+UNI+GRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BI+UNI+GRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                              [erni, cox, erni cox]            3\n",
      "1                                        [holosteum]           10\n",
      "2        [pestarella, tyrrhena, pestarella tyrrhena]            9\n",
      "3  [midsun, junior, high, school, midsun junior, ...            1\n",
      "4  [st, james', church, wrightington, bar, st jam...            6\n",
      "testingData.head()\n",
      "                                     Tokenized Title  Class Index\n",
      "0                          [lajo, draho, lajo draho]            4\n",
      "1  [uss, huntsvil, (1857), uss huntsvil, huntsvil...            5\n",
      "2                                           [scafco]            0\n",
      "3                [mclean', mansion, mclean' mansion]            6\n",
      "4  [avioan, craiova, iar-93, vultur, avioan craio...            5\n",
      "--------BI+UNI+GRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 292286\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 292286)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 95.35%\n",
      "\n",
      "Class 0 -> Precision: 0.9542, Recall: 0.9160, F1-Score: 0.9347\n",
      "Class 1 -> Precision: 0.9336, Recall: 0.9914, F1-Score: 0.9616\n",
      "Class 2 -> Precision: 0.9570, Recall: 0.9169, F1-Score: 0.9365\n",
      "Class 3 -> Precision: 0.9561, Recall: 0.9416, F1-Score: 0.9488\n",
      "Class 4 -> Precision: 0.9199, Recall: 0.9607, F1-Score: 0.9399\n",
      "Class 5 -> Precision: 0.9712, Recall: 0.9751, F1-Score: 0.9732\n",
      "Class 6 -> Precision: 0.9193, Recall: 0.9636, F1-Score: 0.9409\n",
      "Class 7 -> Precision: 0.9739, Recall: 0.9850, F1-Score: 0.9794\n",
      "Class 8 -> Precision: 0.9933, Recall: 0.9877, F1-Score: 0.9905\n",
      "Class 9 -> Precision: 0.9953, Recall: 0.9813, F1-Score: 0.9883\n",
      "Class 10 -> Precision: 0.9956, Recall: 0.9939, F1-Score: 0.9947\n",
      "Class 11 -> Precision: 0.9176, Recall: 0.9349, F1-Score: 0.9261\n",
      "Class 12 -> Precision: 0.9324, Recall: 0.9075, F1-Score: 0.9198\n",
      "Class 13 -> Precision: 0.9341, Recall: 0.8935, F1-Score: 0.9134\n",
      "\n",
      "Macro-Average F1 Score: 0.9534\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 65.01%\n",
      "\n",
      "Class 0 -> Precision: 0.2813, Recall: 0.7476, F1-Score: 0.4088\n",
      "Class 1 -> Precision: 0.8211, Recall: 0.9692, F1-Score: 0.8890\n",
      "Class 2 -> Precision: 0.4602, Recall: 0.3448, F1-Score: 0.3942\n",
      "Class 3 -> Precision: 0.5381, Recall: 0.4772, F1-Score: 0.5058\n",
      "Class 4 -> Precision: 0.5293, Recall: 0.5424, F1-Score: 0.5358\n",
      "Class 5 -> Precision: 0.8944, Recall: 0.8296, F1-Score: 0.8608\n",
      "Class 6 -> Precision: 0.7856, Recall: 0.8748, F1-Score: 0.8278\n",
      "Class 7 -> Precision: 0.8938, Recall: 0.7740, F1-Score: 0.8296\n",
      "Class 8 -> Precision: 0.9229, Recall: 0.5504, F1-Score: 0.6896\n",
      "Class 9 -> Precision: 0.9292, Recall: 0.4984, F1-Score: 0.6488\n",
      "Class 10 -> Precision: 0.9425, Recall: 0.7468, F1-Score: 0.8333\n",
      "Class 11 -> Precision: 0.6553, Recall: 0.6708, F1-Score: 0.6630\n",
      "Class 12 -> Precision: 0.6506, Recall: 0.5548, F1-Score: 0.5989\n",
      "Class 13 -> Precision: 0.6572, Recall: 0.5208, F1-Score: 0.5811\n",
      "\n",
      "Macro-Average F1 Score: 0.6619\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram+unigram - with stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1,2])\n",
    "df_test, _ = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = [1,2])\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BI+UNI+GRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BI+UNI+GRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
