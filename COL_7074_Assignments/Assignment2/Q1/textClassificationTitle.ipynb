{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from naive_bayes import NaiveBayes\n",
    "from utils import tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary, getTrainingAndTestingData, run_model, plot_wordclouds_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape : (140000, 3)\n",
      "   label                              title  \\\n",
      "0      3                          Ernie Cox   \n",
      "1     10                          Holosteum   \n",
      "2      9                Pestarella tyrrhena   \n",
      "3      1          MidSun Junior High School   \n",
      "4      6  St James' Church Wrightington Bar   \n",
      "\n",
      "                                             content  \n",
      "0   Ernest Ernie Cox (February 17 1894 â€“ February...  \n",
      "1   Holosteum is a genus of plants in the Pink fa...  \n",
      "2   Pestarella tyrrhena (formerly Callianassa tyr...  \n",
      "3   MidSun Junior High School is a Canadian middl...  \n",
      "4   St James' Church Wrightington Bar is in Churc...  \n",
      "df_test.shape : (35000, 3)\n",
      "   label                          title  \\\n",
      "0      4                   Lajos Drahos   \n",
      "1      5          USS Huntsville (1857)   \n",
      "2      0                         SCAFCO   \n",
      "3      6               McLean's Mansion   \n",
      "4      5  Avioane Craiova IAR-93 Vultur   \n",
      "\n",
      "                                             content  \n",
      "0   Lajos Drahos (7 March 1895 - 2 June 1983) was...  \n",
      "1   USS Huntsville was a steamer acquired by the ...  \n",
      "2   Founded in 1954 by Ben G. Stone SCAFCO Corpor...  \n",
      "3   McLean's Mansion (originally Holly Lea) is a ...  \n",
      "4   The Avioane Craiova IAR-93 Vultur (Eagle) is ...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "print(f\"df_train.shape : {df_train.shape}\")\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(f\"df_test.shape : {df_test.shape}\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                           Tokenized Title  Class Index\n",
      "0                             [ernie, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "                      Tokenized Title  Class Index\n",
      "0                     [lajos, drahos]            4\n",
      "1           [uss, huntsville, (1857)]            5\n",
      "2                            [scafco]            0\n",
      "3                 [mclean's, mansion]            6\n",
      "4  [avioane, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 41177\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 41177)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 66.10%\n",
      "\n",
      "Class 0 -> Precision: 0.2373, Recall: 0.8308, F1-Score: 0.3691\n",
      "Class 1 -> Precision: 0.8853, Recall: 0.9580, F1-Score: 0.9202\n",
      "Class 2 -> Precision: 0.6358, Recall: 0.4364, F1-Score: 0.5176\n",
      "Class 3 -> Precision: 0.6506, Recall: 0.5342, F1-Score: 0.5867\n",
      "Class 4 -> Precision: 0.6351, Recall: 0.6023, F1-Score: 0.6183\n",
      "Class 5 -> Precision: 0.9246, Recall: 0.8177, F1-Score: 0.8679\n",
      "Class 6 -> Precision: 0.8666, Recall: 0.8613, F1-Score: 0.8639\n",
      "Class 7 -> Precision: 0.9197, Recall: 0.7641, F1-Score: 0.8347\n",
      "Class 8 -> Precision: 0.9356, Recall: 0.4908, F1-Score: 0.6438\n",
      "Class 9 -> Precision: 0.9439, Recall: 0.3783, F1-Score: 0.5401\n",
      "Class 10 -> Precision: 0.9634, Recall: 0.5942, F1-Score: 0.7350\n",
      "Class 11 -> Precision: 0.7476, Recall: 0.7318, F1-Score: 0.7396\n",
      "Class 12 -> Precision: 0.7458, Recall: 0.5975, F1-Score: 0.6634\n",
      "Class 13 -> Precision: 0.7066, Recall: 0.6566, F1-Score: 0.6807\n",
      "\n",
      "Macro-Average F1 Score: 0.6844\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 67.04%\n",
      "\n",
      "Class 0 -> Precision: 0.8034, Recall: 0.5428, F1-Score: 0.6479\n",
      "Class 1 -> Precision: 0.8827, Recall: 0.9628, F1-Score: 0.9210\n",
      "Class 2 -> Precision: 0.4598, Recall: 0.3504, F1-Score: 0.3977\n",
      "Class 3 -> Precision: 0.5278, Recall: 0.4928, F1-Score: 0.5097\n",
      "Class 4 -> Precision: 0.5384, Recall: 0.5248, F1-Score: 0.5315\n",
      "Class 5 -> Precision: 0.9022, Recall: 0.8372, F1-Score: 0.8685\n",
      "Class 6 -> Precision: 0.8560, Recall: 0.8560, F1-Score: 0.8560\n",
      "Class 7 -> Precision: 0.8810, Recall: 0.7820, F1-Score: 0.8286\n",
      "Class 8 -> Precision: 0.9173, Recall: 0.5548, F1-Score: 0.6914\n",
      "Class 9 -> Precision: 0.3472, Recall: 0.9188, F1-Score: 0.5039\n",
      "Class 10 -> Precision: 0.9406, Recall: 0.7468, F1-Score: 0.8326\n",
      "Class 11 -> Precision: 0.6898, Recall: 0.6768, F1-Score: 0.6832\n",
      "Class 12 -> Precision: 0.6713, Recall: 0.5360, F1-Score: 0.5961\n",
      "Class 13 -> Precision: 0.6537, Recall: 0.6032, F1-Score: 0.6274\n",
      "\n",
      "Macro-Average F1 Score: 0.6783\n"
     ]
    }
   ],
   "source": [
    "# Question Part 1 :: Unigram without stemming and removing stop words\n",
    "# Train the model with context only corresponding to the labels\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = 1)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "    \n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayes() \n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_wordclouds_per_class(trainingData, maxWords = 200, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word cloud on training data\n",
      "Word cloud on testing data\n",
      "trainingData.head()\n",
      "                     Tokenized Description  Class Index\n",
      "0                              [erni, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "               Tokenized Description  Class Index\n",
      "0                      [lajo, draho]            4\n",
      "1            [uss, huntsvil, (1857)]            5\n",
      "2                           [scafco]            0\n",
      "3                 [mclean', mansion]            6\n",
      "4  [avioan, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 39106\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 39106)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 65.29%\n",
      "\n",
      "Class 0 -> Precision: 0.2309, Recall: 0.8125, F1-Score: 0.3596\n",
      "Class 1 -> Precision: 0.8732, Recall: 0.9560, F1-Score: 0.9127\n",
      "Class 2 -> Precision: 0.6261, Recall: 0.4412, F1-Score: 0.5176\n",
      "Class 3 -> Precision: 0.6460, Recall: 0.5318, F1-Score: 0.5834\n",
      "Class 4 -> Precision: 0.6242, Recall: 0.6057, F1-Score: 0.6148\n",
      "Class 5 -> Precision: 0.9132, Recall: 0.8181, F1-Score: 0.8630\n",
      "Class 6 -> Precision: 0.8375, Recall: 0.8635, F1-Score: 0.8503\n",
      "Class 7 -> Precision: 0.9050, Recall: 0.7695, F1-Score: 0.8318\n",
      "Class 8 -> Precision: 0.9331, Recall: 0.4894, F1-Score: 0.6420\n",
      "Class 9 -> Precision: 0.9342, Recall: 0.3777, F1-Score: 0.5379\n",
      "Class 10 -> Precision: 0.9606, Recall: 0.5954, F1-Score: 0.7352\n",
      "Class 11 -> Precision: 0.7377, Recall: 0.7078, F1-Score: 0.7224\n",
      "Class 12 -> Precision: 0.7215, Recall: 0.5963, F1-Score: 0.6529\n",
      "Class 13 -> Precision: 0.7425, Recall: 0.5760, F1-Score: 0.6487\n",
      "\n",
      "Macro-Average F1 Score: 0.6766\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 66.14%\n",
      "\n",
      "Class 0 -> Precision: 0.7332, Recall: 0.5276, F1-Score: 0.6136\n",
      "Class 1 -> Precision: 0.8686, Recall: 0.9628, F1-Score: 0.9133\n",
      "Class 2 -> Precision: 0.4509, Recall: 0.3544, F1-Score: 0.3969\n",
      "Class 3 -> Precision: 0.5210, Recall: 0.4860, F1-Score: 0.5029\n",
      "Class 4 -> Precision: 0.5322, Recall: 0.5252, F1-Score: 0.5287\n",
      "Class 5 -> Precision: 0.8855, Recall: 0.8320, F1-Score: 0.8579\n",
      "Class 6 -> Precision: 0.8250, Recall: 0.8524, F1-Score: 0.8385\n",
      "Class 7 -> Precision: 0.8722, Recall: 0.7808, F1-Score: 0.8240\n",
      "Class 8 -> Precision: 0.9069, Recall: 0.5536, F1-Score: 0.6875\n",
      "Class 9 -> Precision: 0.3509, Recall: 0.9068, F1-Score: 0.5060\n",
      "Class 10 -> Precision: 0.9368, Recall: 0.7472, F1-Score: 0.8313\n",
      "Class 11 -> Precision: 0.6774, Recall: 0.6576, F1-Score: 0.6673\n",
      "Class 12 -> Precision: 0.6541, Recall: 0.5476, F1-Score: 0.5961\n",
      "Class 13 -> Precision: 0.6588, Recall: 0.5260, F1-Score: 0.5850\n",
      "\n",
      "Macro-Average F1 Score: 0.6678\n"
     ]
    }
   ],
   "source": [
    "# Question 1, part 2 :: Unigram with stemming and removing stop words\n",
    "\n",
    "# # (a) Perform stemming and remove the stop-words in the training as well as the validation data.\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = 1)\n",
    "\n",
    "# (b) Construct word clouds for both classes on the transformed data\n",
    "print(\"Word cloud on training data\")\n",
    "# plot_wordclouds_per_class(trainingData, data_type = \"Training Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "print(\"Word cloud on testing data\")\n",
    "# plot_wordclouds_per_class(testingData, data_type = \"Testing Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "# (c) Learn a new model on the transformed data. Report the validation set accuracy\n",
    "# 2. unigram - with stem - with stop words       - done\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")\n",
    "\n",
    "# 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0                                         [erni cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                             Tokenized Description  Class Index\n",
      "0                                     [lajo draho]            4\n",
      "1                  [uss huntsvil, huntsvil (1857)]            5\n",
      "2                                               []            0\n",
      "3                                [mclean' mansion]            6\n",
      "4  [avioan craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 47973\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 47973)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 21.34%\n",
      "\n",
      "Class 0 -> Precision: 0.0830, Recall: 0.9847, F1-Score: 0.1531\n",
      "Class 1 -> Precision: 0.9555, Recall: 0.6676, F1-Score: 0.7860\n",
      "Class 2 -> Precision: 0.7432, Recall: 0.0191, F1-Score: 0.0372\n",
      "Class 3 -> Precision: 0.9325, Recall: 0.0635, F1-Score: 0.1189\n",
      "Class 4 -> Precision: 0.8741, Recall: 0.0937, F1-Score: 0.1693\n",
      "Class 5 -> Precision: 0.9567, Recall: 0.1568, F1-Score: 0.2694\n",
      "Class 6 -> Precision: 0.9253, Recall: 0.3543, F1-Score: 0.5124\n",
      "Class 7 -> Precision: 0.9186, Recall: 0.1230, F1-Score: 0.2170\n",
      "Class 8 -> Precision: 0.9573, Recall: 0.1367, F1-Score: 0.2392\n",
      "Class 9 -> Precision: 0.8947, Recall: 0.0085, F1-Score: 0.0168\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.0139, F1-Score: 0.0274\n",
      "Class 11 -> Precision: 0.9074, Recall: 0.1029, F1-Score: 0.1848\n",
      "Class 12 -> Precision: 0.9686, Recall: 0.1879, F1-Score: 0.3147\n",
      "Class 13 -> Precision: 0.8850, Recall: 0.0754, F1-Score: 0.1390\n",
      "\n",
      "Macro-Average F1 Score: 0.2275\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 26.25%\n",
      "\n",
      "Class 0 -> Precision: 0.2025, Recall: 0.3540, F1-Score: 0.2576\n",
      "Class 1 -> Precision: 0.9229, Recall: 0.7324, F1-Score: 0.8167\n",
      "Class 2 -> Precision: 0.4242, Recall: 0.0224, F1-Score: 0.0426\n",
      "Class 3 -> Precision: 0.8266, Recall: 0.0896, F1-Score: 0.1617\n",
      "Class 4 -> Precision: 0.7122, Recall: 0.1336, F1-Score: 0.2250\n",
      "Class 5 -> Precision: 0.9159, Recall: 0.2396, F1-Score: 0.3798\n",
      "Class 6 -> Precision: 0.8670, Recall: 0.4408, F1-Score: 0.5845\n",
      "Class 7 -> Precision: 0.9015, Recall: 0.1868, F1-Score: 0.3095\n",
      "Class 8 -> Precision: 0.9777, Recall: 0.1928, F1-Score: 0.3221\n",
      "Class 9 -> Precision: 0.0793, Recall: 0.7320, F1-Score: 0.1432\n",
      "Class 10 -> Precision: 0.9464, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.7785, Recall: 0.1828, F1-Score: 0.2961\n",
      "Class 12 -> Precision: 0.8144, Recall: 0.2300, F1-Score: 0.3587\n",
      "Class 13 -> Precision: 0.6936, Recall: 0.1168, F1-Score: 0.1999\n",
      "\n",
      "Macro-Average F1 Score: 0.2956\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram with stemming and removing stop words\n",
    "# 3. bigram - with stem - with stop words        -done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = True, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")\n",
    "\n",
    "# 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0                                        [ernie cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                              Tokenized Description  Class Index\n",
      "0                                    [lajos drahos]            4\n",
      "1               [uss huntsville, huntsville (1857)]            5\n",
      "2                                                []            0\n",
      "3                                [mclean's mansion]            6\n",
      "4  [avioane craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 51893\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 51893)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 24.90%\n",
      "\n",
      "Class 0 -> Precision: 0.0879, Recall: 0.9756, F1-Score: 0.1613\n",
      "Class 1 -> Precision: 0.9473, Recall: 0.7184, F1-Score: 0.8171\n",
      "Class 2 -> Precision: 0.7529, Recall: 0.0192, F1-Score: 0.0374\n",
      "Class 3 -> Precision: 0.9310, Recall: 0.0634, F1-Score: 0.1187\n",
      "Class 4 -> Precision: 0.8750, Recall: 0.0945, F1-Score: 0.1706\n",
      "Class 5 -> Precision: 0.9545, Recall: 0.1574, F1-Score: 0.2702\n",
      "Class 6 -> Precision: 0.9164, Recall: 0.3773, F1-Score: 0.5345\n",
      "Class 7 -> Precision: 0.9165, Recall: 0.1207, F1-Score: 0.2133\n",
      "Class 8 -> Precision: 0.9710, Recall: 0.1374, F1-Score: 0.2407\n",
      "Class 9 -> Precision: 0.9341, Recall: 0.0085, F1-Score: 0.0168\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.0138, F1-Score: 0.0272\n",
      "Class 11 -> Precision: 0.7491, Recall: 0.2574, F1-Score: 0.3831\n",
      "Class 12 -> Precision: 0.8090, Recall: 0.2885, F1-Score: 0.4253\n",
      "Class 13 -> Precision: 0.6860, Recall: 0.2537, F1-Score: 0.3704\n",
      "\n",
      "Macro-Average F1 Score: 0.2705\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 28.99%\n",
      "\n",
      "Class 0 -> Precision: 0.2154, Recall: 0.3512, F1-Score: 0.2670\n",
      "Class 1 -> Precision: 0.9270, Recall: 0.7620, F1-Score: 0.8364\n",
      "Class 2 -> Precision: 0.4308, Recall: 0.0224, F1-Score: 0.0426\n",
      "Class 3 -> Precision: 0.8296, Recall: 0.0896, F1-Score: 0.1617\n",
      "Class 4 -> Precision: 0.7249, Recall: 0.1360, F1-Score: 0.2290\n",
      "Class 5 -> Precision: 0.9172, Recall: 0.2392, F1-Score: 0.3794\n",
      "Class 6 -> Precision: 0.8685, Recall: 0.4544, F1-Score: 0.5966\n",
      "Class 7 -> Precision: 0.9082, Recall: 0.1860, F1-Score: 0.3088\n",
      "Class 8 -> Precision: 0.9838, Recall: 0.1940, F1-Score: 0.3241\n",
      "Class 9 -> Precision: 0.0850, Recall: 0.7304, F1-Score: 0.1522\n",
      "Class 10 -> Precision: 0.9815, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.6440, Recall: 0.3068, F1-Score: 0.4156\n",
      "Class 12 -> Precision: 0.6517, Recall: 0.3024, F1-Score: 0.4131\n",
      "Class 13 -> Precision: 0.5739, Recall: 0.2624, F1-Score: 0.3601\n",
      "\n",
      "Macro-Average F1 Score: 0.3235\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram without stemming or removing stop words\n",
    "# bigram - without stem - without stop words  - done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = False, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")\n",
    "\n",
    "# 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                     Tokenized Description  Class Index\n",
      "0                              [erni, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "               Tokenized Description  Class Index\n",
      "0                      [lajo, draho]            4\n",
      "1            [uss, huntsvil, (1857)]            5\n",
      "2                           [scafco]            0\n",
      "3                 [mclean', mansion]            6\n",
      "4  [avioan, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 39218\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 39218)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 66.00%\n",
      "\n",
      "Class 0 -> Precision: 0.2377, Recall: 0.8107, F1-Score: 0.3676\n",
      "Class 1 -> Precision: 0.8799, Recall: 0.9571, F1-Score: 0.9169\n",
      "Class 2 -> Precision: 0.6325, Recall: 0.4429, F1-Score: 0.5210\n",
      "Class 3 -> Precision: 0.6512, Recall: 0.5354, F1-Score: 0.5876\n",
      "Class 4 -> Precision: 0.6313, Recall: 0.6046, F1-Score: 0.6177\n",
      "Class 5 -> Precision: 0.9227, Recall: 0.8117, F1-Score: 0.8636\n",
      "Class 6 -> Precision: 0.8583, Recall: 0.8568, F1-Score: 0.8575\n",
      "Class 7 -> Precision: 0.9134, Recall: 0.7680, F1-Score: 0.8344\n",
      "Class 8 -> Precision: 0.9363, Recall: 0.4908, F1-Score: 0.6440\n",
      "Class 9 -> Precision: 0.9417, Recall: 0.3798, F1-Score: 0.5413\n",
      "Class 10 -> Precision: 0.9614, Recall: 0.5951, F1-Score: 0.7351\n",
      "Class 11 -> Precision: 0.7305, Recall: 0.7355, F1-Score: 0.7330\n",
      "Class 12 -> Precision: 0.7377, Recall: 0.5993, F1-Score: 0.6613\n",
      "Class 13 -> Precision: 0.6961, Recall: 0.6525, F1-Score: 0.6736\n",
      "\n",
      "Macro-Average F1 Score: 0.6825\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 66.75%\n",
      "\n",
      "Class 0 -> Precision: 0.7809, Recall: 0.5288, F1-Score: 0.6306\n",
      "Class 1 -> Precision: 0.8751, Recall: 0.9644, F1-Score: 0.9176\n",
      "Class 2 -> Precision: 0.4538, Recall: 0.3552, F1-Score: 0.3985\n",
      "Class 3 -> Precision: 0.5233, Recall: 0.4888, F1-Score: 0.5055\n",
      "Class 4 -> Precision: 0.5357, Recall: 0.5224, F1-Score: 0.5290\n",
      "Class 5 -> Precision: 0.8948, Recall: 0.8304, F1-Score: 0.8614\n",
      "Class 6 -> Precision: 0.8526, Recall: 0.8516, F1-Score: 0.8521\n",
      "Class 7 -> Precision: 0.8775, Recall: 0.7824, F1-Score: 0.8272\n",
      "Class 8 -> Precision: 0.9102, Recall: 0.5556, F1-Score: 0.6900\n",
      "Class 9 -> Precision: 0.3531, Recall: 0.9092, F1-Score: 0.5087\n",
      "Class 10 -> Precision: 0.9362, Recall: 0.7460, F1-Score: 0.8304\n",
      "Class 11 -> Precision: 0.6790, Recall: 0.6812, F1-Score: 0.6801\n",
      "Class 12 -> Precision: 0.6668, Recall: 0.5308, F1-Score: 0.5911\n",
      "Class 13 -> Precision: 0.6440, Recall: 0.5984, F1-Score: 0.6204\n",
      "\n",
      "Macro-Average F1 Score: 0.6745\n"
     ]
    }
   ],
   "source": [
    "# unigram - with stem - without stop words\n",
    "# Question 4 :: uigram with stemming and removing stop words\n",
    "# 4. uigram - with stem - with stop words        -done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = 1)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                     Tokenized Description  Class Index\n",
      "0                             [ernie, cox]            3\n",
      "1                              [holosteum]           10\n",
      "2                   [pestarella, tyrrhena]            9\n",
      "3           [midsun, junior, high, school]            1\n",
      "4  [st, james', church, wrightington, bar]            6\n",
      "testingData.head()\n",
      "                Tokenized Description  Class Index\n",
      "0                     [lajos, drahos]            4\n",
      "1           [uss, huntsville, (1857)]            5\n",
      "2                            [scafco]            0\n",
      "3                 [mclean's, mansion]            6\n",
      "4  [avioane, craiova, iar-93, vultur]            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 41042\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 41042)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 65.35%\n",
      "\n",
      "Class 0 -> Precision: 0.2295, Recall: 0.8347, F1-Score: 0.3600\n",
      "Class 1 -> Precision: 0.8811, Recall: 0.9576, F1-Score: 0.9178\n",
      "Class 2 -> Precision: 0.6276, Recall: 0.4346, F1-Score: 0.5136\n",
      "Class 3 -> Precision: 0.6448, Recall: 0.5328, F1-Score: 0.5835\n",
      "Class 4 -> Precision: 0.6281, Recall: 0.6026, F1-Score: 0.6151\n",
      "Class 5 -> Precision: 0.9142, Recall: 0.8208, F1-Score: 0.8650\n",
      "Class 6 -> Precision: 0.8443, Recall: 0.8678, F1-Score: 0.8559\n",
      "Class 7 -> Precision: 0.9110, Recall: 0.7668, F1-Score: 0.8327\n",
      "Class 8 -> Precision: 0.9341, Recall: 0.4889, F1-Score: 0.6419\n",
      "Class 9 -> Precision: 0.9354, Recall: 0.3778, F1-Score: 0.5382\n",
      "Class 10 -> Precision: 0.9627, Recall: 0.5944, F1-Score: 0.7350\n",
      "Class 11 -> Precision: 0.7582, Recall: 0.7032, F1-Score: 0.7296\n",
      "Class 12 -> Precision: 0.7290, Recall: 0.5978, F1-Score: 0.6569\n",
      "Class 13 -> Precision: 0.7717, Recall: 0.5695, F1-Score: 0.6554\n",
      "\n",
      "Macro-Average F1 Score: 0.6786\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 66.46%\n",
      "\n",
      "Class 0 -> Precision: 0.7623, Recall: 0.5452, F1-Score: 0.6357\n",
      "Class 1 -> Precision: 0.8816, Recall: 0.9624, F1-Score: 0.9203\n",
      "Class 2 -> Precision: 0.4563, Recall: 0.3512, F1-Score: 0.3969\n",
      "Class 3 -> Precision: 0.5263, Recall: 0.4920, F1-Score: 0.5086\n",
      "Class 4 -> Precision: 0.5326, Recall: 0.5256, F1-Score: 0.5291\n",
      "Class 5 -> Precision: 0.8907, Recall: 0.8380, F1-Score: 0.8636\n",
      "Class 6 -> Precision: 0.8362, Recall: 0.8576, F1-Score: 0.8468\n",
      "Class 7 -> Precision: 0.8761, Recall: 0.7780, F1-Score: 0.8242\n",
      "Class 8 -> Precision: 0.9111, Recall: 0.5536, F1-Score: 0.6887\n",
      "Class 9 -> Precision: 0.3428, Recall: 0.9148, F1-Score: 0.4987\n",
      "Class 10 -> Precision: 0.9397, Recall: 0.7484, F1-Score: 0.8332\n",
      "Class 11 -> Precision: 0.6863, Recall: 0.6536, F1-Score: 0.6695\n",
      "Class 12 -> Precision: 0.6617, Recall: 0.5484, F1-Score: 0.5997\n",
      "Class 13 -> Precision: 0.6746, Recall: 0.5356, F1-Score: 0.5971\n",
      "\n",
      "Macro-Average F1 Score: 0.6723\n"
     ]
    }
   ],
   "source": [
    "# 4. unigram - without stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = 1)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0                                         [erni cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                             Tokenized Description  Class Index\n",
      "0                                     [lajo draho]            4\n",
      "1                  [uss huntsvil, huntsvil (1857)]            5\n",
      "2                                               []            0\n",
      "3                                [mclean' mansion]            6\n",
      "4  [avioan craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 51565\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 51565)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 25.23%\n",
      "\n",
      "Class 0 -> Precision: 0.0883, Recall: 0.9738, F1-Score: 0.1619\n",
      "Class 1 -> Precision: 0.9448, Recall: 0.7237, F1-Score: 0.8196\n",
      "Class 2 -> Precision: 0.7472, Recall: 0.0201, F1-Score: 0.0391\n",
      "Class 3 -> Precision: 0.9325, Recall: 0.0635, F1-Score: 0.1189\n",
      "Class 4 -> Precision: 0.8751, Recall: 0.0967, F1-Score: 0.1742\n",
      "Class 5 -> Precision: 0.9564, Recall: 0.1580, F1-Score: 0.2712\n",
      "Class 6 -> Precision: 0.9176, Recall: 0.3830, F1-Score: 0.5404\n",
      "Class 7 -> Precision: 0.9191, Recall: 0.1239, F1-Score: 0.2184\n",
      "Class 8 -> Precision: 0.9697, Recall: 0.1376, F1-Score: 0.2410\n",
      "Class 9 -> Precision: 0.9043, Recall: 0.0085, F1-Score: 0.0168\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.0139, F1-Score: 0.0274\n",
      "Class 11 -> Precision: 0.7495, Recall: 0.2702, F1-Score: 0.3972\n",
      "Class 12 -> Precision: 0.7754, Recall: 0.3072, F1-Score: 0.4401\n",
      "Class 13 -> Precision: 0.7012, Recall: 0.2527, F1-Score: 0.3715\n",
      "\n",
      "Macro-Average F1 Score: 0.2741\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 29.28%\n",
      "\n",
      "Class 0 -> Precision: 0.2182, Recall: 0.3588, F1-Score: 0.2714\n",
      "Class 1 -> Precision: 0.9279, Recall: 0.7572, F1-Score: 0.8339\n",
      "Class 2 -> Precision: 0.4348, Recall: 0.0240, F1-Score: 0.0455\n",
      "Class 3 -> Precision: 0.8266, Recall: 0.0896, F1-Score: 0.1617\n",
      "Class 4 -> Precision: 0.7161, Recall: 0.1372, F1-Score: 0.2303\n",
      "Class 5 -> Precision: 0.9174, Recall: 0.2400, F1-Score: 0.3805\n",
      "Class 6 -> Precision: 0.8647, Recall: 0.4600, F1-Score: 0.6005\n",
      "Class 7 -> Precision: 0.9080, Recall: 0.1896, F1-Score: 0.3137\n",
      "Class 8 -> Precision: 0.9759, Recall: 0.1940, F1-Score: 0.3237\n",
      "Class 9 -> Precision: 0.0858, Recall: 0.7300, F1-Score: 0.1536\n",
      "Class 10 -> Precision: 0.9636, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.6446, Recall: 0.3192, F1-Score: 0.4270\n",
      "Class 12 -> Precision: 0.6213, Recall: 0.3196, F1-Score: 0.4221\n",
      "Class 13 -> Precision: 0.5829, Recall: 0.2588, F1-Score: 0.3584\n",
      "\n",
      "Macro-Average F1 Score: 0.3260\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - with stem - without stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = False, with_stemming = True, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0                                        [ernie cox]            3\n",
      "1                                                 []           10\n",
      "2                              [pestarella tyrrhena]            9\n",
      "3          [midsun junior, junior high, high school]            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                              Tokenized Description  Class Index\n",
      "0                                    [lajos drahos]            4\n",
      "1               [uss huntsville, huntsville (1857)]            5\n",
      "2                                                []            0\n",
      "3                                [mclean's mansion]            6\n",
      "4  [avioane craiova, craiova iar-93, iar-93 vultur]            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 48122\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 48122)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 21.07%\n",
      "\n",
      "Class 0 -> Precision: 0.0828, Recall: 0.9859, F1-Score: 0.1528\n",
      "Class 1 -> Precision: 0.9580, Recall: 0.6594, F1-Score: 0.7811\n",
      "Class 2 -> Precision: 0.7459, Recall: 0.0182, F1-Score: 0.0355\n",
      "Class 3 -> Precision: 0.9310, Recall: 0.0634, F1-Score: 0.1187\n",
      "Class 4 -> Precision: 0.8735, Recall: 0.0918, F1-Score: 0.1661\n",
      "Class 5 -> Precision: 0.9577, Recall: 0.1562, F1-Score: 0.2686\n",
      "Class 6 -> Precision: 0.9239, Recall: 0.3496, F1-Score: 0.5073\n",
      "Class 7 -> Precision: 0.9151, Recall: 0.1197, F1-Score: 0.2117\n",
      "Class 8 -> Precision: 0.9592, Recall: 0.1365, F1-Score: 0.2390\n",
      "Class 9 -> Precision: 0.9438, Recall: 0.0084, F1-Score: 0.0167\n",
      "Class 10 -> Precision: 1.0000, Recall: 0.0138, F1-Score: 0.0272\n",
      "Class 11 -> Precision: 0.9151, Recall: 0.0948, F1-Score: 0.1718\n",
      "Class 12 -> Precision: 0.9664, Recall: 0.1841, F1-Score: 0.3093\n",
      "Class 13 -> Precision: 0.9026, Recall: 0.0686, F1-Score: 0.1275\n",
      "\n",
      "Macro-Average F1 Score: 0.2238\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 25.84%\n",
      "\n",
      "Class 0 -> Precision: 0.1987, Recall: 0.3448, F1-Score: 0.2521\n",
      "Class 1 -> Precision: 0.9252, Recall: 0.7272, F1-Score: 0.8143\n",
      "Class 2 -> Precision: 0.4173, Recall: 0.0212, F1-Score: 0.0404\n",
      "Class 3 -> Precision: 0.8296, Recall: 0.0896, F1-Score: 0.1617\n",
      "Class 4 -> Precision: 0.7199, Recall: 0.1316, F1-Score: 0.2225\n",
      "Class 5 -> Precision: 0.9171, Recall: 0.2388, F1-Score: 0.3789\n",
      "Class 6 -> Precision: 0.8688, Recall: 0.4344, F1-Score: 0.5792\n",
      "Class 7 -> Precision: 0.9051, Recall: 0.1832, F1-Score: 0.3047\n",
      "Class 8 -> Precision: 0.9857, Recall: 0.1928, F1-Score: 0.3225\n",
      "Class 9 -> Precision: 0.0786, Recall: 0.7324, F1-Score: 0.1420\n",
      "Class 10 -> Precision: 0.9636, Recall: 0.0212, F1-Score: 0.0415\n",
      "Class 11 -> Precision: 0.7944, Recall: 0.1700, F1-Score: 0.2801\n",
      "Class 12 -> Precision: 0.8114, Recall: 0.2220, F1-Score: 0.3486\n",
      "Class 13 -> Precision: 0.7068, Recall: 0.1080, F1-Score: 0.1874\n",
      "\n",
      "Macro-Average F1 Score: 0.2911\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - without stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"title\", target_col = \"Tokenized Title\", remove_stop_words = True, with_stemming = False, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Title\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
