{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from naive_bayes import NaiveBayes\n",
    "from utils import tokenizeAndGetTrainingAndTestingData, run_model, plot_wordclouds_per_class, removeStopWordsAndStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape : (140000, 3)\n",
      "   label                              title  \\\n",
      "0      3                          Ernie Cox   \n",
      "1     10                          Holosteum   \n",
      "2      9                Pestarella tyrrhena   \n",
      "3      1          MidSun Junior High School   \n",
      "4      6  St James' Church Wrightington Bar   \n",
      "\n",
      "                                             content  \n",
      "0   Ernest Ernie Cox (February 17 1894 – February...  \n",
      "1   Holosteum is a genus of plants in the Pink fa...  \n",
      "2   Pestarella tyrrhena (formerly Callianassa tyr...  \n",
      "3   MidSun Junior High School is a Canadian middl...  \n",
      "4   St James' Church Wrightington Bar is in Churc...  \n",
      "df_test.shape : (35000, 3)\n",
      "   label                          title  \\\n",
      "0      4                   Lajos Drahos   \n",
      "1      5          USS Huntsville (1857)   \n",
      "2      0                         SCAFCO   \n",
      "3      6               McLean's Mansion   \n",
      "4      5  Avioane Craiova IAR-93 Vultur   \n",
      "\n",
      "                                             content  \n",
      "0   Lajos Drahos (7 March 1895 - 2 June 1983) was...  \n",
      "1   USS Huntsville was a steamer acquired by the ...  \n",
      "2   Founded in 1954 by Ben G. Stone SCAFCO Corpor...  \n",
      "3   McLean's Mansion (originally Holly Lea) is a ...  \n",
      "4   The Avioane Craiova IAR-93 Vultur (Eagle) is ...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "print(f\"df_train.shape : {df_train.shape}\")\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(f\"df_test.shape : {df_test.shape}\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest, Ernie, Cox, (February, 17, 1894, –, F...            3\n",
      "1  [Holosteum, is, a, genus, of, plants, in, the,...           10\n",
      "2  [Pestarella, tyrrhena, (formerly, Callianassa,...            9\n",
      "3  [MidSun, Junior, High, School, is, a, Canadian...            1\n",
      "4  [St, James', Church, Wrightington, Bar, is, in...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos, Drahos, (7, March, 1895, -, 2, June, 1...            4\n",
      "1  [USS, Huntsville, was, a, steamer, acquired, b...            5\n",
      "2  [Founded, in, 1954, by, Ben, G., Stone, SCAFCO...            0\n",
      "3  [McLean's, Mansion, (originally, Holly, Lea), ...            6\n",
      "4  [The, Avioane, Craiova, IAR-93, Vultur, (Eagle...            5\n"
     ]
    }
   ],
   "source": [
    "# Question Part 1 :: Unigram without stemming and removing stop words\n",
    "# Train the model with context only corresponding to the labels\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 453340\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 453340)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.89%\n",
      "\n",
      "Class 0 -> Precision: 0.9712, Recall: 0.9282, F1-Score: 0.9492\n",
      "Class 1 -> Precision: 0.9732, Recall: 0.9896, F1-Score: 0.9814\n",
      "Class 2 -> Precision: 0.9807, Recall: 0.9566, F1-Score: 0.9685\n",
      "Class 3 -> Precision: 0.9849, Recall: 0.9938, F1-Score: 0.9893\n",
      "Class 4 -> Precision: 0.9801, Recall: 0.9884, F1-Score: 0.9842\n",
      "Class 5 -> Precision: 0.9823, Recall: 0.9941, F1-Score: 0.9882\n",
      "Class 6 -> Precision: 0.9591, Recall: 0.9747, F1-Score: 0.9668\n",
      "Class 7 -> Precision: 0.9761, Recall: 0.9918, F1-Score: 0.9839\n",
      "Class 8 -> Precision: 0.9995, Recall: 0.9587, F1-Score: 0.9787\n",
      "Class 9 -> Precision: 0.9989, Recall: 0.9731, F1-Score: 0.9858\n",
      "Class 10 -> Precision: 0.9875, Recall: 0.9962, F1-Score: 0.9918\n",
      "Class 11 -> Precision: 0.9670, Recall: 0.9962, F1-Score: 0.9814\n",
      "Class 12 -> Precision: 0.9822, Recall: 0.9864, F1-Score: 0.9843\n",
      "Class 13 -> Precision: 0.9633, Recall: 0.9763, F1-Score: 0.9698\n",
      "\n",
      "Macro-Average F1 Score: 0.9788\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.02%\n",
      "\n",
      "Class 0 -> Precision: 0.9418, Recall: 0.8804, F1-Score: 0.9101\n",
      "Class 1 -> Precision: 0.9577, Recall: 0.9792, F1-Score: 0.9684\n",
      "Class 2 -> Precision: 0.9561, Recall: 0.9052, F1-Score: 0.9299\n",
      "Class 3 -> Precision: 0.9817, Recall: 0.9884, F1-Score: 0.9851\n",
      "Class 4 -> Precision: 0.9613, Recall: 0.9824, F1-Score: 0.9717\n",
      "Class 5 -> Precision: 0.9660, Recall: 0.9880, F1-Score: 0.9769\n",
      "Class 6 -> Precision: 0.9410, Recall: 0.9572, F1-Score: 0.9490\n",
      "Class 7 -> Precision: 0.9640, Recall: 0.9844, F1-Score: 0.9741\n",
      "Class 8 -> Precision: 0.9992, Recall: 0.9436, F1-Score: 0.9706\n",
      "Class 9 -> Precision: 0.9949, Recall: 0.9388, F1-Score: 0.9660\n",
      "Class 10 -> Precision: 0.9589, Recall: 0.9880, F1-Score: 0.9732\n",
      "Class 11 -> Precision: 0.9502, Recall: 0.9932, F1-Score: 0.9712\n",
      "Class 12 -> Precision: 0.9611, Recall: 0.9684, F1-Score: 0.9647\n",
      "Class 13 -> Precision: 0.9134, Recall: 0.9452, F1-Score: 0.9290\n",
      "\n",
      "Macro-Average F1 Score: 0.9600\n"
     ]
    }
   ],
   "source": [
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayes() \n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_wordclouds_per_class(trainingData, maxWords = 200, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [lajo, draho, (7, march, 1895, -, 2, june, 198...\n",
      "1    [uss, huntsvil, steamer, acquir, union, navi, ...\n",
      "2    [found, 1954, ben, g., stone, scafco, corpor, ...\n",
      "3    [mclean', mansion, (origin, holli, lea), homes...\n",
      "4    [avioan, craiova, iar-93, vultur, (eagle), twi...\n",
      "Name: Tokenized Description, dtype: object\n",
      "0    [ernest, erni, cox, (februari, 17, 1894, –, fe...\n",
      "1    [holosteum, genu, plant, pink, famili, (caryop...\n",
      "2    [pestarella, tyrrhena, (formerli, callianassa,...\n",
      "3    [midsun, junior, high, school, canadian, middl...\n",
      "4    [st, james', church, wrightington, bar, church...\n",
      "Name: Tokenized Description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Question 1, part 2 :: Unigram with stemming and removing stop words\n",
    "\n",
    "# # (a) Perform stemming and remove the stop-words in the training as well as the validation data.\n",
    "# trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(removeStopWordsAndStem)\n",
    "testingData[\"Tokenized Description\"] = testingData[\"Tokenized Description\"].apply(removeStopWordsAndStem)\n",
    "print(testingData[\"Tokenized Description\"].head())\n",
    "\n",
    "trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(removeStopWordsAndStem)\n",
    "print(trainingData[\"Tokenized Description\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word cloud on training data\n",
      "Word cloud on testing data\n"
     ]
    }
   ],
   "source": [
    "# (b) Construct word clouds for both classes on the transformed data\n",
    "print(\"Word cloud on training data\")\n",
    "# plot_wordclouds_per_class(trainingData, data_type = \"Training Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "print(\"Word cloud on testing data\")\n",
    "# plot_wordclouds_per_class(testingData, data_type = \"Testing Set\", maxWords = 200, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 453340\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 453340)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 93.98%\n",
      "\n",
      "Class 0 -> Precision: 0.8319, Recall: 0.8306, F1-Score: 0.8313\n",
      "Class 1 -> Precision: 0.9392, Recall: 0.9379, F1-Score: 0.9386\n",
      "Class 2 -> Precision: 0.9278, Recall: 0.9016, F1-Score: 0.9145\n",
      "Class 3 -> Precision: 0.9746, Recall: 0.9894, F1-Score: 0.9819\n",
      "Class 4 -> Precision: 0.9560, Recall: 0.9695, F1-Score: 0.9627\n",
      "Class 5 -> Precision: 0.9456, Recall: 0.9780, F1-Score: 0.9615\n",
      "Class 6 -> Precision: 0.9114, Recall: 0.9211, F1-Score: 0.9162\n",
      "Class 7 -> Precision: 0.9395, Recall: 0.9880, F1-Score: 0.9632\n",
      "Class 8 -> Precision: 0.9952, Recall: 0.8791, F1-Score: 0.9336\n",
      "Class 9 -> Precision: 0.9914, Recall: 0.9155, F1-Score: 0.9520\n",
      "Class 10 -> Precision: 0.9678, Recall: 0.9717, F1-Score: 0.9698\n",
      "Class 11 -> Precision: 0.9246, Recall: 0.9887, F1-Score: 0.9556\n",
      "Class 12 -> Precision: 0.9548, Recall: 0.9723, F1-Score: 0.9635\n",
      "Class 13 -> Precision: 0.9074, Recall: 0.9138, F1-Score: 0.9106\n",
      "\n",
      "Macro-Average F1 Score: 0.9396\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 92.38%\n",
      "\n",
      "Class 0 -> Precision: 0.7939, Recall: 0.8136, F1-Score: 0.8036\n",
      "Class 1 -> Precision: 0.9301, Recall: 0.9204, F1-Score: 0.9252\n",
      "Class 2 -> Precision: 0.9023, Recall: 0.8604, F1-Score: 0.8808\n",
      "Class 3 -> Precision: 0.9739, Recall: 0.9840, F1-Score: 0.9789\n",
      "Class 4 -> Precision: 0.9386, Recall: 0.9596, F1-Score: 0.9490\n",
      "Class 5 -> Precision: 0.9333, Recall: 0.9732, F1-Score: 0.9528\n",
      "Class 6 -> Precision: 0.9094, Recall: 0.9156, F1-Score: 0.9125\n",
      "Class 7 -> Precision: 0.9231, Recall: 0.9848, F1-Score: 0.9530\n",
      "Class 8 -> Precision: 0.9955, Recall: 0.8792, F1-Score: 0.9337\n",
      "Class 9 -> Precision: 0.9841, Recall: 0.8664, F1-Score: 0.9215\n",
      "Class 10 -> Precision: 0.9480, Recall: 0.9472, F1-Score: 0.9476\n",
      "Class 11 -> Precision: 0.9173, Recall: 0.9856, F1-Score: 0.9503\n",
      "Class 12 -> Precision: 0.9316, Recall: 0.9636, F1-Score: 0.9473\n",
      "Class 13 -> Precision: 0.8695, Recall: 0.8792, F1-Score: 0.8743\n",
      "\n",
      "Macro-Average F1 Score: 0.9236\n"
     ]
    }
   ],
   "source": [
    "# (c) Learn a new model on the transformed data. Report the validation set accuracy\n",
    "# 2. unigram - with stem - with stop words       - done\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d) How does your accuracy change over the validation set? Comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest Ernie, Ernie Cox, Cox (February, (Febr...            3\n",
      "1  [Holosteum is, is a, a genus, genus of, of pla...           10\n",
      "2  [Pestarella tyrrhena, tyrrhena (formerly, (for...            9\n",
      "3  [MidSun Junior, Junior High, High School, Scho...            1\n",
      "4  [St James', James' Church, Church Wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos Drahos, Drahos (7, (7 March, March 1895...            4\n",
      "1  [USS Huntsville, Huntsville was, was a, a stea...            5\n",
      "2  [Founded in, in 1954, 1954 by, by Ben, Ben G.,...            0\n",
      "3  [McLean's Mansion, Mansion (originally, (origi...            6\n",
      "4  [The Avioane, Avioane Craiova, Craiova IAR-93,...            5\n",
      "0    [lajos draho, drahos (7, (7 march, march 1895,...\n",
      "1    [uss huntsvil, huntsville wa, was a, a steam, ...\n",
      "2    [founded in, in 1954, 1954 bi, by ben, ben g.,...\n",
      "3    [mclean's mans, mansion (origin, (originally h...\n",
      "4    [the avioan, avioane craiova, craiova iar-93, ...\n",
      "Name: Tokenized Description, dtype: object\n",
      "0    [ernest erni, ernie cox, cox (februari, (febru...\n",
      "1    [holosteum i, is a, a genu, genus of, of plant...\n",
      "2    [pestarella tyrrhena, tyrrhena (formerli, (for...\n",
      "3    [midsun junior, junior high, high school, scho...\n",
      "4    [st james', james' church, church wrightington...\n",
      "Name: Tokenized Description, dtype: object\n",
      "--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 2064355\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 2064355)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.31%\n",
      "\n",
      "Class 0 -> Precision: 0.9626, Recall: 0.9472, F1-Score: 0.9548\n",
      "Class 1 -> Precision: 0.9569, Recall: 0.9864, F1-Score: 0.9714\n",
      "Class 2 -> Precision: 0.9620, Recall: 0.9444, F1-Score: 0.9531\n",
      "Class 3 -> Precision: 0.9803, Recall: 0.9854, F1-Score: 0.9828\n",
      "Class 4 -> Precision: 0.9664, Recall: 0.9827, F1-Score: 0.9745\n",
      "Class 5 -> Precision: 0.9818, Recall: 0.9803, F1-Score: 0.9810\n",
      "Class 6 -> Precision: 0.9575, Recall: 0.9673, F1-Score: 0.9624\n",
      "Class 7 -> Precision: 0.9712, Recall: 0.9893, F1-Score: 0.9802\n",
      "Class 8 -> Precision: 0.9961, Recall: 0.9730, F1-Score: 0.9844\n",
      "Class 9 -> Precision: 0.9967, Recall: 0.9557, F1-Score: 0.9758\n",
      "Class 10 -> Precision: 0.9626, Recall: 0.9908, F1-Score: 0.9765\n",
      "Class 11 -> Precision: 0.9792, Recall: 0.9859, F1-Score: 0.9826\n",
      "Class 12 -> Precision: 0.9792, Recall: 0.9818, F1-Score: 0.9805\n",
      "Class 13 -> Precision: 0.9731, Recall: 0.9532, F1-Score: 0.9630\n",
      "\n",
      "Macro-Average F1 Score: 0.9731\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 93.89%\n",
      "\n",
      "Class 0 -> Precision: 0.9050, Recall: 0.8764, F1-Score: 0.8905\n",
      "Class 1 -> Precision: 0.9185, Recall: 0.9692, F1-Score: 0.9432\n",
      "Class 2 -> Precision: 0.9236, Recall: 0.8752, F1-Score: 0.8987\n",
      "Class 3 -> Precision: 0.9712, Recall: 0.9728, F1-Score: 0.9720\n",
      "Class 4 -> Precision: 0.9375, Recall: 0.9724, F1-Score: 0.9546\n",
      "Class 5 -> Precision: 0.9503, Recall: 0.9556, F1-Score: 0.9529\n",
      "Class 6 -> Precision: 0.8966, Recall: 0.9292, F1-Score: 0.9126\n",
      "Class 7 -> Precision: 0.9466, Recall: 0.9716, F1-Score: 0.9589\n",
      "Class 8 -> Precision: 0.9942, Recall: 0.9528, F1-Score: 0.9730\n",
      "Class 9 -> Precision: 0.9871, Recall: 0.8880, F1-Score: 0.9349\n",
      "Class 10 -> Precision: 0.9126, Recall: 0.9732, F1-Score: 0.9419\n",
      "Class 11 -> Precision: 0.9540, Recall: 0.9780, F1-Score: 0.9658\n",
      "Class 12 -> Precision: 0.9523, Recall: 0.9500, F1-Score: 0.9511\n",
      "Class 13 -> Precision: 0.9027, Recall: 0.8796, F1-Score: 0.8910\n",
      "\n",
      "Macro-Average F1 Score: 0.9387\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram with stemming and removing stop words\n",
    "# 3. bigram - with stem - with stop words        -done\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 2)\n",
    "\n",
    "testingData[\"Tokenized Description\"] = testingData[\"Tokenized Description\"].apply(removeStopWordsAndStem)\n",
    "print(testingData[\"Tokenized Description\"].head())\n",
    "\n",
    "trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(removeStopWordsAndStem)\n",
    "print(trainingData[\"Tokenized Description\"].head())\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest Ernie, Ernie Cox, Cox (February, (Febr...            3\n",
      "1  [Holosteum is, is a, a genus, genus of, of pla...           10\n",
      "2  [Pestarella tyrrhena, tyrrhena (formerly, (for...            9\n",
      "3  [MidSun Junior, Junior High, High School, Scho...            1\n",
      "4  [St James', James' Church, Church Wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos Drahos, Drahos (7, (7 March, March 1895...            4\n",
      "1  [USS Huntsville, Huntsville was, was a, a stea...            5\n",
      "2  [Founded in, in 1954, 1954 by, by Ben, Ben G.,...            0\n",
      "3  [McLean's Mansion, Mansion (originally, (origi...            6\n",
      "4  [The Avioane, Avioane Craiova, Craiova IAR-93,...            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 2064355\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 2064355)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 99.62%\n",
      "\n",
      "Class 0 -> Precision: 0.9953, Recall: 0.9913, F1-Score: 0.9933\n",
      "Class 1 -> Precision: 0.9941, Recall: 0.9978, F1-Score: 0.9960\n",
      "Class 2 -> Precision: 0.9963, Recall: 0.9927, F1-Score: 0.9945\n",
      "Class 3 -> Precision: 0.9961, Recall: 0.9983, F1-Score: 0.9972\n",
      "Class 4 -> Precision: 0.9948, Recall: 0.9971, F1-Score: 0.9960\n",
      "Class 5 -> Precision: 0.9976, Recall: 0.9973, F1-Score: 0.9974\n",
      "Class 6 -> Precision: 0.9914, Recall: 0.9948, F1-Score: 0.9931\n",
      "Class 7 -> Precision: 0.9938, Recall: 0.9991, F1-Score: 0.9965\n",
      "Class 8 -> Precision: 0.9996, Recall: 0.9936, F1-Score: 0.9966\n",
      "Class 9 -> Precision: 0.9996, Recall: 0.9940, F1-Score: 0.9968\n",
      "Class 10 -> Precision: 0.9964, Recall: 0.9980, F1-Score: 0.9972\n",
      "Class 11 -> Precision: 0.9970, Recall: 0.9990, F1-Score: 0.9980\n",
      "Class 12 -> Precision: 0.9984, Recall: 0.9972, F1-Score: 0.9978\n",
      "Class 13 -> Precision: 0.9970, Recall: 0.9972, F1-Score: 0.9971\n",
      "\n",
      "Macro-Average F1 Score: 0.9962\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.66%\n",
      "\n",
      "Class 0 -> Precision: 0.9618, Recall: 0.9160, F1-Score: 0.9383\n",
      "Class 1 -> Precision: 0.9547, Recall: 0.9868, F1-Score: 0.9705\n",
      "Class 2 -> Precision: 0.9679, Recall: 0.9176, F1-Score: 0.9421\n",
      "Class 3 -> Precision: 0.9734, Recall: 0.9824, F1-Score: 0.9779\n",
      "Class 4 -> Precision: 0.9476, Recall: 0.9836, F1-Score: 0.9653\n",
      "Class 5 -> Precision: 0.9751, Recall: 0.9856, F1-Score: 0.9803\n",
      "Class 6 -> Precision: 0.9488, Recall: 0.9636, F1-Score: 0.9561\n",
      "Class 7 -> Precision: 0.9717, Recall: 0.9876, F1-Score: 0.9796\n",
      "Class 8 -> Precision: 0.9988, Recall: 0.9676, F1-Score: 0.9829\n",
      "Class 9 -> Precision: 0.9918, Recall: 0.9220, F1-Score: 0.9556\n",
      "Class 10 -> Precision: 0.9378, Recall: 0.9824, F1-Score: 0.9596\n",
      "Class 11 -> Precision: 0.9726, Recall: 0.9940, F1-Score: 0.9832\n",
      "Class 12 -> Precision: 0.9839, Recall: 0.9792, F1-Score: 0.9816\n",
      "Class 13 -> Precision: 0.9522, Recall: 0.9644, F1-Score: 0.9583\n",
      "\n",
      "Macro-Average F1 Score: 0.9665\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram without stemming or removing stop words\n",
    "# bigram - without stem - without stop words  - done\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 2)\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest, Ernie, Cox, (February, 17, 1894, –, F...            3\n",
      "1  [Holosteum, is, a, genus, of, plants, in, the,...           10\n",
      "2  [Pestarella, tyrrhena, (formerly, Callianassa,...            9\n",
      "3  [MidSun, Junior, High, School, is, a, Canadian...            1\n",
      "4  [St, James', Church, Wrightington, Bar, is, in...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos, Drahos, (7, March, 1895, -, 2, June, 1...            4\n",
      "1  [USS, Huntsville, was, a, steamer, acquired, b...            5\n",
      "2  [Founded, in, 1954, by, Ben, G., Stone, SCAFCO...            0\n",
      "3  [McLean's, Mansion, (originally, Holly, Lea), ...            6\n",
      "4  [The, Avioane, Craiova, IAR-93, Vultur, (Eagle...            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 453340\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 453340)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 94.01%\n",
      "\n",
      "Class 0 -> Precision: 0.9081, Recall: 0.8281, F1-Score: 0.8663\n",
      "Class 1 -> Precision: 0.9461, Recall: 0.9357, F1-Score: 0.9409\n",
      "Class 2 -> Precision: 0.9561, Recall: 0.9193, F1-Score: 0.9373\n",
      "Class 3 -> Precision: 0.9818, Recall: 0.9859, F1-Score: 0.9838\n",
      "Class 4 -> Precision: 0.9603, Recall: 0.9767, F1-Score: 0.9684\n",
      "Class 5 -> Precision: 0.9443, Recall: 0.9807, F1-Score: 0.9621\n",
      "Class 6 -> Precision: 0.8296, Recall: 0.9432, F1-Score: 0.8828\n",
      "Class 7 -> Precision: 0.9287, Recall: 0.9836, F1-Score: 0.9554\n",
      "Class 8 -> Precision: 0.9965, Recall: 0.8585, F1-Score: 0.9224\n",
      "Class 9 -> Precision: 0.9942, Recall: 0.8846, F1-Score: 0.9362\n",
      "Class 10 -> Precision: 0.9531, Recall: 0.9763, F1-Score: 0.9646\n",
      "Class 11 -> Precision: 0.9359, Recall: 0.9903, F1-Score: 0.9623\n",
      "Class 12 -> Precision: 0.9668, Recall: 0.9621, F1-Score: 0.9645\n",
      "Class 13 -> Precision: 0.8896, Recall: 0.9367, F1-Score: 0.9126\n",
      "\n",
      "Macro-Average F1 Score: 0.9400\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 92.68%\n",
      "\n",
      "Class 0 -> Precision: 0.8897, Recall: 0.8128, F1-Score: 0.8495\n",
      "Class 1 -> Precision: 0.9404, Recall: 0.9208, F1-Score: 0.9305\n",
      "Class 2 -> Precision: 0.9427, Recall: 0.8880, F1-Score: 0.9145\n",
      "Class 3 -> Precision: 0.9804, Recall: 0.9824, F1-Score: 0.9814\n",
      "Class 4 -> Precision: 0.9471, Recall: 0.9664, F1-Score: 0.9566\n",
      "Class 5 -> Precision: 0.9360, Recall: 0.9716, F1-Score: 0.9535\n",
      "Class 6 -> Precision: 0.8135, Recall: 0.9352, F1-Score: 0.8701\n",
      "Class 7 -> Precision: 0.9162, Recall: 0.9792, F1-Score: 0.9466\n",
      "Class 8 -> Precision: 0.9958, Recall: 0.8596, F1-Score: 0.9227\n",
      "Class 9 -> Precision: 0.9890, Recall: 0.8304, F1-Score: 0.9028\n",
      "Class 10 -> Precision: 0.9317, Recall: 0.9708, F1-Score: 0.9508\n",
      "Class 11 -> Precision: 0.9334, Recall: 0.9924, F1-Score: 0.9620\n",
      "Class 12 -> Precision: 0.9522, Recall: 0.9492, F1-Score: 0.9507\n",
      "Class 13 -> Precision: 0.8476, Recall: 0.9168, F1-Score: 0.8809\n",
      "\n",
      "Macro-Average F1 Score: 0.9266\n"
     ]
    }
   ],
   "source": [
    "# unigram - with stem - without stop words\n",
    "# Question 4 :: uigram with stemming and removing stop words\n",
    "# 4. uigram - with stem - with stop words        -done\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 1)\n",
    "\n",
    "# Apply preprocessing with stopword removal enabled\n",
    "testingData[\"Tokenized Description\"] = testingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=False, with_stemming = True)\n",
    ")\n",
    "trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=False, with_stemming = True)\n",
    ")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest, Ernie, Cox, (February, 17, 1894, –, F...            3\n",
      "1  [Holosteum, is, a, genus, of, plants, in, the,...           10\n",
      "2  [Pestarella, tyrrhena, (formerly, Callianassa,...            9\n",
      "3  [MidSun, Junior, High, School, is, a, Canadian...            1\n",
      "4  [St, James', Church, Wrightington, Bar, is, in...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos, Drahos, (7, March, 1895, -, 2, June, 1...            4\n",
      "1  [USS, Huntsville, was, a, steamer, acquired, b...            5\n",
      "2  [Founded, in, 1954, by, Ben, G., Stone, SCAFCO...            0\n",
      "3  [McLean's, Mansion, (originally, Holly, Lea), ...            6\n",
      "4  [The, Avioane, Craiova, IAR-93, Vultur, (Eagle...            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 453340\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 453340)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.08%\n",
      "\n",
      "Class 0 -> Precision: 0.9509, Recall: 0.9104, F1-Score: 0.9302\n",
      "Class 1 -> Precision: 0.9652, Recall: 0.9849, F1-Score: 0.9750\n",
      "Class 2 -> Precision: 0.9642, Recall: 0.9131, F1-Score: 0.9380\n",
      "Class 3 -> Precision: 0.9842, Recall: 0.9938, F1-Score: 0.9890\n",
      "Class 4 -> Precision: 0.9750, Recall: 0.9785, F1-Score: 0.9767\n",
      "Class 5 -> Precision: 0.9772, Recall: 0.9888, F1-Score: 0.9830\n",
      "Class 6 -> Precision: 0.9581, Recall: 0.9618, F1-Score: 0.9599\n",
      "Class 7 -> Precision: 0.9758, Recall: 0.9912, F1-Score: 0.9834\n",
      "Class 8 -> Precision: 0.9990, Recall: 0.9632, F1-Score: 0.9808\n",
      "Class 9 -> Precision: 0.9977, Recall: 0.9729, F1-Score: 0.9852\n",
      "Class 10 -> Precision: 0.9849, Recall: 0.9942, F1-Score: 0.9895\n",
      "Class 11 -> Precision: 0.9492, Recall: 0.9939, F1-Score: 0.9710\n",
      "Class 12 -> Precision: 0.9689, Recall: 0.9854, F1-Score: 0.9771\n",
      "Class 13 -> Precision: 0.9426, Recall: 0.9590, F1-Score: 0.9507\n",
      "\n",
      "Macro-Average F1 Score: 0.9707\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.91%\n",
      "\n",
      "Class 0 -> Precision: 0.9364, Recall: 0.8892, F1-Score: 0.9122\n",
      "Class 1 -> Precision: 0.9582, Recall: 0.9820, F1-Score: 0.9700\n",
      "Class 2 -> Precision: 0.9470, Recall: 0.8716, F1-Score: 0.9077\n",
      "Class 3 -> Precision: 0.9817, Recall: 0.9880, F1-Score: 0.9848\n",
      "Class 4 -> Precision: 0.9663, Recall: 0.9752, F1-Score: 0.9707\n",
      "Class 5 -> Precision: 0.9681, Recall: 0.9832, F1-Score: 0.9756\n",
      "Class 6 -> Precision: 0.9537, Recall: 0.9564, F1-Score: 0.9551\n",
      "Class 7 -> Precision: 0.9664, Recall: 0.9880, F1-Score: 0.9771\n",
      "Class 8 -> Precision: 0.9996, Recall: 0.9556, F1-Score: 0.9771\n",
      "Class 9 -> Precision: 0.9925, Recall: 0.9480, F1-Score: 0.9697\n",
      "Class 10 -> Precision: 0.9674, Recall: 0.9848, F1-Score: 0.9760\n",
      "Class 11 -> Precision: 0.9338, Recall: 0.9936, F1-Score: 0.9628\n",
      "Class 12 -> Precision: 0.9527, Recall: 0.9740, F1-Score: 0.9632\n",
      "Class 13 -> Precision: 0.9071, Recall: 0.9372, F1-Score: 0.9219\n",
      "\n",
      "Macro-Average F1 Score: 0.9589\n"
     ]
    }
   ],
   "source": [
    "# 4. unigram - without stem - with stop words\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 1)\n",
    "\n",
    "# Apply preprocessing with stopword removal enabled\n",
    "testingData[\"Tokenized Description\"] = testingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=True, with_stemming = False)\n",
    ")\n",
    "trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=True, with_stemming = False)\n",
    ")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest Ernie, Ernie Cox, Cox (February, (Febr...            3\n",
      "1  [Holosteum is, is a, a genus, genus of, of pla...           10\n",
      "2  [Pestarella tyrrhena, tyrrhena (formerly, (for...            9\n",
      "3  [MidSun Junior, Junior High, High School, Scho...            1\n",
      "4  [St James', James' Church, Church Wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos Drahos, Drahos (7, (7 March, March 1895...            4\n",
      "1  [USS Huntsville, Huntsville was, was a, a stea...            5\n",
      "2  [Founded in, in 1954, 1954 by, by Ben, Ben G.,...            0\n",
      "3  [McLean's Mansion, Mansion (originally, (origi...            6\n",
      "4  [The Avioane, Avioane Craiova, Craiova IAR-93,...            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 2064355\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 2064355)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.31%\n",
      "\n",
      "Class 0 -> Precision: 0.9626, Recall: 0.9472, F1-Score: 0.9548\n",
      "Class 1 -> Precision: 0.9569, Recall: 0.9864, F1-Score: 0.9714\n",
      "Class 2 -> Precision: 0.9620, Recall: 0.9444, F1-Score: 0.9531\n",
      "Class 3 -> Precision: 0.9803, Recall: 0.9854, F1-Score: 0.9828\n",
      "Class 4 -> Precision: 0.9664, Recall: 0.9827, F1-Score: 0.9745\n",
      "Class 5 -> Precision: 0.9818, Recall: 0.9803, F1-Score: 0.9810\n",
      "Class 6 -> Precision: 0.9575, Recall: 0.9673, F1-Score: 0.9624\n",
      "Class 7 -> Precision: 0.9712, Recall: 0.9893, F1-Score: 0.9802\n",
      "Class 8 -> Precision: 0.9961, Recall: 0.9730, F1-Score: 0.9844\n",
      "Class 9 -> Precision: 0.9967, Recall: 0.9557, F1-Score: 0.9758\n",
      "Class 10 -> Precision: 0.9626, Recall: 0.9908, F1-Score: 0.9765\n",
      "Class 11 -> Precision: 0.9792, Recall: 0.9859, F1-Score: 0.9826\n",
      "Class 12 -> Precision: 0.9792, Recall: 0.9818, F1-Score: 0.9805\n",
      "Class 13 -> Precision: 0.9731, Recall: 0.9532, F1-Score: 0.9630\n",
      "\n",
      "Macro-Average F1 Score: 0.9731\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 93.89%\n",
      "\n",
      "Class 0 -> Precision: 0.9050, Recall: 0.8764, F1-Score: 0.8905\n",
      "Class 1 -> Precision: 0.9185, Recall: 0.9692, F1-Score: 0.9432\n",
      "Class 2 -> Precision: 0.9236, Recall: 0.8752, F1-Score: 0.8987\n",
      "Class 3 -> Precision: 0.9712, Recall: 0.9728, F1-Score: 0.9720\n",
      "Class 4 -> Precision: 0.9375, Recall: 0.9724, F1-Score: 0.9546\n",
      "Class 5 -> Precision: 0.9503, Recall: 0.9556, F1-Score: 0.9529\n",
      "Class 6 -> Precision: 0.8966, Recall: 0.9292, F1-Score: 0.9126\n",
      "Class 7 -> Precision: 0.9466, Recall: 0.9716, F1-Score: 0.9589\n",
      "Class 8 -> Precision: 0.9942, Recall: 0.9528, F1-Score: 0.9730\n",
      "Class 9 -> Precision: 0.9871, Recall: 0.8880, F1-Score: 0.9349\n",
      "Class 10 -> Precision: 0.9126, Recall: 0.9732, F1-Score: 0.9419\n",
      "Class 11 -> Precision: 0.9540, Recall: 0.9780, F1-Score: 0.9658\n",
      "Class 12 -> Precision: 0.9523, Recall: 0.9500, F1-Score: 0.9511\n",
      "Class 13 -> Precision: 0.9027, Recall: 0.8796, F1-Score: 0.8910\n",
      "\n",
      "Macro-Average F1 Score: 0.9387\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - with stem - without stop words\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 2)\n",
    "\n",
    "# Apply preprocessing with stopword removal enabled\n",
    "testingData[\"Tokenized Description\"] = testingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=False, with_stemming = True)\n",
    ")\n",
    "trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=False, with_stemming = True)\n",
    ")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Ernest Ernie, Ernie Cox, Cox (February, (Febr...            3\n",
      "1  [Holosteum is, is a, a genus, genus of, of pla...           10\n",
      "2  [Pestarella tyrrhena, tyrrhena (formerly, (for...            9\n",
      "3  [MidSun Junior, Junior High, High School, Scho...            1\n",
      "4  [St James', James' Church, Church Wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [Lajos Drahos, Drahos (7, (7 March, March 1895...            4\n",
      "1  [USS Huntsville, Huntsville was, was a, a stea...            5\n",
      "2  [Founded in, in 1954, 1954 by, by Ben, Ben G.,...            0\n",
      "3  [McLean's Mansion, Mansion (originally, (origi...            6\n",
      "4  [The Avioane, Avioane Craiova, Craiova IAR-93,...            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 2064355\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 2064355)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 98.61%\n",
      "\n",
      "Class 0 -> Precision: 0.9851, Recall: 0.9725, F1-Score: 0.9788\n",
      "Class 1 -> Precision: 0.9838, Recall: 0.9946, F1-Score: 0.9892\n",
      "Class 2 -> Precision: 0.9815, Recall: 0.9539, F1-Score: 0.9675\n",
      "Class 3 -> Precision: 0.9833, Recall: 0.9906, F1-Score: 0.9869\n",
      "Class 4 -> Precision: 0.9738, Recall: 0.9880, F1-Score: 0.9808\n",
      "Class 5 -> Precision: 0.9911, Recall: 0.9931, F1-Score: 0.9921\n",
      "Class 6 -> Precision: 0.9776, Recall: 0.9857, F1-Score: 0.9816\n",
      "Class 7 -> Precision: 0.9825, Recall: 0.9952, F1-Score: 0.9888\n",
      "Class 8 -> Precision: 0.9985, Recall: 0.9807, F1-Score: 0.9895\n",
      "Class 9 -> Precision: 0.9984, Recall: 0.9784, F1-Score: 0.9883\n",
      "Class 10 -> Precision: 0.9863, Recall: 0.9939, F1-Score: 0.9901\n",
      "Class 11 -> Precision: 0.9866, Recall: 0.9963, F1-Score: 0.9914\n",
      "Class 12 -> Precision: 0.9925, Recall: 0.9931, F1-Score: 0.9928\n",
      "Class 13 -> Precision: 0.9853, Recall: 0.9896, F1-Score: 0.9874\n",
      "\n",
      "Macro-Average F1 Score: 0.9861\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.24%\n",
      "\n",
      "Class 0 -> Precision: 0.9604, Recall: 0.9208, F1-Score: 0.9402\n",
      "Class 1 -> Precision: 0.9573, Recall: 0.9868, F1-Score: 0.9718\n",
      "Class 2 -> Precision: 0.9529, Recall: 0.8984, F1-Score: 0.9249\n",
      "Class 3 -> Precision: 0.9745, Recall: 0.9780, F1-Score: 0.9762\n",
      "Class 4 -> Precision: 0.9444, Recall: 0.9776, F1-Score: 0.9607\n",
      "Class 5 -> Precision: 0.9722, Recall: 0.9804, F1-Score: 0.9763\n",
      "Class 6 -> Precision: 0.9412, Recall: 0.9600, F1-Score: 0.9505\n",
      "Class 7 -> Precision: 0.9651, Recall: 0.9856, F1-Score: 0.9753\n",
      "Class 8 -> Precision: 0.9983, Recall: 0.9652, F1-Score: 0.9815\n",
      "Class 9 -> Precision: 0.9904, Recall: 0.9112, F1-Score: 0.9492\n",
      "Class 10 -> Precision: 0.9318, Recall: 0.9788, F1-Score: 0.9547\n",
      "Class 11 -> Precision: 0.9718, Recall: 0.9916, F1-Score: 0.9816\n",
      "Class 12 -> Precision: 0.9795, Recall: 0.9768, F1-Score: 0.9782\n",
      "Class 13 -> Precision: 0.9391, Recall: 0.9620, F1-Score: 0.9504\n",
      "\n",
      "Macro-Average F1 Score: 0.9622\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - without stem - with stop words\n",
    "vocabulary, trainingData, testingData = tokenizeAndGetTrainingAndTestingData(df_train, df_test, window = 2)\n",
    "\n",
    "# Apply preprocessing with stopword removal enabled\n",
    "testingData[\"Tokenized Description\"] = testingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=True, with_stemming = False)\n",
    ")\n",
    "trainingData[\"Tokenized Description\"] = trainingData[\"Tokenized Description\"].apply(\n",
    "    lambda tokens: removeStopWordsAndStem(tokens, remove_stop_words=True, with_stemming = False)\n",
    ")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
