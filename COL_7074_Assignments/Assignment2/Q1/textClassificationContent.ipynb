{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from naive_bayes import NaiveBayes\n",
    "from utils import tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary, getTrainingAndTestingData, run_model, plot_wordclouds_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape : (140000, 3)\n",
      "   label                              title  \\\n",
      "0      3                          Ernie Cox   \n",
      "1     10                          Holosteum   \n",
      "2      9                Pestarella tyrrhena   \n",
      "3      1          MidSun Junior High School   \n",
      "4      6  St James' Church Wrightington Bar   \n",
      "\n",
      "                                             content  \n",
      "0   Ernest Ernie Cox (February 17 1894 – February...  \n",
      "1   Holosteum is a genus of plants in the Pink fa...  \n",
      "2   Pestarella tyrrhena (formerly Callianassa tyr...  \n",
      "3   MidSun Junior High School is a Canadian middl...  \n",
      "4   St James' Church Wrightington Bar is in Churc...  \n",
      "df_test.shape : (35000, 3)\n",
      "   label                          title  \\\n",
      "0      4                   Lajos Drahos   \n",
      "1      5          USS Huntsville (1857)   \n",
      "2      0                         SCAFCO   \n",
      "3      6               McLean's Mansion   \n",
      "4      5  Avioane Craiova IAR-93 Vultur   \n",
      "\n",
      "                                             content  \n",
      "0   Lajos Drahos (7 March 1895 - 2 June 1983) was...  \n",
      "1   USS Huntsville was a steamer acquired by the ...  \n",
      "2   Founded in 1954 by Ben G. Stone SCAFCO Corpor...  \n",
      "3   McLean's Mansion (originally Holly Lea) is a ...  \n",
      "4   The Avioane Craiova IAR-93 Vultur (Eagle) is ...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "print(f\"df_train.shape : {df_train.shape}\")\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(f\"df_test.shape : {df_test.shape}\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest, ernie, cox, (february, 17, 1894, –, f...            3\n",
      "1  [holosteum, is, a, genus, of, plants, in, the,...           10\n",
      "2  [pestarella, tyrrhena, (formerly, callianassa,...            9\n",
      "3  [midsun, junior, high, school, is, a, canadian...            1\n",
      "4  [st, james', church, wrightington, bar, is, in...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajos, drahos, (7, march, 1895, -, 2, june, 1...            4\n",
      "1  [uss, huntsville, was, a, steamer, acquired, b...            5\n",
      "2  [founded, in, 1954, by, ben, g., stone, scafco...            0\n",
      "3  [mclean's, mansion, (originally, holly, lea), ...            6\n",
      "4  [the, avioane, craiova, iar-93, vultur, (eagle...            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 158757\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 158757)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.06%\n",
      "\n",
      "Class 0 -> Precision: 0.9524, Recall: 0.9086, F1-Score: 0.9300\n",
      "Class 1 -> Precision: 0.9652, Recall: 0.9843, F1-Score: 0.9747\n",
      "Class 2 -> Precision: 0.9686, Recall: 0.9379, F1-Score: 0.9530\n",
      "Class 3 -> Precision: 0.9820, Recall: 0.9924, F1-Score: 0.9872\n",
      "Class 4 -> Precision: 0.9738, Recall: 0.9822, F1-Score: 0.9780\n",
      "Class 5 -> Precision: 0.9786, Recall: 0.9895, F1-Score: 0.9840\n",
      "Class 6 -> Precision: 0.9622, Recall: 0.9580, F1-Score: 0.9601\n",
      "Class 7 -> Precision: 0.9632, Recall: 0.9920, F1-Score: 0.9774\n",
      "Class 8 -> Precision: 0.9987, Recall: 0.9524, F1-Score: 0.9750\n",
      "Class 9 -> Precision: 0.9948, Recall: 0.9626, F1-Score: 0.9785\n",
      "Class 10 -> Precision: 0.9768, Recall: 0.9911, F1-Score: 0.9839\n",
      "Class 11 -> Precision: 0.9590, Recall: 0.9941, F1-Score: 0.9762\n",
      "Class 12 -> Precision: 0.9670, Recall: 0.9840, F1-Score: 0.9754\n",
      "Class 13 -> Precision: 0.9478, Recall: 0.9593, F1-Score: 0.9535\n",
      "\n",
      "Macro-Average F1 Score: 0.9705\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.13%\n",
      "\n",
      "Class 0 -> Precision: 0.9361, Recall: 0.8848, F1-Score: 0.9097\n",
      "Class 1 -> Precision: 0.9578, Recall: 0.9804, F1-Score: 0.9690\n",
      "Class 2 -> Precision: 0.9527, Recall: 0.9020, F1-Score: 0.9266\n",
      "Class 3 -> Precision: 0.9802, Recall: 0.9888, F1-Score: 0.9845\n",
      "Class 4 -> Precision: 0.9631, Recall: 0.9804, F1-Score: 0.9717\n",
      "Class 5 -> Precision: 0.9689, Recall: 0.9848, F1-Score: 0.9768\n",
      "Class 6 -> Precision: 0.9611, Recall: 0.9476, F1-Score: 0.9543\n",
      "Class 7 -> Precision: 0.9582, Recall: 0.9896, F1-Score: 0.9736\n",
      "Class 8 -> Precision: 0.9983, Recall: 0.9516, F1-Score: 0.9744\n",
      "Class 9 -> Precision: 0.9897, Recall: 0.9656, F1-Score: 0.9775\n",
      "Class 10 -> Precision: 0.9808, Recall: 0.9828, F1-Score: 0.9818\n",
      "Class 11 -> Precision: 0.9491, Recall: 0.9920, F1-Score: 0.9701\n",
      "Class 12 -> Precision: 0.9471, Recall: 0.9748, F1-Score: 0.9608\n",
      "Class 13 -> Precision: 0.9170, Recall: 0.9328, F1-Score: 0.9248\n",
      "\n",
      "Macro-Average F1 Score: 0.9611\n"
     ]
    }
   ],
   "source": [
    "# Question Part 1 :: Unigram without stemming and removing stop words\n",
    "# Train the model with context only corresponding to the labels\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = 1)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "# 1. unigram - without stem - without stop words - done\n",
    "model = NaiveBayes() \n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_wordclouds_per_class(trainingData, maxWords = 200, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word cloud on training data\n",
      "Word cloud on testing data\n",
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest, erni, cox, (februari, 17, 1894, –, fe...            3\n",
      "1  [holosteum, genu, plant, pink, famili, (caryop...           10\n",
      "2  [pestarella, tyrrhena, (formerli, callianassa,...            9\n",
      "3  [midsun, junior, high, school, canadian, middl...            1\n",
      "4  [st, james', church, wrightington, bar, church...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajo, draho, (7, march, 1895, -, 2, june, 198...            4\n",
      "1  [uss, huntsvil, steamer, acquir, union, navi, ...            5\n",
      "2  [found, 1954, ben, g., stone, scafco, corpor, ...            0\n",
      "3  [mclean', mansion, (origin, holli, lea), homes...            6\n",
      "4  [avioan, craiova, iar-93, vultur, (eagle), twi...            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 146633\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 146633)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 96.56%\n",
      "\n",
      "Class 0 -> Precision: 0.9348, Recall: 0.8817, F1-Score: 0.9075\n",
      "Class 1 -> Precision: 0.9617, Recall: 0.9829, F1-Score: 0.9722\n",
      "Class 2 -> Precision: 0.9545, Recall: 0.9067, F1-Score: 0.9300\n",
      "Class 3 -> Precision: 0.9823, Recall: 0.9924, F1-Score: 0.9873\n",
      "Class 4 -> Precision: 0.9749, Recall: 0.9776, F1-Score: 0.9762\n",
      "Class 5 -> Precision: 0.9741, Recall: 0.9867, F1-Score: 0.9804\n",
      "Class 6 -> Precision: 0.9619, Recall: 0.9526, F1-Score: 0.9572\n",
      "Class 7 -> Precision: 0.9723, Recall: 0.9921, F1-Score: 0.9821\n",
      "Class 8 -> Precision: 0.9987, Recall: 0.9629, F1-Score: 0.9805\n",
      "Class 9 -> Precision: 0.9956, Recall: 0.9683, F1-Score: 0.9817\n",
      "Class 10 -> Precision: 0.9796, Recall: 0.9917, F1-Score: 0.9856\n",
      "Class 11 -> Precision: 0.9396, Recall: 0.9921, F1-Score: 0.9651\n",
      "Class 12 -> Precision: 0.9587, Recall: 0.9824, F1-Score: 0.9704\n",
      "Class 13 -> Precision: 0.9306, Recall: 0.9477, F1-Score: 0.9391\n",
      "\n",
      "Macro-Average F1 Score: 0.9654\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.40%\n",
      "\n",
      "Class 0 -> Precision: 0.9178, Recall: 0.8580, F1-Score: 0.8869\n",
      "Class 1 -> Precision: 0.9492, Recall: 0.9796, F1-Score: 0.9642\n",
      "Class 2 -> Precision: 0.9318, Recall: 0.8524, F1-Score: 0.8903\n",
      "Class 3 -> Precision: 0.9798, Recall: 0.9880, F1-Score: 0.9839\n",
      "Class 4 -> Precision: 0.9631, Recall: 0.9708, F1-Score: 0.9669\n",
      "Class 5 -> Precision: 0.9635, Recall: 0.9820, F1-Score: 0.9727\n",
      "Class 6 -> Precision: 0.9577, Recall: 0.9424, F1-Score: 0.9500\n",
      "Class 7 -> Precision: 0.9630, Recall: 0.9896, F1-Score: 0.9761\n",
      "Class 8 -> Precision: 0.9987, Recall: 0.9580, F1-Score: 0.9780\n",
      "Class 9 -> Precision: 0.9906, Recall: 0.9728, F1-Score: 0.9816\n",
      "Class 10 -> Precision: 0.9844, Recall: 0.9840, F1-Score: 0.9842\n",
      "Class 11 -> Precision: 0.9250, Recall: 0.9872, F1-Score: 0.9551\n",
      "Class 12 -> Precision: 0.9369, Recall: 0.9740, F1-Score: 0.9551\n",
      "Class 13 -> Precision: 0.8961, Recall: 0.9172, F1-Score: 0.9065\n",
      "\n",
      "Macro-Average F1 Score: 0.9537\n"
     ]
    }
   ],
   "source": [
    "# Question 1, part 2 :: Unigram with stemming and removing stop words\n",
    "\n",
    "# # (a) Perform stemming and remove the stop-words in the training as well as the validation data.\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = 1)\n",
    "\n",
    "# (b) Construct word clouds for both classes on the transformed data\n",
    "print(\"Word cloud on training data\")\n",
    "# plot_wordclouds_per_class(trainingData, data_type = \"Training Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "print(\"Word cloud on testing data\")\n",
    "# plot_wordclouds_per_class(testingData, data_type = \"Testing Set\", maxWords = 200, width = 800, height = 400)\n",
    "\n",
    "# (c) Learn a new model on the transformed data. Report the validation set accuracy\n",
    "# 2. unigram - with stem - with stop words       - done\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d) How does your accuracy change over the validation set? Comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest erni, erni cox, cox (februari, (februa...            3\n",
      "1  [holosteum genu, genu plant, plant pink, pink ...           10\n",
      "2  [pestarella tyrrhena, tyrrhena (formerli, (for...            9\n",
      "3  [midsun junior, junior high, high school, scho...            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajo draho, draho (7, (7 march, march 1895, 1...            4\n",
      "1  [uss huntsvil, huntsvil steamer, steamer acqui...            5\n",
      "2  [found 1954, 1954 ben, ben g., g. stone, stone...            0\n",
      "3  [mclean' mansion, mansion (origin, (origin hol...            6\n",
      "4  [avioan craiova, craiova iar-93, iar-93 vultur...            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 666694\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 666694)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 96.97%\n",
      "\n",
      "Class 0 -> Precision: 0.9138, Recall: 0.9437, F1-Score: 0.9285\n",
      "Class 1 -> Precision: 0.9659, Recall: 0.9890, F1-Score: 0.9773\n",
      "Class 2 -> Precision: 0.9726, Recall: 0.9122, F1-Score: 0.9414\n",
      "Class 3 -> Precision: 0.9728, Recall: 0.9947, F1-Score: 0.9836\n",
      "Class 4 -> Precision: 0.9668, Recall: 0.9784, F1-Score: 0.9726\n",
      "Class 5 -> Precision: 0.9769, Recall: 0.9781, F1-Score: 0.9775\n",
      "Class 6 -> Precision: 0.9694, Recall: 0.9616, F1-Score: 0.9655\n",
      "Class 7 -> Precision: 0.9798, Recall: 0.9828, F1-Score: 0.9813\n",
      "Class 8 -> Precision: 0.9907, Recall: 0.9834, F1-Score: 0.9871\n",
      "Class 9 -> Precision: 0.9927, Recall: 0.9380, F1-Score: 0.9646\n",
      "Class 10 -> Precision: 0.9566, Recall: 0.9846, F1-Score: 0.9704\n",
      "Class 11 -> Precision: 0.9712, Recall: 0.9911, F1-Score: 0.9810\n",
      "Class 12 -> Precision: 0.9782, Recall: 0.9870, F1-Score: 0.9826\n",
      "Class 13 -> Precision: 0.9727, Recall: 0.9517, F1-Score: 0.9621\n",
      "\n",
      "Macro-Average F1 Score: 0.9697\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.54%\n",
      "\n",
      "Class 0 -> Precision: 0.9353, Recall: 0.8960, F1-Score: 0.9152\n",
      "Class 1 -> Precision: 0.9514, Recall: 0.9784, F1-Score: 0.9647\n",
      "Class 2 -> Precision: 0.9462, Recall: 0.8580, F1-Score: 0.8999\n",
      "Class 3 -> Precision: 0.9708, Recall: 0.9852, F1-Score: 0.9780\n",
      "Class 4 -> Precision: 0.9484, Recall: 0.9636, F1-Score: 0.9560\n",
      "Class 5 -> Precision: 0.9700, Recall: 0.9708, F1-Score: 0.9704\n",
      "Class 6 -> Precision: 0.9628, Recall: 0.9324, F1-Score: 0.9474\n",
      "Class 7 -> Precision: 0.9446, Recall: 0.9888, F1-Score: 0.9662\n",
      "Class 8 -> Precision: 0.9955, Recall: 0.9672, F1-Score: 0.9811\n",
      "Class 9 -> Precision: 0.9690, Recall: 0.9504, F1-Score: 0.9596\n",
      "Class 10 -> Precision: 0.9490, Recall: 0.9832, F1-Score: 0.9658\n",
      "Class 11 -> Precision: 0.9455, Recall: 0.9932, F1-Score: 0.9688\n",
      "Class 12 -> Precision: 0.9596, Recall: 0.9788, F1-Score: 0.9691\n",
      "Class 13 -> Precision: 0.9289, Recall: 0.9300, F1-Score: 0.9294\n",
      "\n",
      "Macro-Average F1 Score: 0.9551\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram with stemming and removing stop words\n",
    "# 3. bigram - with stem - with stop words        -done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = True, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest ernie, ernie cox, cox (february, (febr...            3\n",
      "1  [holosteum is, is a, a genus, genus of, of pla...           10\n",
      "2  [pestarella tyrrhena, tyrrhena (formerly, (for...            9\n",
      "3  [midsun junior, junior high, high school, scho...            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajos drahos, drahos (7, (7 march, march 1895...            4\n",
      "1  [uss huntsville, huntsville was, was a, a stea...            5\n",
      "2  [founded in, in 1954, 1954 by, by ben, ben g.,...            0\n",
      "3  [mclean's mansion, mansion (originally, (origi...            6\n",
      "4  [the avioane, avioane craiova, craiova iar-93,...            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 649720\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 649720)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 98.07%\n",
      "\n",
      "Class 0 -> Precision: 0.9742, Recall: 0.9595, F1-Score: 0.9668\n",
      "Class 1 -> Precision: 0.9758, Recall: 0.9927, F1-Score: 0.9842\n",
      "Class 2 -> Precision: 0.9771, Recall: 0.9604, F1-Score: 0.9687\n",
      "Class 3 -> Precision: 0.9829, Recall: 0.9909, F1-Score: 0.9869\n",
      "Class 4 -> Precision: 0.9764, Recall: 0.9869, F1-Score: 0.9816\n",
      "Class 5 -> Precision: 0.9880, Recall: 0.9896, F1-Score: 0.9888\n",
      "Class 6 -> Precision: 0.9733, Recall: 0.9748, F1-Score: 0.9741\n",
      "Class 7 -> Precision: 0.9762, Recall: 0.9945, F1-Score: 0.9853\n",
      "Class 8 -> Precision: 0.9992, Recall: 0.9773, F1-Score: 0.9881\n",
      "Class 9 -> Precision: 0.9960, Recall: 0.9444, F1-Score: 0.9695\n",
      "Class 10 -> Precision: 0.9562, Recall: 0.9903, F1-Score: 0.9729\n",
      "Class 11 -> Precision: 0.9867, Recall: 0.9958, F1-Score: 0.9912\n",
      "Class 12 -> Precision: 0.9903, Recall: 0.9904, F1-Score: 0.9904\n",
      "Class 13 -> Precision: 0.9790, Recall: 0.9818, F1-Score: 0.9804\n",
      "\n",
      "Macro-Average F1 Score: 0.9806\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.87%\n",
      "\n",
      "Class 0 -> Precision: 0.9499, Recall: 0.9328, F1-Score: 0.9413\n",
      "Class 1 -> Precision: 0.9632, Recall: 0.9852, F1-Score: 0.9741\n",
      "Class 2 -> Precision: 0.9618, Recall: 0.9276, F1-Score: 0.9444\n",
      "Class 3 -> Precision: 0.9766, Recall: 0.9836, F1-Score: 0.9801\n",
      "Class 4 -> Precision: 0.9616, Recall: 0.9808, F1-Score: 0.9711\n",
      "Class 5 -> Precision: 0.9800, Recall: 0.9804, F1-Score: 0.9802\n",
      "Class 6 -> Precision: 0.9610, Recall: 0.9564, F1-Score: 0.9587\n",
      "Class 7 -> Precision: 0.9646, Recall: 0.9920, F1-Score: 0.9781\n",
      "Class 8 -> Precision: 0.9996, Recall: 0.9668, F1-Score: 0.9829\n",
      "Class 9 -> Precision: 0.9874, Recall: 0.9388, F1-Score: 0.9625\n",
      "Class 10 -> Precision: 0.9507, Recall: 0.9804, F1-Score: 0.9653\n",
      "Class 11 -> Precision: 0.9764, Recall: 0.9924, F1-Score: 0.9843\n",
      "Class 12 -> Precision: 0.9773, Recall: 0.9836, F1-Score: 0.9805\n",
      "Class 13 -> Precision: 0.9539, Recall: 0.9612, F1-Score: 0.9576\n",
      "\n",
      "Macro-Average F1 Score: 0.9686\n"
     ]
    }
   ],
   "source": [
    "# Question 3 :: Bigram without stemming or removing stop words\n",
    "# bigram - without stem - without stop words  - done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = False, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0,  text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest, erni, cox, (februari, 17, 1894, –, fe...            3\n",
      "1  [holosteum, is, a, genu, of, plant, in, the, p...           10\n",
      "2  [pestarella, tyrrhena, (formerli, callianassa,...            9\n",
      "3  [midsun, junior, high, school, is, a, canadian...            1\n",
      "4  [st, james', church, wrightington, bar, is, in...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajo, draho, (7, march, 1895, -, 2, june, 198...            4\n",
      "1  [uss, huntsvil, wa, a, steamer, acquir, by, th...            5\n",
      "2  [found, in, 1954, by, ben, g., stone, scafco, ...            0\n",
      "3  [mclean', mansion, (origin, holli, lea), is, a...            6\n",
      "4  [the, avioan, craiova, iar-93, vultur, (eagle)...            5\n",
      "--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 146761\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 146761)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 96.65%\n",
      "\n",
      "Class 0 -> Precision: 0.9381, Recall: 0.8824, F1-Score: 0.9094\n",
      "Class 1 -> Precision: 0.9627, Recall: 0.9828, F1-Score: 0.9726\n",
      "Class 2 -> Precision: 0.9666, Recall: 0.9313, F1-Score: 0.9486\n",
      "Class 3 -> Precision: 0.9808, Recall: 0.9924, F1-Score: 0.9866\n",
      "Class 4 -> Precision: 0.9736, Recall: 0.9806, F1-Score: 0.9771\n",
      "Class 5 -> Precision: 0.9749, Recall: 0.9860, F1-Score: 0.9804\n",
      "Class 6 -> Precision: 0.9570, Recall: 0.9524, F1-Score: 0.9547\n",
      "Class 7 -> Precision: 0.9636, Recall: 0.9906, F1-Score: 0.9769\n",
      "Class 8 -> Precision: 0.9985, Recall: 0.9517, F1-Score: 0.9746\n",
      "Class 9 -> Precision: 0.9943, Recall: 0.9634, F1-Score: 0.9786\n",
      "Class 10 -> Precision: 0.9788, Recall: 0.9901, F1-Score: 0.9844\n",
      "Class 11 -> Precision: 0.9478, Recall: 0.9928, F1-Score: 0.9698\n",
      "Class 12 -> Precision: 0.9628, Recall: 0.9814, F1-Score: 0.9720\n",
      "Class 13 -> Precision: 0.9334, Recall: 0.9531, F1-Score: 0.9431\n",
      "\n",
      "Macro-Average F1 Score: 0.9663\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.74%\n",
      "\n",
      "Class 0 -> Precision: 0.9200, Recall: 0.8644, F1-Score: 0.8913\n",
      "Class 1 -> Precision: 0.9559, Recall: 0.9804, F1-Score: 0.9680\n",
      "Class 2 -> Precision: 0.9526, Recall: 0.8932, F1-Score: 0.9220\n",
      "Class 3 -> Precision: 0.9794, Recall: 0.9872, F1-Score: 0.9833\n",
      "Class 4 -> Precision: 0.9633, Recall: 0.9768, F1-Score: 0.9700\n",
      "Class 5 -> Precision: 0.9653, Recall: 0.9804, F1-Score: 0.9728\n",
      "Class 6 -> Precision: 0.9546, Recall: 0.9428, F1-Score: 0.9487\n",
      "Class 7 -> Precision: 0.9599, Recall: 0.9872, F1-Score: 0.9734\n",
      "Class 8 -> Precision: 0.9979, Recall: 0.9504, F1-Score: 0.9736\n",
      "Class 9 -> Precision: 0.9902, Recall: 0.9668, F1-Score: 0.9783\n",
      "Class 10 -> Precision: 0.9824, Recall: 0.9832, F1-Score: 0.9828\n",
      "Class 11 -> Precision: 0.9372, Recall: 0.9908, F1-Score: 0.9633\n",
      "Class 12 -> Precision: 0.9429, Recall: 0.9712, F1-Score: 0.9568\n",
      "Class 13 -> Precision: 0.9046, Recall: 0.9288, F1-Score: 0.9165\n",
      "\n",
      "Macro-Average F1 Score: 0.9572\n"
     ]
    }
   ],
   "source": [
    "# unigram - with stem - without stop words\n",
    "# Question 4 :: uigram with stemming and removing stop words\n",
    "# 4. uigram - with stem - with stop words        -done\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = True, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = True, window = 1)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest, ernie, cox, (february, 17, 1894, –, f...            3\n",
      "1  [holosteum, genus, plants, pink, family, (cary...           10\n",
      "2  [pestarella, tyrrhena, (formerly, callianassa,...            9\n",
      "3  [midsun, junior, high, school, canadian, middl...            1\n",
      "4  [st, james', church, wrightington, bar, church...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajos, drahos, (7, march, 1895, -, 2, june, 1...            4\n",
      "1  [uss, huntsville, steamer, acquired, union, na...            5\n",
      "2  [founded, 1954, ben, g., stone, scafco, corpor...            0\n",
      "3  [mclean's, mansion, (originally, holly, lea), ...            6\n",
      "4  [avioane, craiova, iar-93, vultur, (eagle), tw...            5\n",
      "--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 158589\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 158589)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 97.07%\n",
      "\n",
      "Class 0 -> Precision: 0.9497, Recall: 0.9084, F1-Score: 0.9286\n",
      "Class 1 -> Precision: 0.9643, Recall: 0.9846, F1-Score: 0.9743\n",
      "Class 2 -> Precision: 0.9623, Recall: 0.9215, F1-Score: 0.9415\n",
      "Class 3 -> Precision: 0.9844, Recall: 0.9933, F1-Score: 0.9889\n",
      "Class 4 -> Precision: 0.9752, Recall: 0.9806, F1-Score: 0.9779\n",
      "Class 5 -> Precision: 0.9790, Recall: 0.9898, F1-Score: 0.9844\n",
      "Class 6 -> Precision: 0.9664, Recall: 0.9578, F1-Score: 0.9621\n",
      "Class 7 -> Precision: 0.9736, Recall: 0.9930, F1-Score: 0.9832\n",
      "Class 8 -> Precision: 0.9987, Recall: 0.9658, F1-Score: 0.9820\n",
      "Class 9 -> Precision: 0.9959, Recall: 0.9691, F1-Score: 0.9823\n",
      "Class 10 -> Precision: 0.9785, Recall: 0.9927, F1-Score: 0.9856\n",
      "Class 11 -> Precision: 0.9531, Recall: 0.9934, F1-Score: 0.9728\n",
      "Class 12 -> Precision: 0.9639, Recall: 0.9857, F1-Score: 0.9747\n",
      "Class 13 -> Precision: 0.9455, Recall: 0.9537, F1-Score: 0.9496\n",
      "\n",
      "Macro-Average F1 Score: 0.9706\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.90%\n",
      "\n",
      "Class 0 -> Precision: 0.9330, Recall: 0.8804, F1-Score: 0.9059\n",
      "Class 1 -> Precision: 0.9541, Recall: 0.9816, F1-Score: 0.9677\n",
      "Class 2 -> Precision: 0.9416, Recall: 0.8708, F1-Score: 0.9048\n",
      "Class 3 -> Precision: 0.9817, Recall: 0.9876, F1-Score: 0.9846\n",
      "Class 4 -> Precision: 0.9640, Recall: 0.9752, F1-Score: 0.9696\n",
      "Class 5 -> Precision: 0.9678, Recall: 0.9852, F1-Score: 0.9764\n",
      "Class 6 -> Precision: 0.9637, Recall: 0.9440, F1-Score: 0.9537\n",
      "Class 7 -> Precision: 0.9638, Recall: 0.9904, F1-Score: 0.9769\n",
      "Class 8 -> Precision: 0.9979, Recall: 0.9604, F1-Score: 0.9788\n",
      "Class 9 -> Precision: 0.9902, Recall: 0.9748, F1-Score: 0.9825\n",
      "Class 10 -> Precision: 0.9848, Recall: 0.9840, F1-Score: 0.9844\n",
      "Class 11 -> Precision: 0.9379, Recall: 0.9904, F1-Score: 0.9634\n",
      "Class 12 -> Precision: 0.9375, Recall: 0.9780, F1-Score: 0.9573\n",
      "Class 13 -> Precision: 0.9101, Recall: 0.9236, F1-Score: 0.9168\n",
      "\n",
      "Macro-Average F1 Score: 0.9588\n"
     ]
    }
   ],
   "source": [
    "# 4. unigram - without stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = False, window = 1)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = False, window = 1)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------UNIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"UNIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest erni, erni cox, cox (februari, (februa...            3\n",
      "1  [holosteum is, is a, a genu, genu of, of plant...           10\n",
      "2  [pestarella tyrrhena, tyrrhena (formerli, (for...            9\n",
      "3  [midsun junior, junior high, high school, scho...            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajo draho, draho (7, (7 march, march 1895, 1...            4\n",
      "1  [uss huntsvil, huntsvil wa, wa a, a steamer, s...            5\n",
      "2  [found in, in 1954, 1954 by, by ben, ben g., g...            0\n",
      "3  [mclean' mansion, mansion (origin, (origin hol...            6\n",
      "4  [the avioan, avioan craiova, craiova iar-93, i...            5\n",
      "--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 621022\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 621022)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 98.06%\n",
      "\n",
      "Class 0 -> Precision: 0.9719, Recall: 0.9588, F1-Score: 0.9653\n",
      "Class 1 -> Precision: 0.9765, Recall: 0.9925, F1-Score: 0.9844\n",
      "Class 2 -> Precision: 0.9778, Recall: 0.9573, F1-Score: 0.9675\n",
      "Class 3 -> Precision: 0.9818, Recall: 0.9918, F1-Score: 0.9868\n",
      "Class 4 -> Precision: 0.9766, Recall: 0.9867, F1-Score: 0.9816\n",
      "Class 5 -> Precision: 0.9885, Recall: 0.9890, F1-Score: 0.9888\n",
      "Class 6 -> Precision: 0.9747, Recall: 0.9745, F1-Score: 0.9746\n",
      "Class 7 -> Precision: 0.9762, Recall: 0.9948, F1-Score: 0.9854\n",
      "Class 8 -> Precision: 0.9991, Recall: 0.9761, F1-Score: 0.9875\n",
      "Class 9 -> Precision: 0.9962, Recall: 0.9491, F1-Score: 0.9721\n",
      "Class 10 -> Precision: 0.9605, Recall: 0.9909, F1-Score: 0.9755\n",
      "Class 11 -> Precision: 0.9854, Recall: 0.9956, F1-Score: 0.9905\n",
      "Class 12 -> Precision: 0.9889, Recall: 0.9906, F1-Score: 0.9898\n",
      "Class 13 -> Precision: 0.9758, Recall: 0.9806, F1-Score: 0.9782\n",
      "\n",
      "Macro-Average F1 Score: 0.9806\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 96.81%\n",
      "\n",
      "Class 0 -> Precision: 0.9471, Recall: 0.9240, F1-Score: 0.9354\n",
      "Class 1 -> Precision: 0.9618, Recall: 0.9860, F1-Score: 0.9737\n",
      "Class 2 -> Precision: 0.9618, Recall: 0.9268, F1-Score: 0.9440\n",
      "Class 3 -> Precision: 0.9762, Recall: 0.9836, F1-Score: 0.9799\n",
      "Class 4 -> Precision: 0.9634, Recall: 0.9796, F1-Score: 0.9714\n",
      "Class 5 -> Precision: 0.9789, Recall: 0.9824, F1-Score: 0.9806\n",
      "Class 6 -> Precision: 0.9632, Recall: 0.9532, F1-Score: 0.9582\n",
      "Class 7 -> Precision: 0.9646, Recall: 0.9932, F1-Score: 0.9787\n",
      "Class 8 -> Precision: 0.9996, Recall: 0.9652, F1-Score: 0.9821\n",
      "Class 9 -> Precision: 0.9891, Recall: 0.9448, F1-Score: 0.9664\n",
      "Class 10 -> Precision: 0.9575, Recall: 0.9816, F1-Score: 0.9694\n",
      "Class 11 -> Precision: 0.9722, Recall: 0.9936, F1-Score: 0.9828\n",
      "Class 12 -> Precision: 0.9754, Recall: 0.9820, F1-Score: 0.9787\n",
      "Class 13 -> Precision: 0.9448, Recall: 0.9576, F1-Score: 0.9511\n",
      "\n",
      "Macro-Average F1 Score: 0.9680\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - with stem - without stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = True, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = False, with_stemming = True, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITH STEMMING -- WITHOUT STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITH STEMMING-WITHOUT STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [ernest ernie, ernie cox, cox (february, (febr...            3\n",
      "1  [holosteum genus, genus plants, plants pink, p...           10\n",
      "2  [pestarella tyrrhena, tyrrhena (formerly, (for...            9\n",
      "3  [midsun junior, junior high, high school, scho...            1\n",
      "4  [st james', james' church, church wrightington...            6\n",
      "testingData.head()\n",
      "                               Tokenized Description  Class Index\n",
      "0  [lajos drahos, drahos (7, (7 march, march 1895...            4\n",
      "1  [uss huntsville, huntsville steamer, steamer a...            5\n",
      "2  [founded 1954, 1954 ben, ben g., g. stone, sto...            0\n",
      "3  [mclean's mansion, mansion (originally, (origi...            6\n",
      "4  [avioane craiova, craiova iar-93, iar-93 vultu...            5\n",
      "--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\n",
      "Number of classes: 14, examples: 140000, vocab size: 687657\n",
      "Shape of phi_y: (14,)\n",
      "Shape of phi_j_given_y: (14, 687657)\n",
      "Evauating on train data...\n",
      "Evaluating on 140000 examples\n",
      "Overall Accuracy: 96.68%\n",
      "\n",
      "Class 0 -> Precision: 0.9022, Recall: 0.9413, F1-Score: 0.9214\n",
      "Class 1 -> Precision: 0.9620, Recall: 0.9870, F1-Score: 0.9743\n",
      "Class 2 -> Precision: 0.9724, Recall: 0.9081, F1-Score: 0.9391\n",
      "Class 3 -> Precision: 0.9692, Recall: 0.9937, F1-Score: 0.9813\n",
      "Class 4 -> Precision: 0.9603, Recall: 0.9779, F1-Score: 0.9690\n",
      "Class 5 -> Precision: 0.9739, Recall: 0.9741, F1-Score: 0.9740\n",
      "Class 6 -> Precision: 0.9635, Recall: 0.9576, F1-Score: 0.9605\n",
      "Class 7 -> Precision: 0.9765, Recall: 0.9798, F1-Score: 0.9781\n",
      "Class 8 -> Precision: 0.9894, Recall: 0.9828, F1-Score: 0.9861\n",
      "Class 9 -> Precision: 0.9916, Recall: 0.9291, F1-Score: 0.9593\n",
      "Class 10 -> Precision: 0.9500, Recall: 0.9820, F1-Score: 0.9657\n",
      "Class 11 -> Precision: 0.9743, Recall: 0.9905, F1-Score: 0.9823\n",
      "Class 12 -> Precision: 0.9809, Recall: 0.9852, F1-Score: 0.9830\n",
      "Class 13 -> Precision: 0.9741, Recall: 0.9456, F1-Score: 0.9597\n",
      "\n",
      "Macro-Average F1 Score: 0.9667\n",
      "Evauating on test data...\n",
      "Evaluating on 35000 examples\n",
      "Overall Accuracy: 95.59%\n",
      "\n",
      "Class 0 -> Precision: 0.9309, Recall: 0.9056, F1-Score: 0.9181\n",
      "Class 1 -> Precision: 0.9540, Recall: 0.9784, F1-Score: 0.9660\n",
      "Class 2 -> Precision: 0.9447, Recall: 0.8672, F1-Score: 0.9043\n",
      "Class 3 -> Precision: 0.9682, Recall: 0.9852, F1-Score: 0.9766\n",
      "Class 4 -> Precision: 0.9491, Recall: 0.9624, F1-Score: 0.9557\n",
      "Class 5 -> Precision: 0.9711, Recall: 0.9676, F1-Score: 0.9693\n",
      "Class 6 -> Precision: 0.9611, Recall: 0.9284, F1-Score: 0.9445\n",
      "Class 7 -> Precision: 0.9441, Recall: 0.9856, F1-Score: 0.9644\n",
      "Class 8 -> Precision: 0.9951, Recall: 0.9680, F1-Score: 0.9813\n",
      "Class 9 -> Precision: 0.9650, Recall: 0.9496, F1-Score: 0.9573\n",
      "Class 10 -> Precision: 0.9483, Recall: 0.9828, F1-Score: 0.9652\n",
      "Class 11 -> Precision: 0.9549, Recall: 0.9920, F1-Score: 0.9731\n",
      "Class 12 -> Precision: 0.9585, Recall: 0.9796, F1-Score: 0.9689\n",
      "Class 13 -> Precision: 0.9379, Recall: 0.9300, F1-Score: 0.9339\n",
      "\n",
      "Macro-Average F1 Score: 0.9556\n"
     ]
    }
   ],
   "source": [
    "# 4. bigram - without stem - with stop words\n",
    "df_train, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_train, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = False, window = 2)\n",
    "df_test, vocabulary = tokenizeAndRemoveStopWordsOrStemAndReturnVocabulary(df_test, \"content\", target_col = \"Tokenized Description\", remove_stop_words = True, with_stemming = False, window = 2)\n",
    "\n",
    "trainingData, testingData = getTrainingAndTestingData(df_train, df_test, target_field = \"Tokenized Description\")\n",
    "\n",
    "model = NaiveBayes()\n",
    "print(\"--------BIGRAM -- WITHOUT STEMMING -- WITH STOP WORDS REMOVAL---------\")\n",
    "results[\"BIGRAM-WITHOUT STEMMING-WITH STOP WORDS REMOVAL\"] = run_model(model, vocabulary, trainingData, testingData, smoothening = 1.0, text_col = \"Tokenized Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
